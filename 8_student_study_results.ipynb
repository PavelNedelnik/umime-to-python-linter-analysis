{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import binomtest, chisquare, kruskal, mannwhitneyu\n",
    "\n",
    "\n",
    "import src.ipython_loader as loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CONFIG_ENV\"] = \"debug\"\n",
    "if False:\n",
    "    os.environ[\"CONFIG_ENV\"] = \"production\"\n",
    "\n",
    "from config import load_config\n",
    "config = load_config()\n",
    "\n",
    "DEBUG = config[\"DEBUG\"]\n",
    "RESOLUTION = config['DEFAULTS']['resolution']\n",
    "\n",
    "# input data\n",
    "STUDY_RESULTS_PATH = config['PATHS']['student_study_results']\n",
    "STUDENT_METADATA_PATH = config[\"PATHS\"][\"student_hold_out_set\"] / 'student_study_submissions'\n",
    "\n",
    "# output data\n",
    "IMAGE_DIR = config['PATHS']['images'] / 'student_study_results'\n",
    "\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing utils\n",
    "def _normalize_model_versions(x):\n",
    "    if x == \"Version A\":\n",
    "        return \"model\"\n",
    "    elif x == \"Version B\":\n",
    "        return \"baseline\"\n",
    "    # neutral or undecided\n",
    "    return \"neutral\"\n",
    "\n",
    "def _normalize_preference_category(x):\n",
    "    if x == \"The ordering made more sense.\":\n",
    "        return \"order\"\n",
    "    elif x == \"The explanations were clearer.\":\n",
    "        return \"explanation\"\n",
    "    elif x == \"I understood one version more clearly.\":\n",
    "        return \"clarity\"\n",
    "    return \"other\"\n",
    "\n",
    "def _normalize_explanations_category(x):\n",
    "    if x == \"Not really.\":\n",
    "        return \"no\"\n",
    "    elif x == \"Somewhat.\":\n",
    "        return \"somewhat\"\n",
    "    elif x == \"Yes, definitely.\":\n",
    "        return \"yes\"\n",
    "    raise ValueError(f\"{x} not recognized.\")\n",
    "    \n",
    "def _normalize_fix_category(x):\n",
    "    if x == \"Yes, it would help.\":\n",
    "        return \"yes\"\n",
    "    elif x == \"No, I would ignore it.\":\n",
    "        return \"no\"\n",
    "    elif x == \"Sometimes.\":\n",
    "        return \"somewhat\"\n",
    "    raise ValueError(f\"{x} not recognized.\")\n",
    "    \n",
    "def _normalize_understood(x):\n",
    "    \"\"\"Set manually based on inspection of the data.\"\"\"\n",
    "    x = str(x).lower()\n",
    "    if \"yes\" in x or \"hope\" in x or \"mostly\" in x:\n",
    "        return \"yes\"\n",
    "    if \"no\" in x:\n",
    "        return \"no\"\n",
    "    return \"maybe\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignment_rows = []\n",
    "\n",
    "for folder in Path(STUDENT_METADATA_PATH).glob(\"student_*\"):\n",
    "    student_idx = int(folder.name.split(\"_\")[1])\n",
    "    assignment_file = folder / \"assignment.json\"\n",
    "\n",
    "    with open(assignment_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for task_idx, version in data[\"left_versions\"].items():\n",
    "        assignment_rows.append({\n",
    "            \"student_id\": str(student_idx),\n",
    "            \"question_id\": str(task_idx),\n",
    "            \"left_version\": 'model' if version == 'A' else 'baseline'\n",
    "        })\n",
    "\n",
    "assignment_df = pd.DataFrame(assignment_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_final = pd.read_csv(STUDY_RESULTS_PATH / \"final.csv\")\n",
    "original_submissions = pd.read_csv(STUDY_RESULTS_PATH / \"per_submission.csv\")\n",
    "\n",
    "per_submission_df = pd.DataFrame()\n",
    "\n",
    "per_submission_df['time'] = pd.to_datetime(\n",
    "    original_submissions[\"Timestamp\"],\n",
    "    format=\"%Y/%m/%d %I:%M:%S %p %Z\",\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "per_submission_df['student_id'] = original_submissions[\"Student ID\"].astype(str)\n",
    "per_submission_df['question_id'] = original_submissions[\"Question ID\"].astype(str)\n",
    "\n",
    "# add if the model was on the left\n",
    "per_submission_df = per_submission_df.merge(\n",
    "    assignment_df,\n",
    "    on=[\"student_id\", \"question_id\"],\n",
    ")\n",
    "\n",
    "per_submission_df[\"prefer_model\"] = original_submissions[\"Which version did you prefer?\"].apply(_normalize_model_versions)\n",
    "per_submission_df[\"helps_first\"] = original_submissions[\"Which version would help you decide what to fix first?\"].apply(_normalize_model_versions)\n",
    "per_submission_df[\"top_defect\"] = original_submissions[\"Focusing only on the first defect, which version ranked it better?\"].apply(_normalize_model_versions)\n",
    "\n",
    "per_submission_df[\"confidence\"] = pd.to_numeric(original_submissions[\"How confident you are in your choices?\"], errors=\"coerce\")\n",
    "\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "final_df['Timestamp'] = pd.to_datetime(\n",
    "    original_final[\"Timestamp\"],\n",
    "    format=\"%Y/%m/%d %I:%M:%S %p %Z\",\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "final_df['student_id'] = original_final[\"Student ID\"].fillna(\"Unknown\")\n",
    "\n",
    "final_df[\"why_version\"] = original_final[\"What made you prefer one version over the other?\"].apply(_normalize_preference_category)\n",
    "final_df[\"explanations_helped\"] = original_final[\"Did the explanations help you understand why defects were ordered in that way?\"].apply(_normalize_explanations_category)\n",
    "final_df[\"ordering_effect\"] = original_final[\"Did the ordering affect how you would approach fixing the code?\"].apply(_normalize_fix_category)\n",
    "final_df[\"confused_tasks\"] = original_final[\"Were any tasks confusing?\"].apply(_normalize_understood)\n",
    "final_df['confused_defects'] = original_final[\"Did you understand all the defects?\"].apply(_normalize_understood)\n",
    "final_df[\"comments\"] = original_final[\"Do you have any additional comments?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_submission_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_pref_df = (\n",
    "    per_submission_df\n",
    "    .loc[per_submission_df[\"prefer_model\"].isin([\"model\", \"baseline\"])]\n",
    "    .groupby(\"student_id\")\n",
    "    .agg(\n",
    "        n_tasks=(\"prefer_model\", \"count\"),\n",
    "        model_votes=(\"prefer_model\", lambda x: (x == \"model\").sum()),\n",
    "        baseline_votes=(\"prefer_model\", lambda x: (x == \"baseline\").sum()),\n",
    "        mean_confidence=(\"confidence\", \"mean\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "student_pref_df[\"model_fraction\"] = (\n",
    "    student_pref_df[\"model_votes\"] /\n",
    "    (student_pref_df[\"model_votes\"] + student_pref_df[\"baseline_votes\"])\n",
    ")\n",
    "\n",
    "student_pref_df[\"student_prefers_model\"] = student_pref_df[\"model_fraction\"] > 0.5\n",
    "\n",
    "student_pref_df[\"dominant_preference\"] = np.where(\n",
    "    student_pref_df[\"model_votes\"] > student_pref_df[\"baseline_votes\"],\n",
    "    \"model\",\n",
    "    \"baseline\"\n",
    ")\n",
    "\n",
    "student_pref_df[\"preference_strength\"] = (\n",
    "    student_pref_df[[\"model_votes\", \"baseline_votes\"]].max(axis=1)\n",
    "    / student_pref_df[\"n_tasks\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_submission_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_pref_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Student-level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of students: {len(student_pref_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(student_pref_df[\"dominant_preference\"].value_counts(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model preference:\")\n",
    "print(student_pref_df[\"model_fraction\"].describe(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportional_preference = student_pref_df[[\"model_votes\", \"baseline_votes\"]]\n",
    "proportional_preference = proportional_preference.div(proportional_preference.sum(axis=1), axis=0)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "proportional_preference.sort_values(\n",
    "    by=\"model_votes\", ascending=False\n",
    ").plot(\n",
    "    kind=\"bar\",\n",
    "    stacked=True,\n",
    "    figsize=(10,5)\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Student\")\n",
    "plt.ylabel(\"Proportion of preferences\")\n",
    "plt.title(\"Student-level preference distribution (model vs baseline)\")\n",
    "plt.legend(title=\"Preferred version\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"student_preference_stacked.png\", dpi=RESOLUTION)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportional_preference = student_pref_df[[\"model_votes\", \"baseline_votes\"]]\n",
    "proportional_preference = proportional_preference.div(proportional_preference.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(student_pref_df[\"preference_strength\"], bins=10)\n",
    "plt.xlabel(\"Fraction of consistent choices per student\")\n",
    "plt.ylabel(\"Number of students\")\n",
    "plt.title(\"Within-student consistency of preferences\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"student_preference_consistency.png\", dpi=RESOLUTION)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Task-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_pref = (\n",
    "    per_submission_df\n",
    "    .groupby(\"question_id\")[\"prefer_model\"]\n",
    "    .value_counts(normalize=True)\n",
    "    .unstack()\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "task_pref.sort_values(by=[\"model\", \"neutral\"], ascending=False, inplace=True)\n",
    "\n",
    "task_pref.plot(\n",
    "    kind=\"bar\",\n",
    "    stacked=True,\n",
    "    figsize=(10,5),\n",
    "    colormap=\"viridis\"\n",
    ")\n",
    "\n",
    "plt.title(\"Preference Proportions per Task (sorted by model preference)\")\n",
    "plt.xlabel(\"Task (question_id)\")\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.legend(title=\"Preferred version\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"preference_by_task.png\", dpi=RESOLUTION)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Submission-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Per-submission preference distribution ===\\n\")\n",
    "\n",
    "pref_counts = per_submission_df[\"prefer_model\"].value_counts()\n",
    "pref_props = pref_counts / pref_counts.sum()\n",
    "\n",
    "display(pd.DataFrame({\n",
    "    \"count\": pref_counts,\n",
    "    \"proportion\": pref_props.round(3)\n",
    "}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = per_submission_df.melt(\n",
    "    value_vars=[\"prefer_model\", \"helps_first\", \"top_defect\"],\n",
    "    var_name=\"question\",\n",
    "    value_name=\"response\"\n",
    ")\n",
    "\n",
    "# Human-readable labels\n",
    "plot_df[\"question\"] = plot_df[\"question\"].map({\n",
    "    \"prefer_model\": \"Overall preference\",\n",
    "    \"helps_first\": \"Helps decide what to fix\",\n",
    "    \"top_defect\": \"Ranks first defect better\"\n",
    "})\n",
    "\n",
    "g = sns.catplot(\n",
    "    data=plot_df,\n",
    "    x=\"response\",\n",
    "    col=\"question\",\n",
    "    kind=\"count\",\n",
    "    order=[\"model\", \"baseline\", \"neutral\"],\n",
    "    col_order=[\n",
    "        \"Overall preference\",\n",
    "        \"Helps decide what to fix\",\n",
    "        \"Ranks first defect better\"\n",
    "    ],\n",
    "    height=4,\n",
    "    aspect=0.9,\n",
    "    sharey=True\n",
    ")\n",
    "\n",
    "g.set_axis_labels(\"Version chosen\", \"Number of responses\")\n",
    "g.set_titles(\"{col_name}\")\n",
    "g.fig.suptitle(\"Student judgments across three related questions\", y=1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"combined_student_judgments.png\", dpi=RESOLUTION)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agreement_df = per_submission_df[\n",
    "    [\"prefer_model\", \"helps_first\", \"top_defect\"]\n",
    "]\n",
    "\n",
    "pairwise_agreement = {\n",
    "    \"preference vs helpfulness\":\n",
    "        (agreement_df[\"prefer_model\"] == agreement_df[\"helps_first\"]).mean(),\n",
    "    \"preference vs top defect\":\n",
    "        (agreement_df[\"prefer_model\"] == agreement_df[\"top_defect\"]).mean(),\n",
    "    \"helpfulness vs top defect\":\n",
    "        (agreement_df[\"helps_first\"] == agreement_df[\"top_defect\"]).mean(),\n",
    "}\n",
    "\n",
    "full_agreement = (agreement_df.nunique(axis=1) == 1).mean()\n",
    "\n",
    "print(\"=== Agreement across related questions ===\\n\")\n",
    "print(f\"Full agreement: {full_agreement:.2f}\\n\")\n",
    "for k, v in pairwise_agreement.items():\n",
    "    print(f\"{k}: {v:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Ordering vs explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Reasons for preferring one version ===\\n\")\n",
    "\n",
    "reason_counts = final_df[\"why_version\"].value_counts()\n",
    "reason_props = reason_counts / reason_counts.sum()\n",
    "\n",
    "display(pd.DataFrame({\n",
    "    \"count\": reason_counts,\n",
    "    \"proportion\": reason_props.round(3)\n",
    "}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(\n",
    "    data=final_df,\n",
    "    x=\"why_version\",\n",
    "    order=[\"order\", \"explanation\", \"clarity\", \"other\"]\n",
    ")\n",
    "plt.xlabel(\"Stated reason for preference\")\n",
    "plt.ylabel(\"Number of students\")\n",
    "plt.title(\"Why did students prefer one version?\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"preference_reasons.png\", dpi=RESOLUTION)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Did explanations help students understand the ordering? ===\\n\")\n",
    "\n",
    "expl_counts = final_df[\"explanations_helped\"].value_counts()\n",
    "expl_props = expl_counts / expl_counts.sum()\n",
    "\n",
    "display(pd.DataFrame({\n",
    "    \"count\": expl_counts,\n",
    "    \"proportion\": expl_props.round(3)\n",
    "}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(\n",
    "    data=final_df,\n",
    "    x=\"explanations_helped\",\n",
    "    order=[\"no\", \"somewhat\", \"yes\"]\n",
    ")\n",
    "plt.xlabel(\"Did explanations help?\")\n",
    "plt.ylabel(\"Number of students\")\n",
    "plt.title(\"Perceived usefulness of explanations\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"explanations_helpfulness.png\", dpi=RESOLUTION)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Left and right placement influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_right_df = (\n",
    "    per_submission_df\n",
    "    .groupby(\"left_version\")[\"prefer_model\"]\n",
    "    .value_counts(normalize=True)\n",
    "    .rename(\"fraction_chosen\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(\n",
    "    data=left_right_df,\n",
    "    x=\"left_version\",\n",
    "    y=\"fraction_chosen\",\n",
    "    hue=\"prefer_model\"\n",
    ")\n",
    "plt.title(\"Does left/right placement affect preference?\")\n",
    "plt.xlabel(\"Version shown on the left (model or baseline)\")\n",
    "plt.ylabel(\"Fraction chosen\")\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"left_right_effect.png\", dpi=RESOLUTION)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(per_submission_df[\"confidence\"], bins=7)\n",
    "plt.xlabel(\"Self-reported confidence\")\n",
    "plt.ylabel(\"Number of responses\")\n",
    "plt.title(\"Distribution of self-reported confidence\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"confidence_distribution.png\", dpi=RESOLUTION)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.boxplot(\n",
    "    data=per_submission_df,\n",
    "    x=\"prefer_model\",\n",
    "    y=\"confidence\",\n",
    "    order=[\"model\", \"baseline\", \"neutral\"]\n",
    ")\n",
    "plt.xlabel(\"Preferred version\")\n",
    "plt.ylabel(\"Self-reported confidence\")\n",
    "plt.title(\"Confidence by preferred version\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"submission_confidence_by_preference.png\", dpi=RESOLUTION)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.boxplot(\n",
    "    data=per_submission_df,\n",
    "    x=\"helps_first\",\n",
    "    y=\"confidence\",\n",
    "    order=[\"model\", \"baseline\", \"neutral\"]\n",
    ")\n",
    "plt.xlabel(\"Version reported as more helpful\")\n",
    "plt.ylabel(\"Self-reported confidence\")\n",
    "plt.title(\"Confidence by perceived helpfulness (descriptive)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"confidence_by_helpfulness.png\", dpi=RESOLUTION)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_confidence = (\n",
    "    per_submission_df\n",
    "    .groupby(\"student_id\")[\"confidence\"]\n",
    "    .mean()\n",
    "    .rename(\"mean_confidence\")\n",
    ")\n",
    "\n",
    "display(student_confidence.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(student_confidence, bins=10)\n",
    "plt.xlabel(\"Mean confidence per student\")\n",
    "plt.ylabel(\"Number of students\")\n",
    "plt.title(\"Average confidence per student\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"student_mean_confidence.png\", dpi=RESOLUTION)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusing tasks and defects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Were any tasks confusing? ===\\n\")\n",
    "print(final_df[\"confused_tasks\"].value_counts(), \"\\n\")\n",
    "\n",
    "print(\"=== Did you understand all the defects? ===\\n\")\n",
    "print(final_df[\"confused_defects\"].value_counts(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "\n",
    "sns.countplot(\n",
    "    data=final_df,\n",
    "    x=\"ordering_effect\",\n",
    "    order=[\"no\", \"somewhat\", \"yes\"]\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Student response\")\n",
    "plt.ylabel(\"Number of students\")\n",
    "plt.title(\"Does defect ordering affect how students approach fixing code?\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(IMAGE_DIR / \"ordering_effect.png\", dpi=RESOLUTION)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open-ended question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in original_final['Were any tasks confusing?'].items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in original_final['Did you understand all the defects?'].items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in original_final['Do you have any additional comments?'].items():\n",
    "    print(k, v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
