{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import binomtest, chisquare, kruskal, mannwhitneyu\n",
    "\n",
    "\n",
    "import src.ipython_loader as loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CONFIG_ENV\"] = \"debug\"\n",
    "if False:\n",
    "    os.environ[\"CONFIG_ENV\"] = \"production\"\n",
    "\n",
    "from config import load_config\n",
    "config = load_config()\n",
    "\n",
    "DEBUG = config[\"DEBUG\"]\n",
    "RESOLUTION = config['DEFAULTS']['resolution']\n",
    "\n",
    "# input data\n",
    "STUDY_RESULTS_PATH = config['PATHS']['student_study_results']\n",
    "STUDENT_METADATA_PATH = config[\"PATHS\"][\"student_hold_out_set\"] / 'student_study_submissions'\n",
    "\n",
    "# output data\n",
    "IMAGE_DIR = config['PATHS']['images'] / 'student_study_preparation'\n",
    "\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing utils\n",
    "def _normalize_model_versions(x):\n",
    "    if x == \"Version A\":\n",
    "        return \"model\"\n",
    "    elif x == \"Version B\":\n",
    "        return \"baseline\"\n",
    "    # neutral or undecided\n",
    "    return \"neutral\"\n",
    "\n",
    "def _normalize_preference_category(x):\n",
    "    if x == \"The ordering made more sense.\":\n",
    "        return \"order\"\n",
    "    elif x == \"The explanations were clearer.\":\n",
    "        return \"explanation\"\n",
    "    elif x == \"I understood one version more clearly.\":\n",
    "        return \"clarity\"\n",
    "    return \"other\"\n",
    "\n",
    "def _normalize_explanations_category(x):\n",
    "    if x == \"Not really.\":\n",
    "        return \"no\"\n",
    "    elif x == \"Somewhat.\":\n",
    "        return \"somewhat\"\n",
    "    elif x == \"Yes, definitely.\":\n",
    "        return \"yes\"\n",
    "    raise ValueError(f\"{x} not recognized.\")\n",
    "    \n",
    "def _normalize_fix_category(x):\n",
    "    if x == \"Yes, it would help.\":\n",
    "        return \"yes\"\n",
    "    elif x == \"No, I would ignore it.\":\n",
    "        return \"no\"\n",
    "    elif x == \"Sometimes.\":\n",
    "        return \"somewhat\"\n",
    "    raise ValueError(f\"{x} not recognized.\")\n",
    "    \n",
    "def _normalize_understood(x):\n",
    "    \"\"\"Set manually based on inspection of the data.\"\"\"\n",
    "    x = str(x).lower()\n",
    "    if \"yes\" in x or \"hope\" in x or \"mostly\" in x:\n",
    "        return \"yes\"\n",
    "    if \"no\" in x:\n",
    "        return \"no\"\n",
    "    return \"maybe\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignment_rows = []\n",
    "\n",
    "for folder in Path(STUDENT_METADATA_PATH).glob(\"student_*\"):\n",
    "    sid = int(folder.name.split(\"_\")[1])\n",
    "    assignment_file = folder / \"assignment.json\"\n",
    "\n",
    "    with open(assignment_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for task_idx, version in data[\"left_versions\"].items():\n",
    "        assignment_rows.append({\n",
    "            \"student_id\": int(sid),\n",
    "            \"question_id\": int(task_idx),\n",
    "            \"left_version\": 'model' if version == 'A' else 'baseline'\n",
    "        })\n",
    "\n",
    "assignment_df = pd.DataFrame(assignment_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_final = pd.read_csv(STUDY_RESULTS_PATH / \"final.csv\")\n",
    "original_submissions = pd.read_csv(STUDY_RESULTS_PATH / \"per_submission.csv\")\n",
    "\n",
    "per_submission_df = pd.DataFrame()\n",
    "\n",
    "per_submission_df['time'] = pd.to_datetime(\n",
    "    original_submissions[\"Timestamp\"],\n",
    "    format=\"%Y/%m/%d %I:%M:%S %p %Z\",\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "per_submission_df['student_id'] = original_submissions[\"Student ID\"].fillna(\"Unknown\")\n",
    "per_submission_df['question_id'] = original_submissions[\"Question ID\"].fillna(\"Unknown\")\n",
    "\n",
    "# add if the model was on the left\n",
    "per_submission_df = per_submission_df.merge(\n",
    "    assignment_df,\n",
    "    on=[\"student_id\", \"question_id\"],\n",
    ")\n",
    "\n",
    "per_submission_df[\"prefer_model\"] = original_submissions[\"Which version did you prefer?\"].apply(_normalize_model_versions)\n",
    "per_submission_df[\"helps_first\"] = original_submissions[\"Which version would help you decide what to fix first?\"].apply(_normalize_model_versions)\n",
    "per_submission_df[\"top_defect\"] = original_submissions[\"Focusing only on the first defect, which version ranked it better?\"].apply(_normalize_model_versions)\n",
    "\n",
    "per_submission_df[\"confidence\"] = pd.to_numeric(original_submissions[\"How confident you are in your choices?\"], errors=\"coerce\")\n",
    "\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "final_df['Timestamp'] = pd.to_datetime(\n",
    "    original_final[\"Timestamp\"],\n",
    "    format=\"%Y/%m/%d %I:%M:%S %p %Z\",\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "final_df[\"Student ID\"] = original_final[\"Student ID\"].fillna(\"Unknown\")\n",
    "\n",
    "final_df[\"why_version\"] = original_final[\"What made you prefer one version over the other?\"].apply(_normalize_preference_category)\n",
    "final_df[\"explanations_helped\"] = original_final[\"Did the explanations help you understand why defects were ordered in that way?\"].apply(_normalize_explanations_category)\n",
    "final_df[\"ordering_effect\"] = original_final[\"Did the ordering affect how you would approach fixing the code?\"].apply(_normalize_fix_category)\n",
    "final_df[\"confused_tasks\"] = original_final[\"Were any tasks confusing?\"].apply(_normalize_understood)\n",
    "final_df['confused_defects'] = original_final[\"Did you understand all the defects?\"].apply(_normalize_understood)\n",
    "final_df[\"comments\"] = original_final[\"Do you have any additional comments?\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_submission_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Preference for model vs baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Overall version preference ===\")\n",
    "print(per_submission_df[\"prefer_model\"].value_counts(dropna=False) / len(per_submission_df), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Which version helps decide what to fix first ===\")\n",
    "print(per_submission_df[\"helps_first\"].value_counts(dropna=False) / len(per_submission_df), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Which version ranked the first defect better ===\")\n",
    "print(per_submission_df[\"top_defect\"].value_counts(dropna=False) / len(per_submission_df), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance of preference\n",
    "\n",
    "TODO take into account confidence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model vs Baseline ---\n",
    "\n",
    "# Filter to hard choices\n",
    "binary_pref = per_submission_df[per_submission_df[\"prefer_model\"].isin([\"model\", \"baseline\"])]\n",
    "\n",
    "n_model = (binary_pref[\"prefer_model\"] == \"model\").sum()\n",
    "n_total = len(binary_pref)\n",
    "\n",
    "binom_result = binomtest(n_model, n_total, p=0.5, alternative='two-sided')\n",
    "\n",
    "print(\"=== Binomial Test (model vs baseline only) ===\")\n",
    "print(f\"Model chosen: {n_model}/{n_total}\")\n",
    "print(\"p-value:\", binom_result.pvalue, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model vs Baseline vs Neutral ---\n",
    "\n",
    "obs = per_submission_df[\"prefer_model\"].value_counts().reindex([\"model\", \"baseline\", \"neutral\"], fill_value=0)\n",
    "exp = [len(per_submission_df)/3]*3\n",
    "\n",
    "chi_result = chisquare(f_obs=obs, f_exp=exp)\n",
    "\n",
    "print(\"=== Chi-square Test (model, baseline, neutral) ===\")\n",
    "print(\"Observed:\", obs.values)\n",
    "print(\"Expected:\", exp)\n",
    "print(\"p-value:\", chi_result.pvalue, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Does left/right placement affect preference? ---\n",
    "\n",
    "n_model = (per_submission_df[\"prefer_model\"] == \"model\").sum()\n",
    "n_total = (per_submission_df[\"prefer_model\"].isin([\"model\", \"baseline\"])).sum()\n",
    "\n",
    "test = binomtest(n_model, n_total, p=0.5, alternative='two-sided')\n",
    "print(\"p =\", test.pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistency across questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agreement_df = per_submission_df[[\"prefer_model\", \"helps_first\", \"top_defect\"]]\n",
    "\n",
    "full_agreement = (agreement_df.nunique(axis=1) == 1).mean()\n",
    "pair_agreements = {\n",
    "    \"prefer vs helps\": (per_submission_df[\"prefer_model\"] == per_submission_df[\"helps_first\"]).mean(),\n",
    "    \"prefer vs top\":   (per_submission_df[\"prefer_model\"] == per_submission_df[\"top_defect\"]).mean(),\n",
    "    \"helps vs top\":    (per_submission_df[\"helps_first\"] == per_submission_df[\"top_defect\"]).mean(),\n",
    "}\n",
    "\n",
    "print(\"=== Agreement Across Related Questions ===\")\n",
    "print(f\"Full agreement across all 3: {full_agreement:.2f}\\n\")\n",
    "\n",
    "for name, val in pair_agreements.items():\n",
    "    print(f\"{name}: {val:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preference across tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_pref = (\n",
    "    per_submission_df\n",
    "    .groupby(\"question_id\")[\"prefer_model\"]\n",
    "    .value_counts(normalize=True)\n",
    "    .unstack()\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "task_pref.sort_values(by=[\"model\", \"neutral\"], ascending=False, inplace=True)\n",
    "\n",
    "task_pref.plot(\n",
    "    kind=\"bar\",\n",
    "    stacked=True,\n",
    "    figsize=(10,5),\n",
    "    colormap=\"viridis\"\n",
    ")\n",
    "\n",
    "plt.title(\"Preference Proportions per Task (sorted by model preference)\")\n",
    "plt.xlabel(\"Task (question_id)\")\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.legend(title=\"Preferred version\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preference across students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_pref = (\n",
    "    per_submission_df.groupby(\"student_id\")[\"prefer_model\"]\n",
    "    .value_counts()\n",
    "    .unstack()\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "student_pref[\"total\"] = student_pref.sum(axis=1)\n",
    "student_pref[\"dominant_score\"] = student_pref[[\"model\", \"baseline\"]].max(axis=1) / student_pref[\"total\"]\n",
    "student_pref[\"dominant_preference\"] = student_pref[[\"model\", \"baseline\"]].idxmax(axis=1)\n",
    "\n",
    "display(student_pref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(student_pref[\"dominant_score\"], bins=10)\n",
    "plt.title(\"How Consistent Were Students In Their Preferences?\")\n",
    "plt.xlabel(\"Proportion of times student chose their dominant version\")\n",
    "plt.ylabel(\"Number of students\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preference distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "sns.countplot(data=per_submission_df, x=\"prefer_model\",\n",
    "              order=[\"model\", \"baseline\", \"neutral\"])\n",
    "plt.title(\"Version Preference Distribution\")\n",
    "plt.xlabel(\"Preferred Version\")\n",
    "plt.ylabel(\"Number of responses\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.boxplot(data=per_submission_df,\n",
    "            x=\"prefer_model\",\n",
    "            y=\"confidence\",\n",
    "            order=[\"model\", \"baseline\", \"neutral\"])\n",
    "plt.title(\"Confidence vs Preferred Version\")\n",
    "plt.xlabel(\"Preferred Version\")\n",
    "plt.ylabel(\"Confidence (1–7)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usefulness for fixing (Which defect to fix first?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Which version helps decide what to fix first? ===\\n\")\n",
    "\n",
    "help_prop = per_submission_df[\"helps_first\"].value_counts(normalize=True)\n",
    "\n",
    "print(\"Proportions:\\n\", help_prop, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_help = per_submission_df[per_submission_df[\"helps_first\"].isin([\"model\", \"baseline\"])]\n",
    "\n",
    "n_model = (binary_help[\"helps_first\"] == \"model\").sum()\n",
    "n_total = len(binary_help)\n",
    "\n",
    "binom_result = binomtest(n_model, n_total, p=0.5, alternative='two-sided')\n",
    "\n",
    "print(\"=== Binomial Test (usefulness: model vs baseline) ===\")\n",
    "print(f\"Model chosen as more helpful: {n_model}/{n_total}\")\n",
    "print(\"p-value:\", binom_result.pvalue, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = per_submission_df[\"helps_first\"].value_counts().reindex(\n",
    "    [\"model\", \"baseline\", \"neutral\"], fill_value=0\n",
    ")\n",
    "\n",
    "expected = [len(per_submission_df)/3]*3\n",
    "\n",
    "chi_result = chisquare(f_obs=obs.values, f_exp=expected)\n",
    "\n",
    "print(\"=== Chi-square Test (usefulness: model/baseline/neutral) ===\")\n",
    "print(\"Observed:\", obs.values)\n",
    "print(\"Expected:\", expected)\n",
    "print(\"p-value:\", chi_result.pvalue, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship with preference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agreement = (per_submission_df[\"prefer_model\"] == per_submission_df[\"helps_first\"]).mean()\n",
    "\n",
    "print(\"=== Alignment of Preference with Helpfulness ===\")\n",
    "print(f\"Proportion where preference == usefulness: {agreement:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "sns.countplot(\n",
    "    data=per_submission_df,\n",
    "    x=\"helps_first\",\n",
    "    order=[\"model\", \"baseline\", \"neutral\"]\n",
    ")\n",
    "plt.title(\"Which Version Helps Decide What to Fix First?\")\n",
    "plt.xlabel(\"Version chosen as more helpful\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.boxplot(\n",
    "    data=per_submission_df,\n",
    "    x=\"helps_first\",\n",
    "    y=\"confidence\",\n",
    "    order=[\"model\", \"baseline\", \"neutral\"]\n",
    ")\n",
    "plt.title(\"Confidence vs Helpful Version Choice\")\n",
    "plt.xlabel(\"Helpful Version\")\n",
    "plt.ylabel(\"Confidence (1–7)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Confidence by helpful version ===\\n\")\n",
    "\n",
    "conf_stats = per_submission_df.groupby(\"helps_first\")[\"confidence\"].describe()\n",
    "display(conf_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- All three groups ---\n",
    "groups = [\n",
    "    per_submission_df.loc[per_submission_df[\"helps_first\"] == \"model\", \"confidence\"].dropna(),\n",
    "    per_submission_df.loc[per_submission_df[\"helps_first\"] == \"baseline\", \"confidence\"].dropna(),\n",
    "    per_submission_df.loc[per_submission_df[\"helps_first\"] == \"neutral\", \"confidence\"].dropna(),\n",
    "]\n",
    "\n",
    "kw = kruskal(*groups)\n",
    "\n",
    "print(\"=== Kruskal-Wallis test: confidence differences across groups ===\")\n",
    "print(\"p-value:\", kw.pvalue, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model vs baseline ---\n",
    "\n",
    "conf_model = per_submission_df.loc[per_submission_df[\"helps_first\"] == \"model\", \"confidence\"]\n",
    "conf_base = per_submission_df.loc[per_submission_df[\"helps_first\"] == \"baseline\", \"confidence\"]\n",
    "\n",
    "u = mannwhitneyu(conf_model, conf_base, alternative=\"two-sided\")\n",
    "\n",
    "print(\"=== Mann-Whitney U test: model vs baseline confidence ===\")\n",
    "print(\"p-value:\", u.pvalue, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.boxplot(\n",
    "    data=per_submission_df,\n",
    "    x=\"helps_first\",\n",
    "    y=\"confidence\",\n",
    "    order=[\"model\", \"baseline\", \"neutral\"]\n",
    ")\n",
    "plt.title(\"Confidence vs Helpful Version Chosen\")\n",
    "plt.xlabel(\"Version chosen as more helpful\")\n",
    "plt.ylabel(\"Confidence (1–7)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Did explanations help students understand the ranking?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Did explanations help? ===\\n\")\n",
    "print(final_df[\"explanations_helped\"].value_counts(), \"\\n\")\n",
    "\n",
    "explanation_stats = final_df[\"explanations_helped\"].value_counts(normalize=True) * 100\n",
    "print(\"Percentage breakdown:\\n\", explanation_stats.round(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(data=final_df, x=\"explanations_helped\",\n",
    "              order=[\"no\", \"somewhat\", \"yes\"])\n",
    "plt.title(\"Did Explanations Help?\")\n",
    "plt.xlabel(\"Student response\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Did the ordering affect how students would approach fixing the code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Did ordering affect how students would fix the code? ===\\n\")\n",
    "print(final_df[\"ordering_effect\"].value_counts(), \"\\n\")\n",
    "\n",
    "print(\"Percentage:\\n\",\n",
    "      (final_df[\"ordering_effect\"].value_counts(normalize=True) * 100).round(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_yes = (final_df[\"ordering_effect\"] == \"yes\").sum()\n",
    "n_total = len(final_df)\n",
    "\n",
    "binom = binomtest(n_yes, n_total, p=0.5, alternative=\"greater\")\n",
    "print(\"Binomial test p-value:\", binom.pvalue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(data=final_df, x=\"ordering_effect\",\n",
    "              order=[\"no\", \"somewhat\", \"yes\"])\n",
    "plt.title(\"Did Ordering Influence How Students Would Fix Code?\")\n",
    "plt.xlabel(\"Response\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Were any tasks/defects confusing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Were any tasks confusing? ===\\n\")\n",
    "print(final_df[\"confused_tasks\"].value_counts(), \"\\n\")\n",
    "\n",
    "print(\"=== Did you understand all the defects? ===\\n\")\n",
    "print(final_df[\"confused_defects\"].value_counts(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(data=final_df, x=\"confused_tasks\",\n",
    "              order=[\"no\", \"maybe\", \"yes\"])\n",
    "plt.title(\"Were Any Tasks Confusing?\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(data=final_df, x=\"confused_defects\",\n",
    "              order=[\"no\", \"maybe\", \"yes\"])\n",
    "plt.title(\"Did Students Understand All Defects?\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_pref = (\n",
    "    per_submission_df.groupby(\"student_id\")[\"prefer_model\"]\n",
    "    .value_counts(normalize=True)\n",
    "    .unstack()\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "student_pref[\"dominant_score\"] = student_pref.max(axis=1)\n",
    "student_pref[\"dominant_preference\"] = student_pref.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_summary = per_submission_df.groupby(\"question_id\").agg(\n",
    "    n_students = (\"prefer_model\", \"count\"),\n",
    "    prefer_counts = (\"prefer_model\", lambda x: x.value_counts().to_dict()),\n",
    "    helps_counts = (\"helps_first\", lambda x: x.value_counts().to_dict()),\n",
    "    top_counts = (\"top_defect\", lambda x: x.value_counts().to_dict()),\n",
    "    mean_confidence = (\"confidence\", \"mean\")\n",
    ").reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "sns.boxplot(data=per_submission_df, x=\"question_id\", y=\"confidence\")\n",
    "plt.title(\"Confidence per task\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Left/Right influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute proportions\n",
    "prop_df = (\n",
    "    per_submission_df\n",
    "    .groupby(\"left_version\")[\"prefer_model\"]\n",
    "    .value_counts(normalize=True)\n",
    "    .rename(\"fraction_chosen\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(\n",
    "    data=prop_df,\n",
    "    x=\"left_version\",\n",
    "    y=\"fraction_chosen\",\n",
    "    hue=\"prefer_model\"\n",
    ")\n",
    "plt.title(\"Does left/right placement affect preference?\")\n",
    "plt.xlabel(\"Version shown on the left (model or baseline)\")\n",
    "plt.ylabel(\"Fraction chosen\")\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl = pd.crosstab(per_submission_df[\"left_version\"], per_submission_df[\"prefer_model\"])\n",
    "chi2, p, dof, expected = stats.chi2_contingency(tbl)\n",
    "\n",
    "print(tbl)\n",
    "print(f\"\\nChi-square p-value: {p:.4f} (reject = significant difference based on left/right placement)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
