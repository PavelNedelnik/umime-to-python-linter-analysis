{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "from src.prioritization import *\n",
    "\n",
    "# input data\n",
    "VERSION = '0.0.0'\n",
    "DATASET_PATH = Path('data') / 'datasets' / f'ipython_{VERSION}'\n",
    "MODEL_PATH = DATASET_PATH / 'trained_heuristics'\n",
    "\n",
    "DATA_PARTITION = 'hold_out'\n",
    "\n",
    "# output data\n",
    "SURVEY_DATA_PATH = Path('labelling_app') / 'survey_data' / f'ipython_{VERSION}'\n",
    "MODELLING_DATA_PATH = DATASET_PATH / 'hold_out_prioritization_scores'\n",
    "\n",
    "# config\n",
    "RANDOM_SEED = 42\n",
    "SAMPLE_SIZE = 70\n",
    "RESOLUTION = 300\n",
    "DEBUG = True\n",
    "\n",
    "# images\n",
    "IMAGE_DIR = Path('images') / \"survey_preparation\"\n",
    "\n",
    "os.makedirs(SURVEY_DATA_PATH, exist_ok=True)\n",
    "os.makedirs(MODELLING_DATA_PATH, exist_ok=True)\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv(DATASET_PATH / 'items.csv', index_col=0)\n",
    "defects = pd.read_csv(DATASET_PATH / f'defects.csv', index_col=0)\n",
    "\n",
    "log = pd.read_csv(DATASET_PATH / DATA_PARTITION/ 'log.csv', index_col=0, parse_dates=['time'])\n",
    "defect_log = pd.read_csv(DATASET_PATH / DATA_PARTITION / 'defect_log.csv', index_col=0)\n",
    "defect_log.columns = defect_log.columns.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    TaskCommonModel,\n",
    "    TaskCharacteristicModel,\n",
    "    StudentFrequencyModel,\n",
    "    StudentCharacteristicModel,\n",
    "    StudentEncounteredBeforeModel,\n",
    "    DefectMultiplicityModel,\n",
    "    SeverityModel,\n",
    "]\n",
    "\n",
    "models = [model.load(MODEL_PATH / f'{model.get_model_name()}.pkl') for model in models]\n",
    "\n",
    "models = {model.get_model_name(): model for model in models}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Quality filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_close_pair(row):\n",
    "    \"\"\"Check if there is a pair of values with difference two or less.\"\"\"\n",
    "    row_values = row.values\n",
    "    return np.any(np.abs(row_values[:, None] - row_values) <= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at least two defects\n",
    "filtered = defect_log[(defect_log > 0).sum(axis=1) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at most difference of two in severity\n",
    "filtered *= defects.loc[filtered.columns]['severity']\n",
    "filtered = filtered[filtered.apply(has_close_pair, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the filter\n",
    "defect_log = defect_log.loc[filtered.index]\n",
    "log = log.loc[filtered.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Sampling criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These defects will not be used to calculate the sampling statistics\n",
    "IGNORE_FOR_CALCULATION = defects[defects['defect name'] == 'inappropriate whitespace: visible'].index\n",
    "\n",
    "# Calculate sampling statistics and remember heuristic scores\n",
    "stats = []\n",
    "discrete_scores = []\n",
    "continuous_scores = []\n",
    "\n",
    "# Ensure log is sorted by time\n",
    "log = log.sort_values(by='time')\n",
    "\n",
    "for idx, submission in tqdm(log.iloc[:200].iterrows() if DEBUG else log.iterrows(), total=log.shape[0], desc=\"Calculating statistics for sampling\"):\n",
    "    defect_counts = defect_log.loc[idx]\n",
    "\n",
    "    # Heuristic scores\n",
    "    discrete_model_scores = {name: model.discretize(submission, defect_counts).dropna() for name, model in models.items()}\n",
    "    continuous_model_scores = {name: model._calculate_scores(submission, defect_counts) for name, model in models.items()}\n",
    "\n",
    "    # Save model scores\n",
    "    for defect in defect_counts[defect_counts > 0].index:\n",
    "        discrete_row = {\"submission id\": idx, \"defect id\": defect}\n",
    "        continuous_row = discrete_row.copy()\n",
    "        for name, scores in discrete_model_scores.items():\n",
    "            discrete_row[name] = scores[defect]\n",
    "            continuous_row[name] = continuous_model_scores[name][defect]\n",
    "        discrete_scores.append(discrete_row)\n",
    "        continuous_scores.append(continuous_row)\n",
    "\n",
    "    # Ignore some defects\n",
    "    discrete_model_scores = {\n",
    "        name: scores.loc[~scores.index.isin(IGNORE_FOR_CALCULATION)]\n",
    "        for name, scores in discrete_model_scores.items()\n",
    "    }\n",
    "\n",
    "    # Disagreement\n",
    "    kendall_distances = []\n",
    "    for left, right in combinations(discrete_model_scores.values(), 2):\n",
    "        tau, _ = kendalltau(left, right, nan_policy='raise')\n",
    "        kendall_distances.append(1 - tau)\n",
    "\n",
    "    disagreement = np.nanmean(kendall_distances) if any(~np.isnan(kendall_distances)) else 0.0\n",
    "\n",
    "    # Signal strength\n",
    "    signal_strength = np.mean([np.ptp(values) for values in discrete_model_scores.values()])\n",
    "\n",
    "    for model in models.values():\n",
    "        model.update(submission, defect_counts)\n",
    "\n",
    "    stats.append({\n",
    "        \"id\": idx,\n",
    "        \"signal strength\": signal_strength,\n",
    "        \"model disagreement\": disagreement,\n",
    "    })\n",
    "\n",
    "stats = pd.DataFrame(stats).set_index(\"id\")\n",
    "discrete_scores = pd.DataFrame(discrete_scores)\n",
    "continuous_scores = pd.DataFrame(continuous_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Survey sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_score = 0.1\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "# Normalize\n",
    "scores = (stats - stats.min()) / (stats.max() - stats.min())\n",
    "\n",
    "# Combine scores\n",
    "scores['total'] = scores.sum(axis=1)\n",
    "scores['total'] = np.where(scores['total'] < min_score, min_score, scores['total'])\n",
    "\n",
    "# Stratify by topic\n",
    "strata_labels = items['topic'].unique()\n",
    "strata_counts = np.zeros(strata_labels.shape[0], dtype=int)\n",
    "scores['strata'] = log.loc[scores.index, 'item'].map(items['topic'])\n",
    "\n",
    "# Greedy sampling\n",
    "sampled_ids = []\n",
    "while len(sampled_ids) < SAMPLE_SIZE:\n",
    "    # Least represented strata\n",
    "    topic_idx = rng.choice(np.where(strata_counts == strata_counts.min())[0], size=1)\n",
    "    # Collect eligible submissions\n",
    "    eligible_ids = scores[scores['strata'] == strata_labels[topic_idx][0]].index.difference(sampled_ids)\n",
    "    # Normalize scores into probabilities\n",
    "    probabilities = scores.loc[eligible_ids, 'total'] / scores.loc[eligible_ids, 'total'].sum()\n",
    "    # Sample, skipping if there are no eligible submissions\n",
    "    try:\n",
    "        sampled_ids.append(rng.choice(eligible_ids, replace=False, p=probabilities.values))\n",
    "    except ValueError:\n",
    "        pass\n",
    "    strata_counts[topic_idx] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_stats = stats.loc[sampled_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "axes[0].hist(sample_stats[\"signal strength\"], bins=30, edgecolor=\"black\")\n",
    "axes[0].set_title(\"Distribution of Signal Strength\")\n",
    "axes[0].set_xlabel(\"Signal Strength\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "axes[1].hist(sample_stats[\"model disagreement\"], bins=30, edgecolor=\"black\")\n",
    "axes[1].set_title(\"Distribution of Model Disagreement\")\n",
    "axes[1].set_xlabel(\"Model Disagreement\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"criterion_distribution.png\", dpi=RESOLUTION)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "ax.scatter(\n",
    "    sample_stats[\"signal strength\"],\n",
    "    sample_stats[\"model disagreement\"],\n",
    "    alpha=0.6,\n",
    "    s=30,\n",
    "    c=log.loc[sample_stats.index, \"item\"].astype(\"category\").cat.codes,  # color by task\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Signal Strength\")\n",
    "ax.set_ylabel(\"Model Disagreement\")\n",
    "ax.set_title(\"Signal Strength vs Model Disagreement\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"signal_vs_disagreement.png\", dpi=RESOLUTION)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defect_counts = defect_log.loc[sample_stats.index].sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defect_counts = defect_log.loc[sample_stats.index].sum().sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.bar(defect_counts.index.astype(str), defect_counts.values)\n",
    "ax.set_xlabel(\"Defect ID\")\n",
    "ax.set_ylabel(\"Number of Submissions\")\n",
    "ax.set_title(\"Coverage of Defects in Hold-Out Partition\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"defect_coverage.png\", dpi=RESOLUTION)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_counts = log.loc[sample_stats.index, \"item\"].value_counts().reindex(items.index).sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.bar(task_counts.index.astype(str), task_counts.values)\n",
    "ax.set_xlabel(\"Task ID\")\n",
    "ax.set_ylabel(\"Number of Submissions\")\n",
    "ax.set_title(\"Coverage of Tasks in Hold-Out Partition\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"task_coverage.png\", dpi=RESOLUTION)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = items['topic'].unique()\n",
    "topic_counts = items.loc[log.loc[sample_stats.index, \"item\"], \"topic\"].value_counts().reindex(topics).sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.bar(topic_counts.index.astype(str), topic_counts.values)\n",
    "ax.set_xlabel(\"Topic\")\n",
    "ax.set_ylabel(\"Number of Submissions\")\n",
    "ax.set_title(\"Coverage of Topics in Hold-Out Partition\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"topic_coverage.png\", dpi=RESOLUTION)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    nrows=int(np.ceil(len(models.keys()) / 3)),\n",
    "    ncols=3,\n",
    "    figsize=(12, 3 * int(np.ceil(len(models.keys()) / 3))),\n",
    "    constrained_layout=True\n",
    ")\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, col in zip(axes, models.keys()):\n",
    "    has_1_5_scale = models[col].get_discretization_scale() == '1-5'\n",
    "    ax.hist(discrete_scores[col].dropna(), bins=np.arange(0.5, 6.5, 1) if has_1_5_scale else np.arange(-2.5, 3.5, 1), edgecolor=\"black\", rwidth=0.8)\n",
    "    ax.set_title(col)\n",
    "    ax.set_xticks([1, 2, 3, 4, 5] if has_1_5_scale else [-2, -1, 0, 1, 2])\n",
    "    ax.set_xlabel(\"Score\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "\n",
    "# Remove empty subplots\n",
    "for ax in axes[len(models.keys()):]:\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Distribution of Heuristic Scores (All Defects)\", fontsize=14, y=1.02)\n",
    "plt.savefig(IMAGE_DIR / \"heuristic_value_distributions.png\", dpi=RESOLUTION, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storytelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High and low agreement / signal strength examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Combine for the survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Submissions ---\n",
    "submission_df = (\n",
    "    log.loc[sampled_ids]\n",
    "    .merge(items[['name', 'instructions']], left_on='item', right_index=True)\n",
    "    .rename(columns={\n",
    "        'answer': 'submission',\n",
    "        'name': 'task name',\n",
    "    })\n",
    "    [['submission', 'task name', 'instructions']]\n",
    ")\n",
    "\n",
    "# --- Defects ---\n",
    "# Long format\n",
    "melted_defects = (\n",
    "    defect_log.loc[sampled_ids]\n",
    "    .stack()\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'submission id', 'level_1': 'defect id', 0: 'count'})\n",
    ")\n",
    "melted_defects = melted_defects[melted_defects['count'] > 0]\n",
    "\n",
    "# Add metadata\n",
    "defect_df = (\n",
    "    melted_defects\n",
    "    .merge(defects, left_on='defect id', right_index=True)\n",
    "    [['submission id', 'defect id', 'defect name', 'description', 'code example', 'code fix example']]\n",
    "    .rename(columns={'defect name': 'name'})\n",
    ")\n",
    "\n",
    "# Add heuristic scores\n",
    "defect_df = defect_df.merge(\n",
    "    discrete_scores,\n",
    "    on=['submission id', 'defect id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# --- Heuristics ---\n",
    "heuristics_df = []\n",
    "for name, model in models.items():\n",
    "    heuristics_df.append({\n",
    "        'name': name,\n",
    "        'description': model.get_model_description(),\n",
    "        'scale': model.get_discretization_scale(),\n",
    "        'interpretation': model.get_model_interpretation()\n",
    "    })\n",
    "\n",
    "heuristics_df = pd.DataFrame(heuristics_df).set_index('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv(SURVEY_DATA_PATH / 'submissions.csv', sep=';', index_label='index')\n",
    "defect_df.to_csv(SURVEY_DATA_PATH / 'defects.csv', sep=';', index_label='index')\n",
    "heuristics_df.to_csv(SURVEY_DATA_PATH / 'heuristics.csv', sep=';', index_label='name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Combine for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_scores.to_csv(MODELLING_DATA_PATH / 'discrete_scores.csv', sep=';', index_label='index')\n",
    "continuous_scores.to_csv(MODELLING_DATA_PATH / 'continuous_scores.csv', sep=';', index_label='index')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
