{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "from src.prioritization import *\n",
    "\n",
    "VERSION = '0.0.0'\n",
    "DATASET_PATH = Path('data') / 'datasets' / f'ipython_{VERSION}'\n",
    "MODEL_PATH = DATASET_PATH / 'trained_heuristics'    \n",
    "\n",
    "DATA_PARTITION = 'hold_out'\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "SAMPLE_SIZE = 70\n",
    "\n",
    "RESOLUTION = 300\n",
    "\n",
    "IMAGE_DIR = Path('images') / \"heuristics\"\n",
    "\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv(DATASET_PATH / 'items.csv', index_col=0)\n",
    "defects = pd.read_csv(DATASET_PATH / f'defects.csv', index_col=0)\n",
    "\n",
    "log = pd.read_csv(DATASET_PATH / DATA_PARTITION/ 'log.csv', index_col=0, parse_dates=['time'])\n",
    "defect_log = pd.read_csv(DATASET_PATH / DATA_PARTITION / 'defect_log.csv', index_col=0)\n",
    "defect_log.columns = defect_log.columns.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Task Common\": TaskCommonModel,\n",
    "    \"Task Characteristic\": TaskCharacteristicModel,\n",
    "    \"Student Frequency\": StudentFrequencyModel,\n",
    "    \"Student Characteristic\": StudentCharacteristicModel,\n",
    "    \"Student Encountered\": StudentEncounteredBeforeModel,\n",
    "    \"Defect Multiplicity\": DefectMultiplicityModel,\n",
    "    \"Severity Baseline\": SeverityModel,\n",
    "}\n",
    "\n",
    "models = {\n",
    "    name: model.load(MODEL_PATH / f'{name}.pkl')\n",
    "    for name, model in models.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Quality filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_close_pair(row):\n",
    "    \"\"\"Check if there is a pair of values with difference two or less.\"\"\"\n",
    "    row_values = row.values\n",
    "    return np.any(np.abs(row_values[:, None] - row_values) <= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at least two defects\n",
    "filtered = defect_log[defect_log.sum(axis=1) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at most difference of one in severity\n",
    "filtered *= defects.loc[filtered.columns]['severity']\n",
    "filtered = filtered[filtered.apply(has_close_pair, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the filter\n",
    "defect_log = defect_log.loc[filtered.index]\n",
    "log = log.loc[filtered.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Sampling criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sampling statistics and remember heuristic scores\n",
    "stats = []\n",
    "heuristic_scores = []\n",
    "for idx, submission in tqdm(log.iloc[:200].iterrows(), total=log.shape[0], desc=\"Calculating statistics for sampling\"):\n",
    "    defect_counts = defect_log.loc[idx]\n",
    "\n",
    "    # Heuristic scores\n",
    "    discrete_scores = {name: model.discretize(submission, defect_counts).dropna() for name, model in models.items()}\n",
    "\n",
    "    for defect in defect_counts[defect_counts > 0].index:\n",
    "        row = {\"submission id\": idx, \"defect id\": defect}\n",
    "        for name, scores in discrete_scores.items():\n",
    "            row[name] = scores[defect]\n",
    "        heuristic_scores.append(row)\n",
    "\n",
    "    # Disagreement\n",
    "    kendall_distances = []\n",
    "    for left, right in combinations(discrete_scores.values(), 2):\n",
    "        tau, _ = kendalltau(left, right, nan_policy='raise')\n",
    "        kendall_distances.append(1 - tau)\n",
    "\n",
    "    disagreement = np.nanmean(kendall_distances) if any(~np.isnan(kendall_distances)) else 0.0\n",
    "\n",
    "    # Signal strength\n",
    "    signal_strength = np.mean([np.ptp(values) for values in discrete_scores.values()])\n",
    "\n",
    "    for model in models.values():\n",
    "        model.update(submission, defect_counts)\n",
    "\n",
    "    stats.append({\n",
    "        \"id\": idx,\n",
    "        \"signal strength\": signal_strength,\n",
    "        \"model disagreement\": disagreement,\n",
    "    })\n",
    "\n",
    "stats = pd.DataFrame(stats).set_index(\"id\")\n",
    "heuristic_scores = pd.DataFrame(heuristic_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Survey sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_score = 0.1\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "# Normalize\n",
    "scores = (stats - stats.min()) / (stats.max() - stats.min())\n",
    "\n",
    "# Combine scores\n",
    "scores['total'] = scores.sum(axis=1)\n",
    "scores['total'] = np.where(scores['total'] < min_score, min_score, scores['total'])\n",
    "\n",
    "# Scale\n",
    "scores['probability'] = scores['total'] / scores['total'].sum()\n",
    "\n",
    "# Sample without replacement\n",
    "sampled_ids = rng.choice(scores.index, size=min(SAMPLE_SIZE, len(scores)), replace=False, p=scores['probability'].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Combine to a final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Submissions ---\n",
    "submission_df = (\n",
    "    log.loc[sampled_ids]\n",
    "    .merge(items[['name', 'instructions']], left_on='item', right_index=True)\n",
    "    .rename(columns={\n",
    "        'answer': 'submission',\n",
    "        'name': 'task name',\n",
    "    })\n",
    "    [['submission', 'task name', 'instructions']]\n",
    ")\n",
    "\n",
    "# --- Defects ---\n",
    "# Long format\n",
    "melted_defects = (\n",
    "    defect_log.loc[sampled_ids]\n",
    "    .stack()\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'submission id', 'level_1': 'defect id', 0: 'count'})\n",
    ")\n",
    "melted_defects = melted_defects[melted_defects['count'] > 0]\n",
    "\n",
    "# Add metadata\n",
    "defect_df = (\n",
    "    melted_defects\n",
    "    .merge(defects, left_on='defect id', right_index=True)\n",
    "    [['submission id', 'defect id', 'defect name', 'description', 'code example', 'code fix example']]\n",
    "    .rename(columns={'defect name': 'name'})\n",
    ")\n",
    "\n",
    "# Add heuristic scores\n",
    "defect_df = defect_df.merge(\n",
    "    heuristic_scores,\n",
    "    on=['submission id', 'defect id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# --- Heuristics ---\n",
    "heuristics_df = []\n",
    "for name, model in models.items():\n",
    "    heuristics_df.append({\n",
    "        'name': name,\n",
    "        'description': model.get_model_description(),\n",
    "        'scale': model.get_discretization_scale(),\n",
    "    })\n",
    "\n",
    "heuristics_df = pd.DataFrame(heuristics_df).set_index('name')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
