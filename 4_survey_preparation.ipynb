{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "from src.prioritization import *\n",
    "\n",
    "VERSION = '0.0.0'\n",
    "DATASET_PATH = Path('data') / 'datasets' / f'ipython_{VERSION}'\n",
    "MODEL_PATH = DATASET_PATH / 'trained_heuristics'\n",
    "OUTPUT_PATH = Path('labelling_app') / 'survey_data' / f'ipython_{VERSION}'\n",
    "\n",
    "DATA_PARTITION = 'hold_out'\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "SAMPLE_SIZE = 70\n",
    "\n",
    "RESOLUTION = 300\n",
    "\n",
    "IMAGE_DIR = Path('images') / \"survey_preparation\"\n",
    "\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv(DATASET_PATH / 'items.csv', index_col=0)\n",
    "defects = pd.read_csv(DATASET_PATH / f'defects.csv', index_col=0)\n",
    "\n",
    "log = pd.read_csv(DATASET_PATH / DATA_PARTITION/ 'log.csv', index_col=0, parse_dates=['time'])\n",
    "defect_log = pd.read_csv(DATASET_PATH / DATA_PARTITION / 'defect_log.csv', index_col=0)\n",
    "defect_log.columns = defect_log.columns.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Task Common\": TaskCommonModel,\n",
    "    \"Task Characteristic\": TaskCharacteristicModel,\n",
    "    \"Student Frequency\": StudentFrequencyModel,\n",
    "    \"Student Characteristic\": StudentCharacteristicModel,\n",
    "    \"Student Encountered\": StudentEncounteredBeforeModel,\n",
    "    \"Defect Multiplicity\": DefectMultiplicityModel,\n",
    "    \"Severity Baseline\": SeverityModel,\n",
    "}\n",
    "\n",
    "models = {\n",
    "    name: model.load(MODEL_PATH / f'{name}.pkl')\n",
    "    for name, model in models.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Quality filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_close_pair(row):\n",
    "    \"\"\"Check if there is a pair of values with difference two or less.\"\"\"\n",
    "    row_values = row.values\n",
    "    return np.any(np.abs(row_values[:, None] - row_values) <= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at least two defects\n",
    "filtered = defect_log[(defect_log > 0).sum(axis=1) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at most difference of two in severity\n",
    "filtered *= defects.loc[filtered.columns]['severity']\n",
    "filtered = filtered[filtered.apply(has_close_pair, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the filter\n",
    "defect_log = defect_log.loc[filtered.index]\n",
    "log = log.loc[filtered.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Sampling criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sampling statistics and remember heuristic scores\n",
    "stats = []\n",
    "heuristic_scores = []\n",
    "for idx, submission in tqdm(log.iloc[:200].iterrows(), total=log.shape[0], desc=\"Calculating statistics for sampling\"):\n",
    "    defect_counts = defect_log.loc[idx]\n",
    "\n",
    "    # Heuristic scores\n",
    "    discrete_scores = {name: model.discretize(submission, defect_counts).dropna() for name, model in models.items()}\n",
    "\n",
    "    for defect in defect_counts[defect_counts > 0].index:\n",
    "        row = {\"submission id\": idx, \"defect id\": defect}\n",
    "        for name, scores in discrete_scores.items():\n",
    "            row[name] = scores[defect]\n",
    "        heuristic_scores.append(row)\n",
    "\n",
    "    # Disagreement\n",
    "    kendall_distances = []\n",
    "    for left, right in combinations(discrete_scores.values(), 2):\n",
    "        tau, _ = kendalltau(left, right, nan_policy='raise')\n",
    "        kendall_distances.append(1 - tau)\n",
    "\n",
    "    disagreement = np.nanmean(kendall_distances) if any(~np.isnan(kendall_distances)) else 0.0\n",
    "\n",
    "    # Signal strength\n",
    "    signal_strength = np.mean([np.ptp(values) for values in discrete_scores.values()])\n",
    "\n",
    "    for model in models.values():\n",
    "        model.update(submission, defect_counts)\n",
    "\n",
    "    stats.append({\n",
    "        \"id\": idx,\n",
    "        \"signal strength\": signal_strength,\n",
    "        \"model disagreement\": disagreement,\n",
    "    })\n",
    "\n",
    "stats = pd.DataFrame(stats).set_index(\"id\")\n",
    "heuristic_scores = pd.DataFrame(heuristic_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Survey sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_score = 0.1\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "# Normalize\n",
    "scores = (stats - stats.min()) / (stats.max() - stats.min())\n",
    "\n",
    "# Combine scores\n",
    "scores['total'] = scores.sum(axis=1)\n",
    "scores['total'] = np.where(scores['total'] < min_score, min_score, scores['total'])\n",
    "\n",
    "# Stratify by topic\n",
    "strata_labels = items['topic'].unique()\n",
    "strata_counts = np.zeros(strata_labels.shape[0], dtype=int)\n",
    "scores['strata'] = log.loc[scores.index, 'item'].map(items['topic'])\n",
    "\n",
    "# Greedy sampling\n",
    "sampled_ids = []\n",
    "while len(sampled_ids) < SAMPLE_SIZE:\n",
    "    # Least represented strata\n",
    "    topic_idx = rng.choice(np.where(strata_counts == strata_counts.min())[0], size=1)\n",
    "    # Collect eligible submissions\n",
    "    eligible_ids = scores[scores['strata'] == strata_labels[topic_idx][0]].index.difference(sampled_ids)\n",
    "    # Normalize scores into probabilities\n",
    "    probabilities = scores.loc[eligible_ids, 'total'] / scores.loc[eligible_ids, 'total'].sum()\n",
    "    # Sample, skipping if there are no eligible submissions\n",
    "    try:\n",
    "        sampled_ids.append(rng.choice(eligible_ids, replace=False, p=probabilities.values))\n",
    "    except ValueError:\n",
    "        pass\n",
    "    strata_counts[topic_idx] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_stats = stats.loc[sampled_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "axes[0].hist(sample_stats[\"signal strength\"], bins=30, edgecolor=\"black\")\n",
    "axes[0].set_title(\"Distribution of Signal Strength\")\n",
    "axes[0].set_xlabel(\"Signal Strength\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "axes[1].hist(sample_stats[\"model disagreement\"], bins=30, edgecolor=\"black\")\n",
    "axes[1].set_title(\"Distribution of Model Disagreement\")\n",
    "axes[1].set_xlabel(\"Model Disagreement\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"criterion_distribution.png\", dpi=RESOLUTION)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "ax.scatter(\n",
    "    sample_stats[\"signal strength\"],\n",
    "    sample_stats[\"model disagreement\"],\n",
    "    alpha=0.6,\n",
    "    s=30,\n",
    "    c=log.loc[sample_stats.index, \"item\"].astype(\"category\").cat.codes,  # color by task\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Signal Strength\")\n",
    "ax.set_ylabel(\"Model Disagreement\")\n",
    "ax.set_title(\"Signal Strength vs Model Disagreement\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"signal_vs_disagreement.png\", dpi=RESOLUTION)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_counts = log.loc[sample_stats.index, \"item\"].value_counts().reindex(items.index).sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.bar(task_counts.index.astype(str), task_counts.values)\n",
    "ax.set_xlabel(\"Task ID\")\n",
    "ax.set_ylabel(\"Number of Submissions\")\n",
    "ax.set_title(\"Coverage of Tasks in Hold-Out Partition\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"task_coverage.png\", dpi=RESOLUTION)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = items['topic'].unique()\n",
    "topic_counts = items.loc[log.loc[sample_stats.index, \"item\"], \"topic\"].value_counts().reindex(topics).sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.bar(topic_counts.index.astype(str), topic_counts.values)\n",
    "ax.set_xlabel(\"Topic\")\n",
    "ax.set_ylabel(\"Number of Submissions\")\n",
    "ax.set_title(\"Coverage of Topics in Hold-Out Partition\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"topic_coverage.png\", dpi=RESOLUTION)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    nrows=int(np.ceil(len(models.keys()) / 3)),\n",
    "    ncols=3,\n",
    "    figsize=(12, 3 * int(np.ceil(len(models.keys()) / 3))),\n",
    "    constrained_layout=True\n",
    ")\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, col in zip(axes, models.keys()):\n",
    "    has_1_5_scale = models[col].get_discretization_scale() == '1-5'\n",
    "    ax.hist(heuristic_scores[col].dropna(), bins=np.arange(0.5, 6.5, 1) if has_1_5_scale else np.arange(-2.5, 3.5, 1), edgecolor=\"black\", rwidth=0.8)\n",
    "    ax.set_title(col)\n",
    "    ax.set_xticks([1, 2, 3, 4, 5] if has_1_5_scale else [-2, -1, 0, 1, 2])\n",
    "    ax.set_xlabel(\"Score\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "\n",
    "# Remove empty subplots\n",
    "for ax in axes[len(models.keys()):]:\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Distribution of Heuristic Scores (All Defects)\", fontsize=14, y=1.02)\n",
    "plt.savefig(IMAGE_DIR / \"heuristic_value_distributions.png\", dpi=RESOLUTION, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storytelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High and low agreement / signal strength examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Combine to a final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Submissions ---\n",
    "submission_df = (\n",
    "    log.loc[sampled_ids]\n",
    "    .merge(items[['name', 'instructions']], left_on='item', right_index=True)\n",
    "    .rename(columns={\n",
    "        'answer': 'submission',\n",
    "        'name': 'task name',\n",
    "    })\n",
    "    [['submission', 'task name', 'instructions']]\n",
    ")\n",
    "\n",
    "# --- Defects ---\n",
    "# Long format\n",
    "melted_defects = (\n",
    "    defect_log.loc[sampled_ids]\n",
    "    .stack()\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'submission id', 'level_1': 'defect id', 0: 'count'})\n",
    ")\n",
    "melted_defects = melted_defects[melted_defects['count'] > 0]\n",
    "\n",
    "# Add metadata\n",
    "defect_df = (\n",
    "    melted_defects\n",
    "    .merge(defects, left_on='defect id', right_index=True)\n",
    "    [['submission id', 'defect id', 'defect name', 'description', 'code example', 'code fix example']]\n",
    "    .rename(columns={'defect name': 'name'})\n",
    ")\n",
    "\n",
    "# Add heuristic scores\n",
    "defect_df = defect_df.merge(\n",
    "    heuristic_scores,\n",
    "    on=['submission id', 'defect id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# --- Heuristics ---\n",
    "heuristics_df = []\n",
    "for name, model in models.items():\n",
    "    heuristics_df.append({\n",
    "        'name': name,\n",
    "        'description': model.get_model_description(),\n",
    "        'scale': model.get_discretization_scale(),\n",
    "    })\n",
    "\n",
    "heuristics_df = pd.DataFrame(heuristics_df).set_index('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv(OUTPUT_PATH / 'submissions.csv', sep=';', index_label='index')\n",
    "defect_df.to_csv(OUTPUT_PATH / 'defects.csv', sep=';', index_label='index')\n",
    "heuristics_df.to_csv(OUTPUT_PATH / 'heuristics.csv', sep=';', index_label='name')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
