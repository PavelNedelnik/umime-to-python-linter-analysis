{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.prioritization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CONFIG_ENV\"] = \"debug\"\n",
    "\n",
    "from config import load_config\n",
    "config = load_config()\n",
    "\n",
    "RESOLUTION = config['DEFAULTS']['resolution']\n",
    "\n",
    "# input data\n",
    "HOLD_OUT_DATA_PATH = config['PATHS']['hold_out_set']\n",
    "STORAGE_PATH = config['PATHS']['storage']\n",
    "RAW_SURVEY_RESPONSES_PATH = config['PATHS']['raw_survey_responses']\n",
    "RAW_SURVEY_FEEDBACK_PATH = config['PATHS']['raw_survey_feedback']\n",
    "CACHED_PRIORITIZATIONS_PATH = config['PATHS']['hold_out_prioritizations']\n",
    "\n",
    "# output data\n",
    "BENCHMARK_OUTPUT_PATH = config['PATHS']['benchmark_dataset']\n",
    "IMAGE_DIR = config['PATHS']['images']\n",
    "\n",
    "os.makedirs(BENCHMARK_OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv(STORAGE_PATH / 'items.csv', index_col=0)\n",
    "defects = pd.read_csv(STORAGE_PATH / 'defects.csv', index_col=0)\n",
    "\n",
    "responses = pd.read_csv(RAW_SURVEY_RESPONSES_PATH, sep=';', parse_dates=['timestamp'])\n",
    "feedback = pd.read_csv(RAW_SURVEY_FEEDBACK_PATH, sep=';', parse_dates=['timestamp'])\n",
    "\n",
    "log = pd.read_csv(HOLD_OUT_DATA_PATH / 'log.csv', index_col=0, parse_dates=['time'])\n",
    "defect_log = pd.read_csv(HOLD_OUT_DATA_PATH / 'defect_log.csv', index_col=0)\n",
    "defect_log.columns = defect_log.columns.astype(int)\n",
    "\n",
    "# keep only single response per user-item pair\n",
    "responses = responses.groupby(['submission id', 'respondent']).first().reset_index()\n",
    "\n",
    "# keep only survey submissions\n",
    "survey_submissions = responses['submission id'].unique()\n",
    "log = log.loc[survey_submissions]\n",
    "defect_log = defect_log.loc[log.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load heuristic scores as features\n",
    "discrete_features = pd.read_csv(CACHED_PRIORITIZATIONS_PATH / 'discrete_scores.csv', index_col=0, sep=';')\n",
    "continuous_features = pd.read_csv(CACHED_PRIORITIZATIONS_PATH / 'continuous_scores.csv', index_col=0, sep=';')\n",
    "\n",
    "# keep only survey submissions\n",
    "discrete_features = discrete_features[discrete_features['submission id'].isin(survey_submissions)]\n",
    "continuous_features = continuous_features[continuous_features['submission id'].isin(survey_submissions)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_counts = responses.groupby(['submission id', 'answer']).size().reset_index(name='count')\n",
    "ties = vote_counts.groupby('submission id', group_keys=False).apply(lambda x: (x['count'] == x['count'].max()).sum() > 1, include_groups=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of respondents:', responses['respondent'].nunique())\n",
    "print('Average number of responses:', responses.groupby('respondent').count()['answer'].mean())\n",
    "print('Average number of answers per submission:', responses.groupby('submission id').count()['answer'].mean())\n",
    "print('Percentage of tied results:', np.round(ties.mean() * 100, 2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Dataset Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defect pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract defect pairs\n",
    "long_defects = defect_log.melt(var_name='defect id', value_name='count', ignore_index=False).reset_index(names=['submission id'])\n",
    "long_defects = long_defects[long_defects['count'] > 0]\n",
    "\n",
    "defect_pairs = (\n",
    "    responses\n",
    "    .merge(long_defects, on=\"submission id\", how=\"left\")\n",
    "    .rename(columns={\n",
    "        \"answer\": \"left\",\n",
    "        \"defect id\": \"right\"\n",
    "    })[[\"submission id\", \"left\", \"right\"]]\n",
    ")\n",
    "# remove self-pairs\n",
    "defect_pairs = defect_pairs[defect_pairs[\"left\"] != defect_pairs[\"right\"]]\n",
    "\n",
    "# add negated pairs\n",
    "defect_pairs['left won'] = True\n",
    "negated_pairs = defect_pairs.rename(columns={\"left\": \"right\", \"right\": \"left\"})\n",
    "negated_pairs['left won'] = False\n",
    "defect_pairs = pd.concat([defect_pairs, negated_pairs]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristics as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_features(defect_pairs: pd.DataFrame, features: pd.DataFrame, which: str=\"left\", suffix: str=\"\"):\n",
    "    \"\"\"\n",
    "    Construct a dataframe with heuristic scores of one of the defects.\n",
    "    \n",
    "    Args:\n",
    "        defect_pairs (pd.DataFrame): DataFrame with defect pairs.\n",
    "        features (pd.DataFrame): DataFrame with heuristic scores.\n",
    "        which (str): Which defect to use.\n",
    "        suffix (str): Suffix to add to column names.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with heuristic scores.\n",
    "    \"\"\"\n",
    "    if which not in defect_pairs.columns:\n",
    "        raise ValueError(f\"Column '{which}' not found in defect pairs.\")\n",
    "    return defect_pairs[[\"submission id\", which]].merge(\n",
    "        features,\n",
    "        left_on=[\"submission id\", which],\n",
    "        right_on=[\"submission id\", \"defect id\"],\n",
    "        how=\"left\"\n",
    "    ).drop(columns=[\"defect id\", \"submission id\", which]).add_suffix(suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine to a single dataframe\n",
    "df = pd.concat([\n",
    "    defect_pairs,\n",
    "    construct_features(defect_pairs, discrete_features, \"left\", \" (Left Discrete)\"),\n",
    "    construct_features(defect_pairs, discrete_features, \"right\", \" (Right Discrete)\"),\n",
    "    construct_features(defect_pairs, continuous_features, \"left\", \" (Left Continuous)\"),\n",
    "    construct_features(defect_pairs, continuous_features, \"right\", \" (Right Continuous)\")\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(BENCHMARK_OUTPUT_PATH / 'benchmark_dataset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
