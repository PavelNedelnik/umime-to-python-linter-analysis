{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.prioritization import *\n",
    "\n",
    "# input data\n",
    "VERSION = '0.0.0'\n",
    "DATASET_PATH = Path('data') / 'datasets' / f'ipython_{VERSION}'\n",
    "MODEL_PATH = DATASET_PATH / 'trained_heuristics'\n",
    "SURVEY_PATH = DATASET_PATH / 'teacher_survey'\n",
    "FEATURES_PATH = DATASET_PATH / 'hold_out_prioritization_scores'\n",
    "\n",
    "DATA_PARTITION = 'hold_out'\n",
    "\n",
    "# output data\n",
    "OUTPUT_DIR = DATASET_PATH / 'benchmark_dataset'\n",
    "\n",
    "# config\n",
    "RESOLUTION = 300\n",
    "\n",
    "# images\n",
    "IMAGE_DIR = Path('images') / \"survey_results\"\n",
    "\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv(DATASET_PATH / 'items.csv', index_col=0)\n",
    "defects = pd.read_csv(DATASET_PATH / f'defects.csv', index_col=0)\n",
    "\n",
    "responses = pd.read_csv(SURVEY_PATH / 'responses.csv', sep=';', parse_dates=['timestamp'])\n",
    "feedback = pd.read_csv(SURVEY_PATH / 'feedback.csv', sep=';', parse_dates=['timestamp'])\n",
    "\n",
    "log = pd.read_csv(DATASET_PATH / DATA_PARTITION/ 'log.csv', index_col=0, parse_dates=['time'])\n",
    "defect_log = pd.read_csv(DATASET_PATH / DATA_PARTITION / 'defect_log.csv', index_col=0)\n",
    "defect_log.columns = defect_log.columns.astype(int)\n",
    "\n",
    "# keep only single response per user-item pair\n",
    "responses = responses.groupby(['submission id', 'item id']).first().reset_index()\n",
    "\n",
    "# keep only survey submissions\n",
    "survey_submissions = responses['submission id'].unique()\n",
    "log = log.loc[survey_submissions]\n",
    "defect_log = defect_log.loc[log.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models for metadata\n",
    "models = {\n",
    "    \"Task Common\": TaskCommonModel,\n",
    "    \"Task Characteristic\": TaskCharacteristicModel,\n",
    "    \"Student Frequency\": StudentFrequencyModel,\n",
    "    \"Student Characteristic\": StudentCharacteristicModel,\n",
    "    \"Student Encountered\": StudentEncounteredBeforeModel,\n",
    "    \"Defect Multiplicity\": DefectMultiplicityModel,\n",
    "    \"Severity Baseline\": SeverityModel,\n",
    "}\n",
    "\n",
    "models = {\n",
    "    name: model.load(MODEL_PATH / f'{name}.pkl')\n",
    "    for name, model in models.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load heuristic scores as features\n",
    "discrete_features = pd.read_csv(FEATURES_PATH / 'discrete_scores.csv', index_col=0, sep=';')\n",
    "continuous_features = pd.read_csv(FEATURES_PATH / 'continuous_scores.csv', index_col=0, sep=';')\n",
    "\n",
    "# keep only survey submissions\n",
    "discrete_features = discrete_features[discrete_features['submission id'].isin(survey_submissions)]\n",
    "continuous_features = continuous_features[continuous_features['submission id'].isin(survey_submissions)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_counts = responses.groupby(['submission id', 'answer']).size().reset_index(name='count')\n",
    "ties = vote_counts.groupby('submission id', group_keys=False).apply(lambda x: (x['count'] == x['count'].max()).sum() > 1, include_groups=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of respondents:', responses['respondent'].nunique())\n",
    "print('Average number of responses:', responses.groupby('respondent').count()['answer'].mean())\n",
    "print('Average number of answers per submission:', responses.groupby('submission id').count()['answer'].mean())\n",
    "print('Percentage of tied results:', np.round(ties.mean() * 100, 2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Benchmark Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defect pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract defect pairs\n",
    "long_defects = defect_log.melt(var_name='defect id', value_name='count', ignore_index=False).reset_index(names=['submission id'])\n",
    "long_defects = long_defects[long_defects['count'] > 0]\n",
    "\n",
    "defect_pairs = (\n",
    "    responses\n",
    "    .merge(long_defects, on=\"submission id\", how=\"left\")\n",
    "    .rename(columns={\n",
    "        \"answer\": \"left\",\n",
    "        \"defect id\": \"right\"\n",
    "    })[[\"submission id\", \"left\", \"right\"]]\n",
    ")\n",
    "# remove self-pairs\n",
    "defect_pairs = defect_pairs[defect_pairs[\"left\"] != defect_pairs[\"right\"]]\n",
    "\n",
    "# add negated pairs\n",
    "defect_pairs['left won'] = True\n",
    "negated_pairs = defect_pairs.rename(columns={\"left\": \"right\", \"right\": \"left\"})\n",
    "negated_pairs['left won'] = False\n",
    "defect_pairs = pd.concat([defect_pairs, negated_pairs])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristics as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add discrete features\n",
    "defect_pairs_with_features = (\n",
    "    defect_pairs\n",
    "    .merge(\n",
    "        discrete_features,\n",
    "        left_on=[\"submission id\", \"left\"],\n",
    "        right_on=[\"submission id\", \"defect id\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .drop(columns=[\"defect id\"])  # merged from discrete_features\n",
    ")\n",
    "\n",
    "defect_pairs_with_features = (\n",
    "    defect_pairs_with_features\n",
    "    .merge(\n",
    "        discrete_features,\n",
    "        left_on=[\"submission id\", \"right\"],\n",
    "        right_on=[\"submission id\", \"defect id\"],\n",
    "        how=\"left\",\n",
    "        suffixes=(\" (Left)\", \" (Right)\")\n",
    "    )\n",
    "    .drop(columns=[\"defect id\"])  # merged from discrete_features\n",
    ")\n",
    "discrete_columns = [col for col in defect_pairs_with_features.columns if col.endswith(\" (Left)\") or col.endswith(\" (Right)\")]\n",
    "\n",
    "# add continuous features\n",
    "left_continuous_features = defect_pairs_with_features[[\"submission id\", \"left\"]].merge(\n",
    "    continuous_features,\n",
    "    left_on=[\"submission id\", \"left\"],\n",
    "    right_on=[\"submission id\", \"defect id\"],\n",
    "    how=\"left\",\n",
    ").drop(columns=[\"defect id\", \"submission id\", \"left\"])\n",
    "\n",
    "right_continuous_features = defect_pairs_with_features[[\"submission id\", \"right\"]].merge(\n",
    "    continuous_features,\n",
    "    left_on=[\"submission id\", \"right\"],\n",
    "    right_on=[\"submission id\", \"defect id\"],\n",
    "    how=\"left\",\n",
    ").drop(columns=[\"defect id\", \"submission id\", \"right\"])\n",
    "\n",
    "differential_features = (left_continuous_features - right_continuous_features).add_suffix(\"_diff\")\n",
    "\n",
    "defect_pairs_with_features = defect_pairs_with_features.merge(differential_features, left_index=True, right_index=True)\n",
    "continuous_columns = differential_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the dataframe\n",
    "df = defect_pairs_with_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "X = df[discrete_columns]\n",
    "y = df['left won']\n",
    "\n",
    "# convert into binary itemsets\n",
    "itemsets = X.apply(lambda x: [f\"{col}>\" if x[col] > 0 else f\"{col}<=\" for col in X.columns], axis=1)\n",
    "\n",
    "# encode\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(itemsets).transform(itemsets)\n",
    "encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "encoded['winner'] = y.values.astype(bool)\n",
    "\n",
    "# run apriori\n",
    "frequent_itemsets = apriori(encoded, min_support=0.1, use_colnames=True)\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.6)\n",
    "\n",
    "# filter rules\n",
    "# predicting the output variable\n",
    "rules = rules[rules['consequents'].apply(lambda x: 'winner' in x)]\n",
    "# sufficient confidence and support\n",
    "rules = rules[\n",
    "    (rules['confidence'] > 0.7) & \n",
    "    (rules['support'] > 0.15)\n",
    "]\n",
    "# sort\n",
    "rules = rules.sort_values(by='lift', ascending=False)\n",
    "# only one rule per antecedent\n",
    "rules = rules.drop_duplicates(subset=['antecedents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rules = rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(5)\n",
    "\n",
    "# generated by chatGPT\n",
    "html = f\"\"\"\n",
    "<div style=\"background-color: #121212; color: #f0f0f0; padding: 15px; font-family: 'Segoe UI', sans-serif; max-width: 900px; margin: 20px auto; border-radius: 6px;\">\n",
    "    <h2 style=\"text-align: center; margin-bottom: 15px;\">Top Unique Association Rules Predicting Target=1</h2>\n",
    "    {best_rules.to_html(index=False, border=0, classes='rules-table')}\n",
    "</div>\n",
    "<style>\n",
    "    .rules-table {{\n",
    "        width: 100%;\n",
    "        border-collapse: collapse;\n",
    "        color: #dcdcdc;\n",
    "    }}\n",
    "    .rules-table th {{\n",
    "        background-color: #2b2b2b;\n",
    "        padding: 8px;\n",
    "        text-align: left;\n",
    "    }}\n",
    "    .rules-table td {{\n",
    "        background-color: #1e1e1e;\n",
    "        padding: 8px;\n",
    "    }}\n",
    "    .rules-table tr:hover td {{\n",
    "        background-color: #3a3a3a;\n",
    "    }}\n",
    "</style>\n",
    "\"\"\"\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derived_df = {}\n",
    "\n",
    "for antecedents in best_rules['antecedents']:\n",
    "    def match_rule(row):  # noqa: D103\n",
    "        for cond in antecedents:\n",
    "            if '>' in cond:\n",
    "                feat = cond.split('>')[0]\n",
    "                if not row[feat] > 0:\n",
    "                    return 0\n",
    "            else:\n",
    "                feat = cond.split('<=')[0]\n",
    "                if not row[feat] <= 0:\n",
    "                    return 0\n",
    "        return 1\n",
    "    derived_df[\" & \".join(sorted(antecedents))] = pairwise_df.apply(match_rule, axis=1)\n",
    "\n",
    "derived_rules = derived_df.keys()\n",
    "derived_df = pd.DataFrame(derived_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discretized_df = responses.merge(sampled_defects[['submission id', 'defect id']], on='submission id', how='left')\n",
    "discretized_df = discretized_df[~(discretized_df['answer'] == discretized_df['defect id'])]\n",
    "\n",
    "defect1_features = []\n",
    "defect2_features = []\n",
    "\n",
    "# add categorical features\n",
    "for feature, discretized_values in features.items():\n",
    "    defect1_name = f'defect1 {feature}'\n",
    "    defect2_name = f'defect2 {feature}'\n",
    "\n",
    "    defect1_features.append(defect1_name)\n",
    "    defect2_features.append(defect2_name)\n",
    "\n",
    "    discretized_df[defect1_name] = np.zeros(len(discretized_df))\n",
    "    discretized_df[defect2_name] = np.zeros(len(discretized_df))\n",
    "\n",
    "    for idx, row in discretized_df.iterrows():\n",
    "        discretized_df.at[idx, defect1_name] = discretized_values.loc[row['submission id'], row['answer']]\n",
    "        discretized_df.at[idx, defect2_name] = discretized_values.loc[row['submission id'], row['defect id']]\n",
    "\n",
    "discretized_pairwise_df = []\n",
    "\n",
    "for index, (_, group) in enumerate(discretized_df.groupby(['submission id', 'respondent'])):\n",
    "    for _, row in group.iterrows():    \n",
    "        discretized_pairwise_df.append({\n",
    "            'response id': index,\n",
    "            'defect1': row['answer'],\n",
    "            'defect2': row['defect id'],\n",
    "            'first chosen': 1,\n",
    "            **{defect1_features[i]: row[defect2_features[i]] for i in range(len(defect1_features))},\n",
    "            **{defect2_features[i]: row[defect1_features[i]] for i in range(len(defect2_features))}\n",
    "        })\n",
    "\n",
    "        # also add the reverse\n",
    "        discretized_pairwise_df.append({\n",
    "            'response id': index,\n",
    "            'defect1': row['defect id'],\n",
    "            'defect2': row['answer'],\n",
    "            'first chosen': 0,\n",
    "            **{defect1_features[i]: row[defect1_features[i]] for i in range(len(defect1_features))},\n",
    "            **{defect2_features[i]: row[defect2_features[i]] for i in range(len(defect2_features))}\n",
    "        })\n",
    "\n",
    "discretized_pairwise_df = pd.DataFrame(discretized_pairwise_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "X = discretized_pairwise_df[defect1_features + defect2_features]\n",
    "y = discretized_pairwise_df['first chosen']\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "tree.fit(X, y)\n",
    "\n",
    "plt.figure(figsize=big_figsize, layout=\"constrained\")\n",
    "plot_tree(tree, feature_names=X.columns, filled=True, rounded=True, class_names=['Chosen Second','Chosen First'])\n",
    "plt.title(f\"Shallow Decision Tree for Feature Interactions (ACC: {tree.score(X, y):.2f})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = tree.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': importances\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "print(importance_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([pairwise_df[differential_features], discretized_pairwise_df[defect1_features + defect2_features], derived_df[derived_rules]], axis=1)\n",
    "y = pairwise_df['first chosen'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([pairwise_df[differential_features], discretized_pairwise_df[defect1_features + defect2_features], derived_df[derived_rules]], axis=1)\n",
    "y = pairwise_df['first chosen'].astype(bool)\n",
    "\n",
    "if training:\n",
    "    X, _, y, _, groups, _ = train_test_split(\n",
    "        X, y, groups, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "models = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(max_depth=5, random_state=4444),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Naive Bayes\": GaussianNB()\n",
    "}\n",
    "\n",
    "results_df = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    preds, truths = leave_one_group_out_for_model(model, X, y, groups=pairwise_df['response id'])\n",
    "    summary_df = summarize_model_performance(truths, preds, model_name=name)\n",
    "    results_df.append(summary_df)\n",
    "\n",
    "final_results_df = pd.concat(results_df).reset_index(drop=True)\n",
    "print(final_results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
