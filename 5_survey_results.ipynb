{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import entropy, kendalltau\n",
    "\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "from src.prioritization import *\n",
    "from src.feature_engineering import add_heuristic_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CONFIG_ENV\"] = \"debug\"\n",
    "if False:\n",
    "    os.environ[\"CONFIG_ENV\"] = \"production\"\n",
    "\n",
    "from config import load_config\n",
    "config = load_config()\n",
    "\n",
    "RESOLUTION = config['DEFAULTS']['resolution']\n",
    "\n",
    "# input data\n",
    "HOLD_OUT_DATA_PATH = config['PATHS']['teacher_hold_out_set']\n",
    "STORAGE_PATH = config['PATHS']['storage']\n",
    "RAW_SURVEY_RESPONSES_PATH = config['PATHS']['raw_survey_responses']\n",
    "RAW_SURVEY_FEEDBACK_PATH = config['PATHS']['raw_survey_feedback']\n",
    "CACHED_PRIORITIZATIONS_PATH = config['PATHS']['teacher_hold_out_prioritizations']\n",
    "\n",
    "# output data\n",
    "BENCHMARK_OUTPUT_PATH = config['PATHS']['benchmark_dataset']\n",
    "IMAGE_DIR = config['PATHS']['images'] / 'survey_results'\n",
    "\n",
    "os.makedirs(BENCHMARK_OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv(STORAGE_PATH / 'items.csv', index_col=0)\n",
    "defects = pd.read_csv(STORAGE_PATH / 'defects.csv', index_col=0)\n",
    "\n",
    "responses = pd.read_csv(RAW_SURVEY_RESPONSES_PATH, sep=';', parse_dates=['timestamp'])\n",
    "feedback = pd.read_csv(RAW_SURVEY_FEEDBACK_PATH, sep=';', parse_dates=['timestamp'])\n",
    "\n",
    "log = pd.read_csv(HOLD_OUT_DATA_PATH / 'log.csv', index_col=0, parse_dates=['time'])\n",
    "defect_log = pd.read_csv(HOLD_OUT_DATA_PATH / 'defect_log.csv', index_col=0)\n",
    "defect_log.columns = defect_log.columns.astype(int)\n",
    "\n",
    "# load heuristic scores as features\n",
    "discrete_features = pd.read_csv(CACHED_PRIORITIZATIONS_PATH / 'discrete_scores.csv', index_col=0, sep=';')\n",
    "continuous_features = pd.read_csv(CACHED_PRIORITIZATIONS_PATH / 'continuous_scores.csv', index_col=0, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model metadata\n",
    "data = items, defects\n",
    "\n",
    "models = [\n",
    "    TaskCommonModel(*data),\n",
    "    TaskCharacteristicModel(*data),\n",
    "    StudentCommonModel(*data),\n",
    "    StudentCharacteristicModel(*data),\n",
    "    StudentEncounteredBeforeModel(*data),\n",
    "    DefectMultiplicityModel(*data),\n",
    "    SeverityModel(*data),\n",
    "]\n",
    "\n",
    "# shift -2-2 scale to 1-5 scale\n",
    "for model in models:\n",
    "    if model.get_discretization_scale() == '-2-2':\n",
    "        name = model.get_model_name()\n",
    "        discrete_features[name] = discrete_features[name] + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify respondent ids\n",
    "unique_ids = sorted(responses[\"respondent\"].unique())\n",
    "id_map = {rid: f\"R{i+1}\" for i, rid in enumerate(unique_ids)}\n",
    "\n",
    "responses[\"respondent\"] = responses[\"respondent\"].map(id_map)\n",
    "feedback[\"respondent\"] = feedback[\"respondent\"].map(id_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Quality filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only single response per user-item pair\n",
    "responses = responses.groupby(['submission id', 'respondent']).first().reset_index()\n",
    "\n",
    "# keep only survey submissions\n",
    "survey_submissions = responses['submission id'].unique()\n",
    "log = log.loc[survey_submissions]\n",
    "defect_log = defect_log.loc[log.index]\n",
    "discrete_features = discrete_features[discrete_features['submission id'].isin(survey_submissions)]\n",
    "continuous_features = continuous_features[continuous_features['submission id'].isin(survey_submissions)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_respondents = responses['respondent'].nunique()\n",
    "n_submissions = responses['submission id'].nunique()\n",
    "n_annotations = len(responses)\n",
    "\n",
    "responses_per_respondent = responses.groupby('respondent').size()\n",
    "responses_per_submission = responses.groupby('submission id').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Respondents: {n_respondents}\")\n",
    "print(f\"Survey submissions seen by respondents: {n_submissions}\")\n",
    "print(f\"Total annotations: {n_annotations}\")\n",
    "print()\n",
    "print(\"Responses per respondent (summary):\")\n",
    "print(responses_per_respondent.describe().to_string())\n",
    "print()\n",
    "print(\"Responses per submission (summary):\")\n",
    "print(responses_per_submission.describe().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(feedback.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of feedback entries:\", len(feedback))\n",
    "print(\"Unique respondents:\", feedback['respondent'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = responses[[\"submission id\", \"respondent\", \"comment\"]].dropna(subset=[\"comment\"])\n",
    "\n",
    "print(\"Number of comments:\", len(comments))\n",
    "print(\"Percentage of responses with comments:\", round(100 * len(comments) / len(responses), 1), \"%\")\n",
    "print(\"Unique commenters:\", comments[\"respondent\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for submission_id, group in comments.groupby(\"submission id\"):\n",
    "    print(f\"\\n=== Submission {submission_id} ===\")\n",
    "    for _, row in group.iterrows():\n",
    "        print(f\"- {row['respondent']}: {row['comment']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for respondent, group in comments.groupby(\"respondent\"):\n",
    "    print(f\"\\n=== {respondent} ===\")\n",
    "    for _, row in group.iterrows():\n",
    "        print(f\"[{row['submission id']}] {row['comment']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_df = responses_per_submission.value_counts().sort_index()\n",
    "coverage_df = pd.DataFrame({\n",
    "    'n_responses': coverage_df.index,\n",
    "    'n_submissions': coverage_df.values,\n",
    "    'pct_submissions': 100 * coverage_df.values / coverage_df.values.sum()\n",
    "}).reset_index(drop=True).sort_values('n_responses')\n",
    "display(coverage_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(\n",
    "    coverage_df['n_responses'],\n",
    "    coverage_df.cumsum()['pct_submissions'],\n",
    "    marker='o'\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Number of Responses per Submission\")\n",
    "plt.ylabel(\"Cumulative % of Submissions\")\n",
    "plt.title(\"Cumulative Coverage of Survey Submissions\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"cumulative_coverage.png\", dpi=150)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-submission agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for submission_id, group in responses.groupby(\"submission id\"):\n",
    "    counts = group[\"answer\"].value_counts().sort_index()\n",
    "\n",
    "    # Majority vote\n",
    "    top_count = counts.max()\n",
    "    top_answers = counts[counts == top_count].index.tolist()\n",
    "    tie = len(top_answers) > 1\n",
    "\n",
    "    # Entropy\n",
    "    probabilities = counts / counts.sum()\n",
    "    H = entropy(probabilities, base=2)\n",
    "\n",
    "    # Normalized entropy\n",
    "    k = len(counts)\n",
    "    max_H = math.log2(k) if k > 1 else 1\n",
    "    H_norm = H / max_H\n",
    "\n",
    "    rows.append({\n",
    "        \"submission id\": submission_id,\n",
    "        \"n_votes\": counts.sum(),\n",
    "        \"n_choices\": k,\n",
    "        \"majority\": \", \".join(map(str, top_answers)),\n",
    "        \"tie_for_top\": tie,\n",
    "        \"entropy_bits\": H,\n",
    "        \"entropy_norm\": H_norm,\n",
    "    })\n",
    "\n",
    "results = pd.DataFrame(rows).set_index(\"submission id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_tied = 100 * results[\"tie_for_top\"].mean()\n",
    "print(f\"Percentage of submissions with tie for top choice: {pct_tied:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# highest disagreement\n",
    "results.sort_values(\"entropy_norm\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"entropy_norm\"].sort_values().plot(kind=\"bar\")\n",
    "plt.title(\"Normalized Entropy per Submission\")\n",
    "plt.ylabel(\"Entropy (0 = agreement, 1 = maximum disagreement)\")\n",
    "plt.xlabel(\"Submission ID\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"submission_entropy.png\", dpi=RESOLUTION)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"n_votes\"].sort_values().plot(kind=\"bar\")\n",
    "plt.title(\"Number of Responses per Submission\")\n",
    "plt.ylabel(\"Votes\")\n",
    "plt.xlabel(\"Submission ID\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"submission_n_responses.png\", dpi=RESOLUTION)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(results[\"entropy_norm\"].dropna(), bins=12)\n",
    "plt.xlabel('Normalized vote entropy (0=perfect agreement, 1=max)')\n",
    "plt.ylabel('Number of submissions')\n",
    "plt.title('Distribution of submission vote entropy (agreement)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / 'submission_entropy_distribution.png', dpi=RESOLUTION)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inter-anotator agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_kappas = []\n",
    "\n",
    "for a, b in combinations(id_map.values(), 2):\n",
    "    a_df = responses[responses['respondent'] == a][['submission id', 'answer']].set_index('submission id')\n",
    "    b_df = responses[responses['respondent'] == b][['submission id', 'answer']].set_index('submission id')\n",
    "    \n",
    "    # Only consider overlapping submissions\n",
    "    common = a_df.index.intersection(b_df.index)\n",
    "    if len(common) < 2:  # too few items\n",
    "        continue\n",
    "    \n",
    "    k = cohen_kappa_score(a_df.loc[common, 'answer'], b_df.loc[common, 'answer'])\n",
    "    pair_kappas.append(k)\n",
    "\n",
    "pair_kappas = np.array(pair_kappas)\n",
    "\n",
    "print(\"Pairwise Cohen's kappa (overlapping annotator pairs):\")\n",
    "print(f\"  Number of pairs considered: {len(pair_kappas)}\")\n",
    "if len(pair_kappas) > 0:\n",
    "    print(f\"  Mean kappa:   {pair_kappas.mean():.3f}\")\n",
    "    print(f\"  Median kappa: {np.median(pair_kappas):.3f}\")\n",
    "    print(f\"  Std kappa:    {pair_kappas.std(ddof=1):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(pair_kappas, bins=10, kde=True)\n",
    "plt.xlabel(\"Cohen's kappa\")\n",
    "plt.ylabel(\"Number of annotator pairs\")\n",
    "plt.title(\"Distribution of Pairwise Cohen's Kappa\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / 'annotator_pairwise_kappa.png', dpi=RESOLUTION)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fleiss_kappa(table):\n",
    "    \"\"\"Compute Fleiss' kappa from a count table (N_items x k_categories).\"\"\"\n",
    "    table = np.asarray(table, dtype=float)\n",
    "    N, k = table.shape\n",
    "    n_raters_per_item = table.sum(axis=1)\n",
    "    \n",
    "    # Probabilities per category\n",
    "    p_j = table.sum(axis=0) / table.sum()\n",
    "    # Agreement per item\n",
    "    P_i = ((table * (table - 1)).sum(axis=1)) / (n_raters_per_item * (n_raters_per_item - 1))\n",
    "    \n",
    "    P_bar = P_i.mean()\n",
    "    P_e = (p_j**2).sum()\n",
    "    \n",
    "    return (P_bar - P_e) / (1 - P_e) if (1 - P_e) != 0 else np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build contingency table\n",
    "all_answers = sorted(responses['answer'].unique())\n",
    "answer_to_idx = {a: i for i, a in enumerate(all_answers)}\n",
    "submission_ids = responses['submission id'].unique()\n",
    "\n",
    "table = np.zeros((len(submission_ids), len(all_answers)), dtype=int)\n",
    "submission_idx_map = {sid: i for i, sid in enumerate(submission_ids)}\n",
    "\n",
    "for sid, grp in responses.groupby('submission id'):\n",
    "    i = submission_idx_map[sid]\n",
    "    for ans, cnt in grp['answer'].value_counts().items():\n",
    "        table[i, answer_to_idx[ans]] = cnt\n",
    "\n",
    "fleiss = fleiss_kappa(table)\n",
    "print(f\"Fleiss' kappa (all submissions): {fleiss:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Dataset Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defect pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract defect pairs\n",
    "long_defects = defect_log.melt(var_name='defect id', value_name='count', ignore_index=False).reset_index(names=['submission id'])\n",
    "long_defects = long_defects[long_defects['count'] > 0]\n",
    "\n",
    "defect_pairs = (\n",
    "    responses\n",
    "    .merge(long_defects, on=\"submission id\", how=\"left\")\n",
    "    .rename(columns={\n",
    "        \"answer\": \"left\",\n",
    "        \"defect id\": \"right\"\n",
    "    })[[\"submission id\", \"left\", \"right\"]]\n",
    ")\n",
    "# remove self-pairs\n",
    "defect_pairs = defect_pairs[defect_pairs[\"left\"] != defect_pairs[\"right\"]]\n",
    "\n",
    "# add item id\n",
    "defect_pairs['item'] = log['item'].loc[defect_pairs['submission id']].values\n",
    "\n",
    "# add negated pairs\n",
    "defect_pairs['left won'] = True\n",
    "negated_pairs = defect_pairs.rename(columns={\"left\": \"right\", \"right\": \"left\"})\n",
    "negated_pairs['left won'] = False\n",
    "defect_pairs = pd.concat([defect_pairs, negated_pairs]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristics as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine to a single dataframe\n",
    "df = add_heuristic_scores(defect_pairs, discrete_features, continuous_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weigh all submissions equally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_counts = df.groupby('submission id')['submission id'].transform('count')\n",
    "df['weight'] = 1 / submission_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(BENCHMARK_OUTPUT_PATH / 'benchmark_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
