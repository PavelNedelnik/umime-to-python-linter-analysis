{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "from matplotlib.colors import ListedColormap\n",
    "from IPython.display import display, HTML\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "import src.ipython_loader as loader\n",
    "from src.prioritization import *\n",
    "\n",
    "RESOLUTION = 300\n",
    "VERSION = '0.0.0'\n",
    "DATASET_PATH = Path('data') / 'datasets' / f'ipython_{VERSION}'\n",
    "OUTPUT_PATH = DATASET_PATH / 'heuristics'\n",
    "BINARY_CMAP = ListedColormap(['red', 'green'])\n",
    "\n",
    "IMAGE_DIR = Path('images') / \"heuristics\"\n",
    "\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "\n",
    "MIN_TASK_DEFECT_SUBMISSIONS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_text(text: str, max_length: int=20):\n",
    "    \"\"\"Truncate text if it exceeds the maximum length.\"\"\"\n",
    "    if len(text) > max_length:\n",
    "        return text[:max_length - 3] + '...'\n",
    "    return text\n",
    "\n",
    "def abbreviate_and_truncate_text(text: str, ordered_abbreviations: dict | None=None, max_length: int=20):\n",
    "    \"\"\"Shorten text by applying a list of abbreviations and truncating if necessary.\"\"\"\n",
    "    current_text = text\n",
    "\n",
    "    if ordered_abbreviations:\n",
    "        for full_word, abbr in ordered_abbreviations.items():\n",
    "            # Use regex with word boundaries to ensure we replace full words only\n",
    "            pattern = re.escape(full_word)\n",
    "            current_text = re.sub(pattern, abbr, current_text, flags=re.IGNORECASE)\n",
    "\n",
    "            if len(current_text) <= max_length:\n",
    "                return current_text\n",
    "    \n",
    "    return truncate_text(current_text)\n",
    "\n",
    "ordered_abbreviations = {\n",
    "    'whitespace': 'ws',\n",
    "    'constant': 'const',\n",
    "    'variable': 'var',\n",
    "    'function': 'func',\n",
    "    'parameter': 'param',\n",
    "    'expression': 'expr',\n",
    "    'argument': 'arg',\n",
    "    'operator': 'op',\n",
    "    'operation': 'op',\n",
    "    'augmentable': 'aug',\n",
    "    'assignment': 'assign',\n",
    "    'container': 'cont',\n",
    "    'statement': 'stmt',\n",
    "    'arithmetic': 'arith',\n",
    "    'condition': 'cond',\n",
    "    'identifier': 'identif',\n",
    "    'multiple': 'multi',\n",
    "    'redundant': 'redun',\n",
    "    'necessary': 'necces',\n",
    "    'comparison': 'compar',\n",
    "    'negated': 'neg',\n",
    "    'unreachable': 'unreach',\n",
    "    'inappropriate': 'inapp',\n",
    "    'parenthesis': '()',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_and_defect_description(task, defect, items, defects, log, defect_log):\n",
    "    \"\"\"Generate an HTML display for a specific task and defect.\"\"\"\n",
    "    task_row = items.loc[task]\n",
    "    defect_row = defects.loc[defect]\n",
    "    submissions = log[(log[\"item\"] == task) & (defect_log[defect])]\n",
    "    \n",
    "    return f\"\"\"\n",
    "    <div style=\"display: flex; justify-content: space-between; gap: 20px;\">\n",
    "        \n",
    "        <div style=\"width: 48%; border: 1px solid #ccc; padding: 10px; border-radius: 5px;\">\n",
    "            <h3>{task_row[\"name\"]}</h3>\n",
    "            <div><strong>Instructions:</strong><br>{task_row[\"instructions\"]}</div>\n",
    "            <div><strong>Solution:</strong><br>\n",
    "                <pre style=\"background-color: #2e2e2e; color: #f5f5f5; padding: 10px; border-radius: 5px; font-family: monospace;\">{task_row[\"solution\"]}</pre>\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        \n",
    "        <div style=\"width: 48%; border: 1px solid #ccc; padding: 10px; border-radius: 5px;\">\n",
    "            <h3>{defect_row[\"defect name\"]}</h3>\n",
    "            <div><strong>Defect Type:</strong> {defect_row[\"defect type\"]}</div>\n",
    "            <div><strong>Severity:</strong> {defect_row[\"severity\"]}</div>\n",
    "            <div><strong>Description:</strong><br>{defect_row[\"description\"]}</div>\n",
    "            \n",
    "            <div style=\"display: flex; justify-content: space-between; margin-top: 20px;\">\n",
    "                <div style=\"width: 48%; padding: 10px;\">\n",
    "                    <strong>Code Example:</strong><br>\n",
    "                    <pre style=\"background-color: #2e2e2e; color: #f5f5f5; padding: 10px; border-radius: 5px; font-family: monospace;\">{defect_row[\"code example\"]}</pre>\n",
    "                </div>\n",
    "                <div style=\"width: 48%; padding: 10px;\">\n",
    "                    <strong>Code Fix Example:</strong><br>\n",
    "                    <pre style=\"background-color: #2e2e2e; color: #f5f5f5; padding: 10px; border-radius: 5px; font-family: monospace;\">{defect_row[\"code fix example\"]}</pre>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "    \n",
    "    \n",
    "    <div style=\"border: 1px solid #ccc; padding: 10px; margin-top: 20px; border-radius: 5px;\">\n",
    "        <strong>Example Submission:</strong><br>\n",
    "        <pre style=\"background-color: #2e2e2e; color: #f5f5f5; padding: 10px; border-radius: 5px; font-family: monospace;\">{submissions[\"answer\"].iloc[random.randint(0, len(submissions) - 1)] if len(submissions) else 'No submissions found'}</pre>\n",
    "    </div>\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(array):\n",
    "    \"\"\"Compute the Gini coefficient of a sorted numpy array.\"\"\"\n",
    "    array = array.flatten()\n",
    "    # avoid zero division\n",
    "    if array.sum() == 0:\n",
    "        return 0.0\n",
    "    # ensure all values are non-negative\n",
    "    if np.amin(array) < 0:\n",
    "        array -= np.amin(array)\n",
    "    # order values for computation\n",
    "    array = np.sort(array)\n",
    "    index = np.arange(1, array.shape[0] + 1)\n",
    "    # gini formula\n",
    "    return ((np.sum((2 * index - array.shape[0] - 1) * array)) / (array.shape[0] * np.sum(array)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv(DATASET_PATH / f'items_{VERSION}.csv', index_col=0)\n",
    "log = pd.read_csv(DATASET_PATH / f'log_{VERSION}.csv', index_col=0, parse_dates=['time'])\n",
    "defects = pd.read_csv(DATASET_PATH / f'defects_{VERSION}.csv', index_col=0)\n",
    "defect_log = pd.read_csv(DATASET_PATH / f'defect_log_{VERSION}.csv', index_col=0)\n",
    "defect_log.columns = defect_log.columns.astype(int)\n",
    "code_to_defect_id = json.load(open(DATASET_PATH / f'code_to_defect_id_{VERSION}.json', \"r\"))\n",
    "defect_presence = defect_log > 0\n",
    "\n",
    "defects['display name'] = defects['defect name'].apply(lambda x: abbreviate_and_truncate_text(x, ordered_abbreviations))\n",
    "items['display name'] = items['name'].apply(truncate_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Task filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-defect pairs without minimal support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insufficient_support = defect_presence.groupby(log[\"item\"]).sum() < MIN_TASK_DEFECT_SUBMISSIONS\n",
    "insufficient_support = insufficient_support.reindex(items.index).reindex(defects.index, axis=1).astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Data Split Analysis\n",
    "\n",
    "TODO?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_by_users(log, defect_log, pct=0.8):\n",
    "    \"\"\"\n",
    "    Split the data into train and test sets, keeping submissions from the same user together.\n",
    "    \n",
    "    Expects log and defect log to share index.\n",
    "    \"\"\"\n",
    "    all_users = log[\"user\"].unique()\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(all_users)\n",
    "\n",
    "    train_users = all_users[:int(len(all_users) * pct)]\n",
    "    mask = log[\"user\"].isin(train_users)\n",
    "\n",
    "    return log[mask], log[~mask], defect_log[mask], defect_log[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_log, test_log, train_defect_log, test_defect_log = split_data_by_users(log, defect_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_defects = test_defect_log > 0\n",
    "print(\"Fraction of Submissions with Multiple Defects:\", (test_defects.sum(axis=1) > 1).mean())\n",
    "print(\"Total Number\", (test_defects.sum(axis=1) > 1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = items, defects\n",
    "\n",
    "models = {\n",
    "    \"Task Common\": TaskCommonModel(*data),\n",
    "    \"Task Characteristic\": TaskCharacteristicModel(*data),\n",
    "    \"Currently Taught\": CurrentlyTaughtPrioritizer(*data, data_path='data/currently_taught.txt'),\n",
    "    \"Student Frequency\": StudentFrequencyModel(*data),\n",
    "    \"Student Characteristic\": StudentCharacteristicModel(*data),\n",
    "    \"Student Encountered\": StudentEncounteredBeforeModel(*data),\n",
    "    \"Defect Multiplicity\": DefectMultiplicityModel(*data),\n",
    "    \"Severity Baseline\": SeverityModel(*data),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in (pbar :=tqdm(models.items(), desc=\"Training Models\")):\n",
    "    pbar.set_description(f\"Training {name}\")\n",
    "    model.update(train_log, train_defect_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Weight Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_weight_histogram(model_name, model, iamge_dir, bins=60):\n",
    "    \"\"\"Plot a histogram of model weights.\"\"\"\n",
    "    values = model.get_model_weights()\n",
    "\n",
    "    if values is None:\n",
    "        print(f\"Model {model_name} has no weights. Skipping...\")\n",
    "        return\n",
    "\n",
    "    values = values.values.flatten()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8), layout=\"constrained\")\n",
    "\n",
    "    n_unique = len(np.unique(values))\n",
    "    discrete = n_unique <= 10\n",
    "\n",
    "    sns.histplot(data=values, bins=bins if discrete else n_unique, ax=ax, discrete=discrete, shrink=0.8)\n",
    "    \n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel(model.get_measure_name())\n",
    "    ax.set_ylabel('Frequency (Log Scale)')\n",
    "    ax.set_title(f'Distribution of {model.get_measure_description()}')\n",
    "    \n",
    "    plt.savefig(iamge_dir / model_name.lower().replace(\" \", \"_\"), dpi=RESOLUTION)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_image_dir = IMAGE_DIR / 'model_weight_histograms'\n",
    "os.makedirs(temp_image_dir, exist_ok=True)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    plot_model_weight_histogram(model_name, model, temp_image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-Defect Weight Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_task_weight_heatmap(model_name, model, image_dir, defects, items, mask=None, normalize_cmap=False):\n",
    "    \"\"\"Plot a heatmap of task-defect model weights.\"\"\"\n",
    "    model_weights = model.get_model_weights().copy()\n",
    "\n",
    "    if model_weights is None:\n",
    "        print(f\"Model {model_name} has no weights. Skipping...\")\n",
    "        return\n",
    "\n",
    "    defect_names = defects['display name'].loc[model_weights.columns]\n",
    "    task_names = items['display name'].loc[model_weights.index]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 17), layout=\"constrained\")\n",
    "    \n",
    "    if normalize_cmap:\n",
    "        lim = max(np.abs(model_weights.values).max(), 1)\n",
    "        kwargs = {'cmap': 'coolwarm', 'vmin':-lim, 'vmax':lim}\n",
    "    else:\n",
    "        kwargs = {'cmap': 'Reds'}\n",
    "\n",
    "    model_weights[mask] = np.nan\n",
    "        \n",
    "    sns.heatmap(\n",
    "        model_weights, xticklabels=defect_names, yticklabels=task_names, cbar=True,\n",
    "        cbar_kws={'label': model.get_measure_name()}, **kwargs\n",
    "    )\n",
    "\n",
    "    ax.tick_params(axis='x', labelsize=7)\n",
    "    ax.tick_params(axis='y', labelsize=8)\n",
    "    plt.title(model.get_measure_description())\n",
    "    plt.xlabel(\"Defects\")\n",
    "    plt.ylabel(\"Tasks\")\n",
    "    title = model_name.lower().replace(\" \", \"_\")\n",
    "    plt.savefig(image_dir / (title if mask is None else title + \"_masked\"), dpi=RESOLUTION)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_image_dir = IMAGE_DIR / 'task_defect_weight_maps'\n",
    "os.makedirs(temp_image_dir, exist_ok=True)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    if model.get_context_type() == \"task\":\n",
    "        normalize_cmap = model_name == \"Task Characteristic\"\n",
    "        plot_task_weight_heatmap(model_name, model, temp_image_dir, defects, items, normalize_cmap=normalize_cmap)\n",
    "    else:\n",
    "        print(f\"Model {model_name} is not a task model. Skipping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_image_dir = IMAGE_DIR / 'task_defect_weight_maps_masked'\n",
    "os.makedirs(temp_image_dir, exist_ok=True)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    if model.get_context_type() == \"task\":\n",
    "        normalize_cmap = model_name == \"Task Characteristic\"\n",
    "        plot_task_weight_heatmap(model_name, model, temp_image_dir, defects, items, normalize_cmap=normalize_cmap, mask=insufficient_support)\n",
    "    else:\n",
    "        print(f\"Model {model_name} is not a task model. Skipping...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLOSE_TIE_THRESHOLD = 0.05  # threshold for close ties as a percentage of the max score\n",
    "\n",
    "metrics = {\n",
    "    'exact_ties': {name: [] for name in models.keys()},\n",
    "    'close_ties': {name: [] for name in models.keys()},\n",
    "    'gini': {name: [] for name in models.keys()}\n",
    "}\n",
    "\n",
    "model_correlations = {\n",
    "    name1: {name2: [] for name2 in models.keys()}\n",
    "    for name1 in models.keys()\n",
    "}\n",
    "\n",
    "for _, submission in tqdm(test_log.iterrows(), total=test_log.shape[0]):\n",
    "    defect_counts = test_defects.loc[submission.name]\n",
    "    present_defects = defect_counts[defect_counts > 0]\n",
    "    \n",
    "    # Skip submissions with too few issues to prioritize\n",
    "    if len(present_defects) < 2:\n",
    "        for model in models.values():\n",
    "            model.update(submission, defect_counts)\n",
    "        continue\n",
    "\n",
    "    prioritizations = []\n",
    "\n",
    "    for name, model in models.items():\n",
    "        scores = model.prioritize(submission, present_defects)\n",
    "        \n",
    "        # --- Exact Ties ---\n",
    "        # Count items with the same score as the max\n",
    "        max_score = scores.max()\n",
    "        exact_ties = (scores == max_score).sum() > 1\n",
    "        metrics['exact_ties'][name].append(exact_ties)\n",
    "\n",
    "        # --- Close Ties ---\n",
    "        # Count items within a relative threshold of the max score\n",
    "        close_tie_threshold_value = max_score * CLOSE_TIE_THRESHOLD\n",
    "        close_ties = (scores >= (max_score - close_tie_threshold_value)).sum() > 1\n",
    "        metrics['close_ties'][name].append(close_ties)\n",
    "\n",
    "        # --- Gini Coefficient ---\n",
    "        metrics['gini'][name].append(gini(scores.values))\n",
    "\n",
    "        # Store the prioritization to calculate intra-model agreement\n",
    "        prioritizations.append(scores.values)\n",
    "\n",
    "        # Update the model\n",
    "        model.update(submission, defect_counts)\n",
    "    \n",
    "    # --- Agreement ---\n",
    "    for i, first in enumerate(models.keys()):\n",
    "        for j, second in enumerate(models.keys()):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            first_score = prioritizations[i]\n",
    "            second_score = prioritizations[j]\n",
    "            # Check for constant arrays\n",
    "            if np.all(first_score == first_score[0]) or np.all(second_score == second_score[0]):\n",
    "                rho = np.nan\n",
    "            else:\n",
    "                rho, _ = spearmanr(prioritizations[i], prioritizations[j])\n",
    "            model_correlations[first][second].append(rho)\n",
    "            model_correlations[second][first].append(rho)\n",
    "\n",
    "# --- Final Aggregation and Formatting ---\n",
    "results = {}\n",
    "for metric_name, data in metrics.items():\n",
    "    avg_values = {name: np.mean(values) for name, values in data.items()}\n",
    "    results[f'avg_{metric_name}'] = pd.Series(avg_values)\n",
    "\n",
    "print(f\"Average Exact Ties: \\n{results['avg_exact_ties']}\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"Average Close Ties: \\n{results['avg_close_ties']}\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"Average Gini Coefficients: \\n{results['avg_gini']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(model_correlations, open('model_corelations.json', mode='w'))\n",
    "json.dump(results, open('results.json', mode='w'))\n",
    "json.dump(defects, open('defects.json', mode='w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decisivness - Ties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = pd.DataFrame({\n",
    "    'Exact Ties': results['avg_exact_ties'],\n",
    "    'Close Ties': results['avg_close_ties'] - results['avg_exact_ties']\n",
    "}).T.reset_index()\n",
    "\n",
    "plot_data = plot_data.rename(columns={'index': 'Tie Type'})\n",
    "\n",
    "plot_data_melted = plot_data.melt(id_vars='Tie Type', var_name='Model', value_name='Average Count')\n",
    "\n",
    "ax = plot_data.set_index('Tie Type').T.plot(\n",
    "    kind='bar', \n",
    "    stacked=True, \n",
    "    figsize=(12, 8),\n",
    "    colormap='tab10'\n",
    ")\n",
    "\n",
    "plt.title(\"Average Number of Exact and Close Ties per Model\", fontsize=16)\n",
    "plt.ylabel(\"Average Count\", fontsize=12)\n",
    "plt.xlabel(\"Prioritization Model\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "plt.legend(title='Tie Type')\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / 'task_models_tie_bar_plot.png', dpi=RESOLUTION)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decisivness - Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = []\n",
    "for model_name, gini_list in metrics['gini'].items():\n",
    "    for gini_value in gini_list:\n",
    "        plot_data.append({'Model': model_name, 'Gini Coefficient': gini_value})\n",
    "\n",
    "plot_data = pd.DataFrame(plot_data)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "sns.boxplot(\n",
    "    x='Model',\n",
    "    y='Gini Coefficient',\n",
    "    data=plot_data,\n",
    "    notch=True,\n",
    "    palette='viridis',\n",
    "    hue='Model',\n",
    "    legend=False\n",
    ")\n",
    "\n",
    "plt.title(\"Distribution of Gini Coefficients by Prioritization Model\", fontsize=16)\n",
    "plt.xlabel(\"Prioritization Model\", fontsize=12)\n",
    "plt.ylabel(\"Gini Coefficient\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(IMAGE_DIR / 'task_models_gini_box_plot.png', dpi=RESOLUTION)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inter-Model Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = np.empty((len(task_models.keys()), len(task_models.keys())))\n",
    "correlation_matrix[:] = np.nan\n",
    "\n",
    "for i, first in enumerate(task_models.keys()):\n",
    "    for j, second in enumerate(task_models.keys()):\n",
    "        if i >= j:\n",
    "            continue\n",
    "        correlation_matrix[i, j] = np.nanmean(model_correlations[first][second])\n",
    "        correlation_matrix[j, i] = correlation_matrix[i, j]\n",
    "\n",
    "correlation_matrix = pd.DataFrame(correlation_matrix, index=task_models.keys(), columns=task_models.keys())\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "sns.heatmap(\n",
    "    correlation_matrix,\n",
    "    annot=True,\n",
    "    cmap='coolwarm',\n",
    "    fmt=\".2f\",\n",
    "    linewidths=.5,\n",
    "    cbar_kws={'label': \"Average Spearman's Correlation\"},\n",
    "    vmin=-1,\n",
    "    vmax=1\n",
    ")\n",
    "\n",
    "plt.title(\"Inter-Model Prioritization Agreement (Spearman's Rho)\", fontsize=16)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(IMAGE_DIR / 'task_models_correlation_heatmap.png', dpi=RESOLUTION)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agreement with baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = []\n",
    "for model, corr_list in model_correlations['Severity Baseline'].items():\n",
    "    for corr in corr_list:\n",
    "        plot_data.append({'Model': model, 'Spearman_Rho': corr})\n",
    "\n",
    "plot_data = pd.DataFrame(plot_data)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "sns.boxplot(\n",
    "    x='Model',\n",
    "    y='Spearman_Rho',\n",
    "    data=plot_data,\n",
    "    palette='rocket',\n",
    "    hue='Model',\n",
    "    legend=False,\n",
    "    notch=True\n",
    ")\n",
    "\n",
    "plt.title(\"Distribution of Spearman's Rho by Prioritization Model\", fontsize=16)\n",
    "plt.xlabel(\"Prioritization Model\", fontsize=12)\n",
    "plt.ylabel(\"Spearman's Rho\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(IMAGE_DIR / 'task_models_baseline_correlation_box_plot.png', dpi=RESOLUTION)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices = {}\n",
    "for name, model in tqdm(models.items(), desc=\"Analyzing Task Models\"):\n",
    "    if model.get_context_type() != \"task\":\n",
    "        continue\n",
    "    \n",
    "    weights = model.get_model_weights()\n",
    "\n",
    "    correlation_matrix = pd.DataFrame(index=items.index, columns=items.index)\n",
    "\n",
    "    if weights is None:\n",
    "        matrix_values = np.ones((len(items.index), len(items.index)))\n",
    "        np.fill_diagonal(matrix_values, 0)\n",
    "        correlation_matrix = pd.DataFrame(matrix_values, index=items.index, columns=items.index)\n",
    "        matrices[name] = correlation_matrix\n",
    "        continue\n",
    "\n",
    "    for first in items.index:\n",
    "        for second in items.index:\n",
    "            if first >= second:\n",
    "                continue\n",
    "            weights_first = weights.loc[first]\n",
    "            weights_second = weights.loc[second]\n",
    "\n",
    "            if np.all(weights_first == weights_first.iloc[0]) or np.all(weights_second == weights_second.iloc[0]):\n",
    "                rho = np.nan\n",
    "            else:\n",
    "                rho, _ = spearmanr(weights_first, weights_second)\n",
    "\n",
    "            correlation_matrix.loc[first, second] = rho\n",
    "            correlation_matrix.loc[second, first] = rho\n",
    "    matrices[name] = correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    nrows=2, ncols=2, figsize=(20, 18),\n",
    "    sharex=True, sharey=True,\n",
    "    gridspec_kw={'wspace': 0.05, 'hspace': 0.05} # Fine-tune the spacing\n",
    ")\n",
    "axes = axes.flatten()\n",
    "\n",
    "models = [\"Task Common\", \"Task Characteristic\", \"Currently Taught\", \"Severity Baseline\"]\n",
    "cmaps = ['coolwarm', 'coolwarm', 'coolwarm', 'coolwarm']\n",
    "\n",
    "for i, name in enumerate(models):\n",
    "    ax = axes[i]\n",
    "    correlation_matrix = matrices[name]\n",
    "    \n",
    "    sns.heatmap(\n",
    "        correlation_matrix.astype(float),\n",
    "        ax=ax,\n",
    "        cmap=cmaps[i],\n",
    "        vmin=-1,\n",
    "        vmax=1,\n",
    "        linewidths=.5,\n",
    "        cbar=False,\n",
    "    )\n",
    "\n",
    "    ax.set_title(f\"Task-to-Task Agreement for {name}\", fontsize=18)\n",
    "    if i in [2, 3]:\n",
    "        ax.set_xlabel('Task ID', fontsize=12)\n",
    "    else:\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_xticklabels([])\n",
    "    if i in [0, 2]:\n",
    "        ax.set_ylabel('Task ID', fontsize=12)\n",
    "    else:\n",
    "        ax.set_ylabel('')\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "# Create a single color bar for the entire figure\n",
    "fig.subplots_adjust(right=0.85)\n",
    "cbar_ax = fig.add_axes([0.88, 0.15, 0.02, 0.7])\n",
    "last_heatmap = axes[-1].collections[0]\n",
    "cbar = fig.colorbar(last_heatmap, cax=cbar_ax)\n",
    "cbar.set_label(\"Spearman's Correlation\", fontsize=16)\n",
    "\n",
    "plt.savefig(IMAGE_DIR / 'task_to_task_correlation_heatmaps.png', dpi=RESOLUTION)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_devs = {}\n",
    "for name, correlation_matrix in matrices.items():\n",
    "    upper_triangle_mask = np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)\n",
    "    off_diagonal_values = correlation_matrix.where(upper_triangle_mask).stack().values\n",
    "    off_diagonal_std = off_diagonal_values.std()\n",
    "    \n",
    "    std_devs[name] = off_diagonal_std\n",
    "\n",
    "std_df = pd.DataFrame(std_devs.items(), columns=['Model', 'Standard Deviation'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Model', y='Standard Deviation', data=std_df, palette='viridis', hue='Model', legend=False)\n",
    "\n",
    "plt.title(\"Standard Deviation of Inter-Task Correlation\", fontsize=16)\n",
    "plt.ylabel(\"Standard Deviation of Spearman's Rho\", fontsize=12)\n",
    "plt.xlabel(\"Prioritization Model\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(IMAGE_DIR / 'task_models_sensitivity_bar_plot.png', dpi=RESOLUTION)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pilot Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
