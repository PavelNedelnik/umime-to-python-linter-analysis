{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "from matplotlib.colors import ListedColormap\n",
    "from IPython.display import display, HTML\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GroupKFold, cross_val_score, LeaveOneGroupOut, train_test_split\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import src.ipython_loader as loader\n",
    "\n",
    "RESOLUTION = 300\n",
    "VERSION = '0.0.0'\n",
    "DATASET_PATH = Path('data') / 'datasets' / f'ipython_{VERSION}'\n",
    "BINARY_CMAP = ListedColormap(['red', 'green'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(values, title, bins=10, cutoff=None, save=False):  # noqa: D103\n",
    "    if cutoff:\n",
    "        values[values >= cutoff] = cutoff\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.hist(values, bins=bins)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(True)\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(f'images/{title.lower().replace(\" \", \"_\")}.png', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_and_defect_description(task, defect, items, defects, log, defect_log):  # noqa: D103\n",
    "    task_row = items.loc[task]\n",
    "    defect_row = defects.loc[defect]\n",
    "    submissions = log[(log[\"item\"] == task) & (defect_log[defect])]\n",
    "    \n",
    "    return f\"\"\"\n",
    "    <div style=\"display: flex; justify-content: space-between; gap: 20px;\">\n",
    "        <!-- Task Section -->\n",
    "        <div style=\"width: 48%; border: 1px solid #ccc; padding: 10px; border-radius: 5px;\">\n",
    "            <h3>{task_row[\"name\"]}</h3>\n",
    "            <div><strong>Instructions:</strong><br>{task_row[\"instructions\"]}</div>\n",
    "            <div><strong>Solution:</strong><br>\n",
    "                <pre style=\"background-color: #2e2e2e; color: #f5f5f5; padding: 10px; border-radius: 5px; font-family: monospace;\">{task_row[\"solution\"]}</pre>\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <!-- Defect Section -->\n",
    "        <div style=\"width: 48%; border: 1px solid #ccc; padding: 10px; border-radius: 5px;\">\n",
    "            <h3>{defect_row[\"defect name\"]}</h3>\n",
    "            <div><strong>Defect Type:</strong> {defect_row[\"defect type\"]}</div>\n",
    "            <div><strong>Severity:</strong> {defect_row[\"severity\"]}</div>\n",
    "            <div><strong>Description:</strong><br>{defect_row[\"description\"]}</div>\n",
    "            \n",
    "            <div style=\"display: flex; justify-content: space-between; margin-top: 20px;\">\n",
    "                <div style=\"width: 48%; padding: 10px;\">\n",
    "                    <strong>Code Example:</strong><br>\n",
    "                    <pre style=\"background-color: #2e2e2e; color: #f5f5f5; padding: 10px; border-radius: 5px; font-family: monospace;\">{defect_row[\"code example\"]}</pre>\n",
    "                </div>\n",
    "                <div style=\"width: 48%; padding: 10px;\">\n",
    "                    <strong>Code Fix Example:</strong><br>\n",
    "                    <pre style=\"background-color: #2e2e2e; color: #f5f5f5; padding: 10px; border-radius: 5px; font-family: monospace;\">{defect_row[\"code fix example\"]}</pre>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "    \n",
    "    <!-- Code Snippet Section -->\n",
    "    <div style=\"border: 1px solid #ccc; padding: 10px; margin-top: 20px; border-radius: 5px;\">\n",
    "        <strong>Example Submission:</strong><br>\n",
    "        <pre style=\"background-color: #2e2e2e; color: #f5f5f5; padding: 10px; border-radius: 5px; font-family: monospace;\">{submissions[\"answer\"].iloc[random.randint(0, len(submissions) - 1)] if len(submissions) else 'No submissions found'}</pre>\n",
    "    </div>\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_defect_plot(matrix, title='', save=False, interactive=False, *args, **kwargs):  # noqa: D103\n",
    "    defect_names = [defects['defect name'].loc[idx][:20] for idx in matrix.columns]\n",
    "    task_names = [items['name'].loc[idx][:20] for idx in matrix.index]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=big_figsize, layout=\"constrained\")\n",
    "    if interactive:\n",
    "        sns.heatmap(matrix.T, cbar=False, *args, **kwargs)\n",
    "    else:\n",
    "        sns.heatmap(matrix.T, xticklabels=task_names, yticklabels=defect_names, cbar=True, *args, **kwargs)\n",
    "        ax.tick_params(axis='x', labelsize=7)\n",
    "        ax.tick_params(axis='y', labelsize=8)\n",
    "        plt.title(title)\n",
    "    plt.xlabel(\"\")\n",
    "    plt.ylabel(\"\")\n",
    "\n",
    "    if save:\n",
    "        plt.savefig('images/' + title.lower().replace(' ', '_')[:title.find(' t=')] + '.png', dpi=300)\n",
    "\n",
    "    if interactive:\n",
    "        output_html = display(HTML(\"<b>Click a cell to see details</b>\"), display_id=True)\n",
    "\n",
    "        def on_click(event):\n",
    "            if event.inaxes == ax:\n",
    "                x = int(event.xdata)\n",
    "                y = int(event.ydata)\n",
    "                \n",
    "                if 0 <= x < len(task_names) and 0 <= y < len(defect_names):\n",
    "                    html = HTML(task_and_defect_description(matrix.index[x], matrix.columns[y]))\n",
    "                    #html = HTML(f'{x}, {y}')\n",
    "                    output_html.update(html)\n",
    "\n",
    "        fig.canvas.mpl_connect('button_press_event', on_click)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_abbreviations = {\n",
    "    'whitespace': 'ws',\n",
    "    'constant': 'const',\n",
    "    'variable': 'var',\n",
    "    'function': 'func',\n",
    "    'parameter': 'param',\n",
    "    'expression': 'expr',\n",
    "    'argument': 'arg',\n",
    "    'operator': 'op',\n",
    "    'operation': 'op',\n",
    "    'augmentable': 'aug',\n",
    "    'assignment': 'assign',\n",
    "    'container': 'cont',\n",
    "    'statement': 'stmt',\n",
    "    'arithmetic': 'arith',\n",
    "    'condition': 'cond',\n",
    "    'identifier': 'identif',\n",
    "    'multiple': 'multi',\n",
    "    'redundant': 'redun',\n",
    "    'necessary': 'necces',\n",
    "    'comparison': 'compar',\n",
    "    'negated': 'neg',\n",
    "    'unreachable': 'unreach',\n",
    "    'inappropriate': 'inapp',\n",
    "    'parenthesis': '()',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abbreviate_text(text: str, ordered_abbreviations: dict | None=None, max_length: int=20):\n",
    "    \"\"\"\n",
    "    Shorten text by applying a list of abbreviations in a specific order, and truncate the string if necessary.\n",
    "\n",
    "    Arguments:\n",
    "        text -- The original text string to be abbreviated.\n",
    "\n",
    "    Keyword Arguments:\n",
    "        ordered_abbreviations -- An ordered dictionary of abbreviations. (default: {None})\n",
    "        max_length -- The target maximum length for the string. (default: {20})\n",
    "\n",
    "    Returns:\n",
    "        _description_\n",
    "    \"\"\"\n",
    "    current_text = text\n",
    "\n",
    "    if ordered_abbreviations:\n",
    "        for full_word, abbr in ordered_abbreviations.items():\n",
    "            # Use regex with word boundaries to ensure we replace full words only\n",
    "            pattern = re.escape(full_word)\n",
    "            current_text = re.sub(pattern, abbr, current_text, flags=re.IGNORECASE)\n",
    "\n",
    "            if len(current_text) <= max_length:\n",
    "                return current_text\n",
    "\n",
    "    if len(current_text) > max_length:\n",
    "        return current_text[:max_length - 3] + '...'\n",
    "    \n",
    "    return current_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv(DATASET_PATH / f'items_{VERSION}.csv', index_col=0)\n",
    "log = pd.read_csv(DATASET_PATH / f'log_{VERSION}.csv', index_col=0, parse_dates=['time'])\n",
    "defects = pd.read_csv(DATASET_PATH / f'defects_{VERSION}.csv', index_col=0)\n",
    "defect_log = pd.read_csv(DATASET_PATH / f'defect_log_{VERSION}.csv', index_col=0)\n",
    "defect_log.columns = defect_log.columns.astype(int)\n",
    "code_to_defect_id = json.load(open(DATASET_PATH / f'code_to_defect_id_{VERSION}.json', \"r\"))\n",
    "defect_presence = defect_log > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defects['display name'] = defects['defect name'].apply(lambda x: abbreviate_text(x, ordered_abbreviations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Feature analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomalously frequent task-defect pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = defect_log.groupby(log['item']).mean()\n",
    "threshold = 0.9\n",
    "unreasonable = frequencies > threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task-defect rarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.01, 0.02, 0.03, 0.04]\n",
    "defect_names = defects[\"defect name\"]\n",
    "\n",
    "all_vals = []\n",
    "for threshold in thresholds:\n",
    "    rare = (frequencies < threshold).astype('int')\n",
    "    common = 1 - rare\n",
    "    vals = common.sum(axis=0)\n",
    "    all_vals.append(vals)\n",
    "\n",
    "stack_data = pd.concat(all_vals, axis=1).fillna(0)\n",
    "stack_data.columns = [f\"t={t:.2f}\" for t in thresholds]\n",
    "\n",
    "stack_data = stack_data.loc[(stack_data.median(axis=1) + 0.1 * stack_data.max(axis=1)).sort_values(ascending=False).index]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=long_figsize, layout='constrained')\n",
    "x = np.arange(stack_data.shape[0])\n",
    "bar_width = 0.2\n",
    "n_thresholds = len(thresholds)\n",
    "\n",
    "for i, col in enumerate(stack_data.columns):\n",
    "    offset = (i - n_thresholds / 2) * bar_width + bar_width / 2\n",
    "    ax.bar(x + offset, stack_data[col], width=bar_width, label=col)\n",
    "\n",
    "\n",
    "ax.set_xticks(x, labels=[defect_names.loc[idx][:20] for idx in stack_data.index], rotation=90)\n",
    "ax.set_xlabel('Defect')\n",
    "ax.set_ylabel('Number of Common Tasks')\n",
    "ax.set_title('Number of Common Task-Defect Pairs as Threshold Decreases')\n",
    "ax.legend(title='Threshold')\n",
    "\n",
    "if save:\n",
    "    plt.savefig('images/number_of_common_task-defect_pairs_as_threshold_decreases.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_threshold = 0.005\n",
    "rare = (frequencies < rare_threshold).astype('int')\n",
    "rare[unreasonable] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(rare.values.flatten(), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## characteristic task-defect pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score = (defect_log.groupby(log['item']).mean() - defect_log.mean()) / defect_log.std()\n",
    "z_score[unreasonable] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score[unreasonable] = np.nan\n",
    "fig = task_defect_plot(z_score, title=f\"Task-Defect Pair Z-Scores\", interactive=False, save=save)\n",
    "z_score[unreasonable] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score[unreasonable] = np.nan\n",
    "fig = task_defect_plot(z_score, title=f\"Task-Defect Pair Z-Scores\", interactive=True)\n",
    "z_score[unreasonable] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasonable_z_scores = z_score.stack().dropna()\n",
    "quantile = 0.8\n",
    "threshold = reasonable_z_scores.quantile(quantile)\n",
    "\n",
    "plt.figure(figsize=small_figsize)\n",
    "\n",
    "plt.hist(reasonable_z_scores, bins=100, color='skyblue', edgecolor='black')\n",
    "plt.axvline(x=threshold, color='red', linestyle='--', label=f'{int(quantile * 100)}-percentile Threshold (= {threshold:.2f})')\n",
    "\n",
    "plt.title('Histogram of Z-Scores for Reasonable Task-Defect Pairs')\n",
    "plt.xlabel('Z-Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "if save:\n",
    "    plt.savefig('images/histogram_of_z-scores_for_reasonable_task-defect_pairs.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characteristic_threshold = reasonable_z_scores.quantile(quantile)\n",
    "characteristic = (z_score > characteristic_threshold).astype(int)\n",
    "characteristic[unreasonable] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characteristic[unreasonable] = np.nan\n",
    "fig = task_defect_plot(characteristic, title=f\"Characteristic Defects for Threshold t={characteristic_threshold:.2f}\", interactive=False, save=save, cmap=binary_cmap)\n",
    "characteristic[unreasonable] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_by_topic = log.merge(items, left_on='item', right_index=True)['topic']\n",
    "defect_frequencies_by_topic = defect_log.groupby(log_by_topic).mean()\n",
    "\n",
    "topic_z_score = (defect_log.groupby(log_by_topic).mean() - defect_log.mean()) / defect_log.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_z_score[defect_frequencies_by_topic < 0.01] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=big_figsize, layout=\"constrained\")\n",
    "sns.heatmap(topic_z_score.T, vmin=-2, vmax=2, yticklabels=[defects['defect name'].loc[idx][:30] for idx in topic_z_score.columns], cmap=\"vlag\", cbar=True)\n",
    "plt.ylabel(\"\")\n",
    "plt.xlabel(\"\")\n",
    "plt.title(\"Topic-Level Defect Anomalies (Z-scores)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items[items['name'].str.contains('VelkÃ©')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defects[defects['defect name'].str.contains('for with redu')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## currently taught topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currently_taught = pd.read_csv('data/currently_taught.txt', sep='|', index_col=False)\n",
    "task_name_to_id = items.drop_duplicates(subset='name').reset_index().set_index('name')['id']\n",
    "currently_taught['Task ID'] = currently_taught['Task Name'].map(task_name_to_id)\n",
    "#currently_taught = currently_taught[['Defect ID', 'Task ID']]\n",
    "currently_taught = pd.crosstab(currently_taught['Task ID'], currently_taught['Defect ID']).astype(bool).astype(int)\n",
    "currently_taught = currently_taught.reindex(index=frequencies.index, columns=frequencies.columns, fill_value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = task_defect_plot(currently_taught, title=f\"Related Defects\", interactive=False, save=save, cmap=binary_cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## student-specific frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare in advance\n",
    "df = log.merge(defect_log, left_index=True, right_index=True)\n",
    "df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "df = df.sort_values(by=['user', 'time'])\n",
    "\n",
    "accuracy_log = []\n",
    "\n",
    "# for each user\n",
    "for user_id, history in tqdm(df.groupby('user')):\n",
    "    correct_count = {defect: 0 for defect in defect_log.columns}\n",
    "    encounter_count = {defect: 0 for defect in defect_log.columns}\n",
    "    \n",
    "    # iterate over history\n",
    "    for i, (idx, row) in enumerate(history.iterrows()):\n",
    "        accuracy_row = {}\n",
    "        \n",
    "        task_id = row['item']\n",
    "        \n",
    "        for defect in defect_log.columns:\n",
    "            is_reasonable = not unreasonable.loc[task_id, defect]\n",
    "            if row[defect] == 0 and is_reasonable:\n",
    "                correct_count[defect] += 1\n",
    "            if row[defect] == 1 or is_reasonable:\n",
    "                encounter_count[defect] += 1\n",
    "                accuracy_row[defect] = correct_count[defect] / encounter_count[defect]\n",
    "            else:\n",
    "                accuracy_row[defect] = np.nan\n",
    "\n",
    "        accuracy_row['submission id'] = idx\n",
    "        accuracy_log.append(accuracy_row)\n",
    "    \n",
    "# create dataframe\n",
    "accuracy_log = pd.DataFrame(accuracy_log).set_index('submission id')\n",
    "accuracy_log.index.name = 'submission id'\n",
    "\n",
    "accuracy_at_least_once = accuracy_log[accuracy_log < 1]\n",
    "\n",
    "student_specific_frequency = (accuracy_log - accuracy_log.mean()) / accuracy_log.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = pd.Series(student_specific_frequency.values.flatten()).dropna().values\n",
    "\n",
    "upper_quantile = 0.90\n",
    "student_upper_threshold = np.quantile(values, upper_quantile)\n",
    "lower_quantile = 0.20\n",
    "student_lower_threshold = np.quantile(values, lower_quantile)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.hist(values, bins=100)\n",
    "\n",
    "plt.axvline(student_lower_threshold, color='red', linestyle='--', linewidth=2, label=f'{int(lower_quantile*100)}% threshold')\n",
    "plt.axvline(student_upper_threshold, color='green', linestyle='--', linewidth=2, label=f'{int(upper_quantile*100)}% threshold')\n",
    "\n",
    "plt.title('Distribution of Student-Specific Frequency')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "if save:\n",
    "    plt.savefig(f'images/distribution_of_student_specific_frequency.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_specific_log = (student_specific_frequency > student_upper_threshold).astype('int')\n",
    "student_specific_log[student_specific_frequency < student_lower_threshold] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(student_specific_log.values.flatten(), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(accuracy_at_least_once.values.flatten(), 'Distribution of User-Defect Accuracy', bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(accuracy_at_least_once.groupby(log['user']).mean().values.flatten(), 'Distribution of User-Defect Accuracy (User Averages)', bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_means = accuracy_log.mean().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=long_figsize, layout=\"constrained\")\n",
    "\n",
    "ticks = np.arange(len(accuracy_means))\n",
    "\n",
    "plt.bar(ticks, accuracy_means)\n",
    "plt.title(\"Average Accuracy per Defect\")\n",
    "plt.ylabel(\"Average Accuracy\")\n",
    "plt.xlabel(\"Defect\")\n",
    "plt.xticks(ticks, [defects['defect name'][idx][:20] for idx in accuracy_means.index], rotation=90)\n",
    "plt.grid(axis='y')\n",
    "\n",
    "if save:\n",
    "    plt.savefig('images/average_accuracy_per_defect.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_reasonable = (~unreasonable).sum()\n",
    "\n",
    "has_reasonable = has_reasonable.loc[accuracy_means.index]\n",
    "\n",
    "plt.figure(figsize=(12, 6), layout=\"constrained\")\n",
    "\n",
    "ticks = np.arange(len(has_reasonable))\n",
    "\n",
    "plt.bar(ticks, has_reasonable)\n",
    "plt.title(\"Average Accuracy per Defect\")\n",
    "plt.ylabel(\"Average Accuracy\")\n",
    "plt.xlabel(\"Defect\")\n",
    "plt.xticks(ticks, [defects['defect name'][idx][:20] for idx in has_reasonable.index], rotation=90)\n",
    "plt.grid(axis='y')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defect multiplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress outliers\n",
    "multiplicity_log[multiplicity_log > 10] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = multiplicity_log[multiplicity_log > 0].mean().sort_values().sort_values(ascending=False)\n",
    "\n",
    "ticks = np.arange(len(means)) + 0.4\n",
    "\n",
    "plt.figure(figsize=long_figsize, layout=\"constrained\")\n",
    "plt.bar(ticks, means.values)\n",
    "\n",
    "plt.xticks(ticks,[defects['defect name'][idx][:20] for idx in means.index], rotation=90)\n",
    "plt.title(f\"Defects by Mean Multiplicity (When Occuring)\")\n",
    "plt.xlabel(\"Defect\")\n",
    "plt.ylabel(\"Mean Multiplicity\")\n",
    "\n",
    "if save:\n",
    "    plt.savefig('images/defects_by_mean_multiplicity.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = multiplicity_log[multiplicity_log > 0].var().sort_values().sort_values(ascending=False)\n",
    "\n",
    "ticks = np.arange(len(means)) + 0.4\n",
    "\n",
    "plt.figure(figsize=(10, 4), layout=\"constrained\")\n",
    "plt.bar(ticks, vars.values)\n",
    "\n",
    "plt.xticks(ticks,[defects['defect name'][idx][:20] for idx in means.index], rotation=90)\n",
    "plt.title(f\"Multiplicity Variance\")\n",
    "plt.xlabel(\"Defect\")\n",
    "plt.ylabel(\"Multiplicity Varaince\")\n",
    "\n",
    "if save:\n",
    "    plt.savefig('images/multiplicity_variance.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(multiplicity_log[multiplicity_log > 0].values.flatten(), 'Distribution of Multiplicity', bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplicity_thresholds = [0, 1, 5]\n",
    "multiplicity_labels = [0, 1, 2]\n",
    "bins = multiplicity_thresholds + [np.inf]\n",
    "\n",
    "# flatten\n",
    "multiplicity = multiplicity_log.values.flatten()\n",
    "\n",
    "# add labels\n",
    "multiplicity = pd.cut(multiplicity, bins=bins, labels=multiplicity_labels, right=False)\n",
    "\n",
    "# reshape\n",
    "multiplicity = pd.DataFrame(\n",
    "    multiplicity.reshape(multiplicity_log.shape),\n",
    "    index=multiplicity_log.index,\n",
    "    columns=multiplicity_log.columns\n",
    ").astype('Int64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(multiplicity.values.flatten(), return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## student recently fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "recently_fixed_log = []\n",
    "\n",
    "# prepare in advance to make the computation faster\n",
    "df = log.merge(defect_log, left_index=True, right_index=True)\n",
    "df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "df = df.sort_values(by=['user', 'time'])\n",
    "\n",
    "# for each user\n",
    "for user_id, history in tqdm(df.groupby('user')):\n",
    "    last_fixed = {defect: None for defect in defect_log.columns}\n",
    "\n",
    "    for i, (idx, row) in enumerate(history.iterrows()):\n",
    "        recency_row = {}\n",
    "\n",
    "        for defect in defect_log.columns:\n",
    "            if row[defect] == 1:\n",
    "                # 80% chance the student fixes it\n",
    "                if np.random.rand() < 0.8:\n",
    "                    recency_row[defect] = 0\n",
    "                    last_fixed[defect] = i\n",
    "                else:\n",
    "                    recency_row[defect] = np.nan\n",
    "            else:\n",
    "                if last_fixed[defect] is not None:\n",
    "                    recency_row[defect] = i - last_fixed[defect]\n",
    "                else:\n",
    "                    recency_row[defect] = np.nan\n",
    "\n",
    "        recency_row['submission id'] = idx\n",
    "        recently_fixed_log.append(recency_row)\n",
    "\n",
    "recently_fixed_log = pd.DataFrame(recently_fixed_log).set_index('submission id')\n",
    "recently_fixed_log.index.name = 'submission id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_time_rate = (recently_fixed_log == 0)[~recently_fixed_log.isna()].mean().sort_values(ascending=True)\n",
    "\n",
    "plt.figure(figsize=long_figsize, layout=\"constrained\")\n",
    "\n",
    "ticks = np.arange(len(first_time_rate))\n",
    "\n",
    "plt.bar(ticks, first_time_rate)\n",
    "plt.title(\"Percentage of First-Time Occurances per Defect\")\n",
    "plt.ylabel(\"First-Time Rate\")\n",
    "plt.xlabel(\"Defect\")\n",
    "plt.xticks(ticks, [defects['defect name'][idx][:30] for idx in first_time_rate.index], rotation=90)\n",
    "plt.grid(axis='y')\n",
    "\n",
    "if save:\n",
    "    plt.savefig('images/percentage_of_first_time_occurances_per_defect.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bins\n",
    "bins = [0, 1, 2, 4, 9, 14, 19, 24, 29, np.inf]\n",
    "bin_labels = ['1', '2', '3-4', '5-9', '10-14', '15-19', '20-24', '25-29', '30+']\n",
    "recency = recently_fixed_log.apply(lambda col: pd.cut(col, bins=bins, labels=bin_labels))\n",
    "\n",
    "# histogram\n",
    "recency = recency.apply(lambda col: col.value_counts()).fillna(0).astype(int)\n",
    "\n",
    "# scaling\n",
    "recency = recency.div(recency.sum(axis=0), axis=1)\n",
    "\n",
    "# sort as the previous graph\n",
    "recency = recency.loc[:, first_time_rate.index]\n",
    "\n",
    "plt.figure(figsize=(12, 6), layout=\"constrained\")\n",
    "sns.heatmap(recency, cmap='viridis', cbar_kws={'label': 'frequency'}, xticklabels=[defects['defect name'][idx][:30] for idx in recency.columns])\n",
    "plt.title(\"Number of Sessions Before Fix Reoccurance\")\n",
    "plt.xlabel(\"Defect\")\n",
    "plt.ylabel(\"Recency Bin\")\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "if save:\n",
    "    plt.savefig('images/heatmap_of_recency_bins_per_defect.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_recency = recently_fixed_log.replace(0, np.nan).median()\n",
    "\n",
    "# sort as the other graphs\n",
    "mean_recency = mean_recency.loc[first_time_rate.index]\n",
    "\n",
    "plt.figure(figsize=long_figsize, layout=\"constrained\")\n",
    "\n",
    "ticks = np.arange(len(mean_recency))\n",
    "\n",
    "plt.bar(ticks, mean_recency)\n",
    "plt.title(\"Average Recency (# of Submissions Since Last Seen) per Defect\")\n",
    "plt.ylabel(\"Average Recency\")\n",
    "plt.xlabel(\"Defect\")\n",
    "plt.xticks(ticks, [defects['defect name'][idx][:30] for idx in first_time_rate.index], rotation=90)\n",
    "plt.grid(axis='y')\n",
    "\n",
    "if save:\n",
    "    plt.savefig('images/average_recency_per_defect.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recency_thresholds = [-np.inf, -0.1, 5, np.inf]\n",
    "recency_labels = [0, 2, 1]  # 0 = never, 2 = recently, 1 = fixed but not recently\n",
    "\n",
    "recency = recently_fixed_log.values.flatten()\n",
    "\n",
    "recency = np.where(np.isnan(recency), -1, recency)\n",
    "\n",
    "recency = pd.cut(recency, bins=recency_thresholds, labels=recency_labels, right=False)\n",
    "\n",
    "recency = pd.DataFrame(\n",
    "    recency.reshape(recently_fixed_log.shape),\n",
    "    index=recently_fixed_log.index,\n",
    "    columns=recently_fixed_log.columns\n",
    ").astype('Int64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(recency.values.flatten(), return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity_log = defect_log * defects.loc[defect_log.columns]['severity']\n",
    "severity_log[severity_log.isna()] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = severity_log.copy()\n",
    "df[df == 0] = np.nan\n",
    "means = df.groupby(log['item']).max().mean(axis=1).sort_values()\n",
    "\n",
    "ticks = np.arange(len(means))\n",
    "\n",
    "plt.figure(figsize=(13, 4), layout=\"constrained\")\n",
    "\n",
    "plt.bar(ticks, means.values)\n",
    "\n",
    "plt.xticks(ticks, [items['name'][idx][:20] for idx in means.index], rotation=90)\n",
    "plt.title(f\"Mean Severity for Each Task\")\n",
    "plt.xlabel(\"Task\")\n",
    "plt.ylabel(\"Severity\")\n",
    "plt.tick_params(axis='x', labelsize=7)\n",
    "\n",
    "if save:\n",
    "    plt.savefig('images/mean_severity.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "differences = df.apply(lambda row: -row.nlargest(2).diff().iloc[-1], axis=1).value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=small_figsize, layout=\"constrained\")\n",
    "\n",
    "plt.bar(differences.index.astype(int), differences.values, edgecolor='black')\n",
    "plt.title('Histogram of Differences in Severity')\n",
    "plt.xlabel('Difference')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "if save:\n",
    "    plt.savefig(f'images/severity_differences.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rejected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### time spent on task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log['relative_time_spent'] = log['responseTime'] / log.groupby('item')['responseTime'].transform('mean')\n",
    "\n",
    "# clip all submissions to X times the task mean\n",
    "clip_threshold = 5\n",
    "\n",
    "print('Clipping all submissions to', clip_threshold, 'times the task mean:', (log['relative_time_spent'] > clip_threshold).mean(), 'changed.')\n",
    "\n",
    "log[log['relative_time_spent'] > clip_threshold] = clip_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_spent_threshold = log['relative_time_spent'].quantile(0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=small_figsize, layout=\"constrained\")\n",
    "\n",
    "sns.histplot(log['relative_time_spent'], bins=1000, kde=True)\n",
    "plt.axvline(time_spent_threshold, color='red', linestyle='--', label='75th Percentile Threshold')\n",
    "plt.title(\"Distribution of Relative Time Spent per Task (Values Over 5 Clipped)\")\n",
    "plt.xlabel(\"Relative Time Spent\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "\n",
    "if save:\n",
    "    plt.savefig('images/distribution_of_relative_time_spent_per_task.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### associated with poor performance (locally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "_, not_log, _, not_defect_log, _ = loader.load(ipython_path, data_path, only_correct=False)\n",
    "\n",
    "df = not_log[['item', 'correct']].merge(not_defect_log, left_index=True, right_index=True)\n",
    "\n",
    "correlations = {}\n",
    "\n",
    "# for each item\n",
    "for task_id, task_df in df.groupby('item'):\n",
    "    corr_dict = {}\n",
    "    # for each defect\n",
    "    for defect in defect_log.columns:\n",
    "        # get vectors\n",
    "        defect_presence = task_df[defect]\n",
    "        incorrect = ~task_df['correct']\n",
    "        # caluclate correlation\n",
    "        if defect_presence.nunique() > 1 and incorrect.nunique() > 1:\n",
    "            corr = precision_score(incorrect, defect_presence)\n",
    "            # corr, _ = pointbiserialr(defect_presence, incorrect)\n",
    "            corr_dict[defect] = corr\n",
    "        else:\n",
    "            corr_dict[defect] = np.nan\n",
    "\n",
    "    correlations[task_id] = corr_dict\n",
    "\n",
    "# construct df\n",
    "performance = pd.DataFrame.from_dict(correlations, orient='index')\n",
    "performance[unreasonable] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = task_defect_plot(performance, title=\"Defect-Failure Precision\", interactive=False, save=save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performances = performance.stack().dropna()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.hist(performances, bins=100, color='skyblue', edgecolor='black')\n",
    "plt.title('Histogram of Task-Defect Pair Precision in Predicting Failures')\n",
    "plt.xlabel('Z-Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "\n",
    "if save:\n",
    "    plt.savefig('images/histogram_of_task-defect_pair_precision.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_threshold = 0.25\n",
    "# failure_threshold = performances.quantile(quantile)\n",
    "\n",
    "fig = task_defect_plot(performance > failure_threshold, title=f\"High failure rates t={failure_threshold:.2f}\", interactive=False, save=save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = task_defect_plot(performance > failure_threshold, title=f\"High failure rates t={failure_threshold:.2f}\", interactive=True, cmap=binary_cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### future opportunity likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tasks = items.index\n",
    "\n",
    "opportunity_log = []\n",
    "\n",
    "# prepare in advance to make the computation faster\n",
    "df = log.merge(defect_log, left_index=True, right_index=True)\n",
    "df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "df = df.sort_values(by=['user', 'time'])\n",
    "\n",
    "# for each user\n",
    "for user_id, history in tqdm(df.groupby('user')):\n",
    "    completed_tasks = set()\n",
    "\n",
    "    # iterate over history\n",
    "    for submission_id, row in history.iterrows():\n",
    "        # unfinished tasks\n",
    "        completed_tasks.add(row['item'])\n",
    "        remaining_tasks = [t for t in all_tasks if t not in completed_tasks]\n",
    "\n",
    "        if not remaining_tasks:\n",
    "            opportunity_log.append({\n",
    "                'submission id': submission_id,\n",
    "                **{defect: 0.0 for defect in frequencies.columns}\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        weights = np.array([2.0 if t > row['item'] else 1.0 for t in remaining_tasks])\n",
    "        weighted_avg = (frequencies.loc[remaining_tasks].T @ weights) / weights.sum()\n",
    "\n",
    "        opportunity_log.append({\n",
    "            'submission id': submission_id,\n",
    "            **weighted_avg.to_dict()\n",
    "        })\n",
    "\n",
    "\n",
    "# create dataframe\n",
    "opportunity_log = pd.DataFrame(opportunity_log).set_index('submission id')\n",
    "opportunity_log.index.name = 'submission id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.hist(opportunity_log.values.flatten(), bins=100)\n",
    "plt.title('Histogram of Future Opportunities')\n",
    "plt.xlabel('Opportunity')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(True)\n",
    "\n",
    "if save:\n",
    "    plt.savefig(f'images/histogram_of_future_opportunities.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = opportunity_log.mean().sort_values().sort_values(ascending=False)\n",
    "\n",
    "ticks = np.arange(len(means)) + 0.4\n",
    "bar_width = 0.4\n",
    "\n",
    "plt.figure(figsize=(10, 4), layout=\"constrained\")\n",
    "\n",
    "plt.bar(ticks - bar_width / 2, means.values, label='Opportunity', width=bar_width)\n",
    "plt.bar(ticks + bar_width / 2, defect_log[means.index].mean().values, label='Frequency', width=bar_width)\n",
    "\n",
    "plt.xticks(ticks,[defects['defect name'][idx][:20] for idx in means.index], rotation=90)\n",
    "plt.title(f\"Defects by Mean Opportunity vs Frequency\")\n",
    "plt.xlabel(\"Defect\")\n",
    "plt.ylabel(\"Mean Opportunity\")\n",
    "plt.legend()\n",
    "\n",
    "if save:\n",
    "    plt.savefig('images/defects_by_mean_opportunity.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_opportunities = opportunity_log.groupby(log['item']).mean()\n",
    "task_opportunities = (task_opportunities - task_opportunities.mean(axis=0))\n",
    "\n",
    "fig = task_defect_plot(task_opportunities, title=\"Future Opportunity to Make Defect by Task\", interactive=False, save=save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filtering before sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_close_pair(row):\n",
    "    \"\"\"Check if there is a pair of values with difference one or less.\"\"\"\n",
    "    row_values = row.values\n",
    "    return np.any(np.abs(row_values[:, None] - row_values) <= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at least two defects\n",
    "filtered = defect_log[defect_log.sum(axis=1) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at most difference of one in severity\n",
    "filtered *= defects.loc[filtered.columns]['severity']\n",
    "filtered = filtered[filtered.apply(has_close_pair, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the filter\n",
    "defect_log = defect_log.loc[filtered.index]\n",
    "log = log.loc[filtered.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_task_defect_table(log, task_defect_table):\n",
    "    \"\"\"Expand task-defect table to feature log.\"\"\"\n",
    "    feature_log = log[['item']].join(task_defect_table, on='item')\n",
    "    feature_log.drop('item', axis=1, inplace=True)\n",
    "    return feature_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_values = {\n",
    "    'rare': expand_task_defect_table(log, frequencies),\n",
    "    'characteristic': expand_task_defect_table(log, z_score),\n",
    "    'currently_taught': expand_task_defect_table(log, currently_taught),\n",
    "    'student specific': student_specific_frequency.loc[log.index],\n",
    "    'multiplicity': multiplicity_log.loc[log.index],\n",
    "    # use discrete values for the recently fixed (big difference between never fixed and recently fixed)\n",
    "    'recently fixed': recency.loc[log.index].astype('int'), # recently_fixed_log.loc[log.index],\n",
    "    'severity': severity_log.loc[log.index],\n",
    "}\n",
    "\n",
    "features = {\n",
    "    'rare': expand_task_defect_table(log, rare).astype('int'),\n",
    "    'characteristic': expand_task_defect_table(log, characteristic).astype('int64'),\n",
    "    'currently_taught': expand_task_defect_table(log, currently_taught).astype('int'),\n",
    "    'student specific': student_specific_log.loc[log.index].astype('int'),\n",
    "    'multiplicity': multiplicity.loc[log.index].astype('int'),\n",
    "    'recently fixed': recency.loc[log.index].astype('int'),\n",
    "    'severity': severity_log.loc[log.index].astype('int'),\n",
    "}\n",
    "\n",
    "feature_embeddings = {\n",
    "    'rare': {\n",
    "        0: '',\n",
    "        1: 'Defect is rare for this task.'\n",
    "    },\n",
    "    'characteristic': {\n",
    "        0: '',\n",
    "        1: 'Defect is much more common for this task than other tasks.'\n",
    "    },\n",
    "    'currently_taught': {\n",
    "        0: '',\n",
    "        1: 'A LLM has identified that this Defect relates to material currently being taught.'\n",
    "    },\n",
    "    'student specific': {\n",
    "        -1: 'Student makes this defect less frequently than peers.',\n",
    "        0: '',\n",
    "        1: 'Student makes this defect more frequently than peers.'\n",
    "    },\n",
    "    'multiplicity': {\n",
    "        0: '',\n",
    "        1: 'Defect occurs a few times.',\n",
    "        2: 'Defect occurs many times.'\n",
    "    },\n",
    "    'recently fixed': {\n",
    "        0: 'Student has never fixed this defect.',\n",
    "        1: 'Student has fixed this defect at some point.',\n",
    "        2: 'Student fixed this defect recently.'\n",
    "    },\n",
    "    'severity': {\n",
    "        0: '',\n",
    "        2: 'This defect is benign.',\n",
    "        3: 'This defect is of moderate severity.',\n",
    "        4: 'This defect is severe.',\n",
    "        5: 'This defect is highly severe.'\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submission_html(submission_id, feature):\n",
    "    \"\"\"Simulate a decision based on the value of single feature.\"\"\"\n",
    "    task_id = log.loc[submission_id, 'item']\n",
    "    \n",
    "    present_defects = defect_log.loc[submission_id]\n",
    "    present_defects = present_defects[present_defects == 1].index.tolist()\n",
    "\n",
    "\n",
    "    defect_rows = []\n",
    "    for defect in present_defects:\n",
    "        defect_rows.append({\n",
    "            \"Defect\": defects.loc[defect, \"defect name\"],\n",
    "            \"Description\": defects.loc[defect, \"description\"],\n",
    "            f\"{feature}\": f\"{feature_values[feature].loc[submission_id, defect]:.2f}\"\n",
    "        })\n",
    "    \n",
    "    defect_df = pd.DataFrame(defect_rows)\n",
    "    \n",
    "    # generated by chatGPT\n",
    "    html = f\"\"\"\n",
    "    <div style=\"background-color: #121212; color: #f0f0f0; font-family: 'Segoe UI', sans-serif; padding: 20px;\">\n",
    "        <div style=\"text-align: left;\">\n",
    "            <table style=\"width: 90%; margin-left: auto; border-collapse: collapse; background-color: #1e1e1e; border: 1px solid #444;\">\n",
    "                <tr>\n",
    "                    <td style=\"vertical-align: top; width: 50%; border-right: 1px solid #333; padding: 20px; text-align: left;\">\n",
    "                        <h2 style=\"color: #ffffff;\">{items.loc[task_id, 'name']}</h2>\n",
    "                        <p><strong>Instructions:</strong><br>{items.loc[task_id, 'instructions']}</p>\n",
    "                        <div style=\"background-color: #2b2b2b; color: #dcdcdc; padding: 15px; border-radius: 5px; overflow-x: auto; text-align: left;\">\n",
    "                            <pre style=\"margin: 0; white-space: pre-wrap;\">{log.loc[submission_id,'answer']}</pre>\n",
    "                        </div>\n",
    "                    </td>\n",
    "                    <td style=\"vertical-align: top; width: 50%; padding: 20px;\">\n",
    "                        <h2 style=\"color: #ffffff;\">Detected Defects</h2>\n",
    "                        {defect_df.to_html(index=False, escape=False, border=0, justify='left', classes='defect-table')}\n",
    "                    </td>\n",
    "                </tr>\n",
    "            </table>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    return html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'characteristic'\n",
    "\n",
    "# identify submissions where feature is represented\n",
    "sampled_df = log[features[feature].sum(axis=1) > 0]\n",
    "\n",
    "# sample 10 unique submissions\n",
    "sampled_df = sampled_df.sample(n=10, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(generate_submission_html(sampled_df.index[3], feature)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# survey sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_sample(features, log, n_samples=200, seed=42):\n",
    "    \"\"\"Sample log indexes using a greedy algorithm to balance features and maximize task coverage.\"\"\"\n",
    "    random.seed(seed)\n",
    "\n",
    "    sample = []\n",
    "    feature_counts = feature_counts = pd.Series(0, index=features.keys())\n",
    "    task_counts = pd.Series(0, index=log['item'].unique())\n",
    "\n",
    "    for _ in tqdm(range(n_samples)):\n",
    "        # least represented feature\n",
    "        feature = feature_counts.idxmin()\n",
    "\n",
    "        # filter submissions with feature and not in the sample\n",
    "        candidates = features[feature]\n",
    "        candidates = candidates[candidates.sum(axis=1) > 0].index.difference(sample)\n",
    "        \n",
    "        if candidates.empty:\n",
    "            print(\"[WARNING] No candidates left for feature, skipping...\")\n",
    "            feature_counts[feature] += 1\n",
    "            continue\n",
    "\n",
    "        # filter submissions with yet unused task\n",
    "        task_indices = task_counts[task_counts == task_counts.min()].index\n",
    "        task_candidates = log.loc[candidates]\n",
    "        task_candidates = task_candidates[task_candidates['item'].apply(lambda x: x in task_indices)].index\n",
    "\n",
    "        if not task_candidates.empty:\n",
    "            choice = random.choice(task_candidates)\n",
    "            sample.append(choice)\n",
    "            task_counts[log.loc[choice, 'item']] += 1\n",
    "        else:\n",
    "            print(\"[WARNING] No candidates left for task, choosing randomly...\")\n",
    "            sample.append(random.choice(candidates))\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = greedy_sample(features, log, n_samples=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_freq = defect_log.loc[sample].mean()\n",
    "global_freq = defect_log.mean()\n",
    "uniform = pd.Series(1 / len(defect_log.columns), index=defect_log.columns)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Sample': sample_freq,\n",
    "    'Global': global_freq,\n",
    "    'Uniform': uniform\n",
    "}).sort_values(by='Global', ascending=False)\n",
    "\n",
    "labels = [defects['defect name'][idx][:20] for idx in df.index]\n",
    "ticks = np.arange(len(df))\n",
    "\n",
    "bar_width = 0.4\n",
    "plt.figure(figsize=(13, 5), layout='constrained')\n",
    "\n",
    "# sample vs global\n",
    "plt.bar(ticks - bar_width/2, df['Global'], width=bar_width, label='Global', color='lightgray')\n",
    "plt.bar(ticks + bar_width/2, df['Sample'], width=bar_width, label='Sample', color='steelblue')\n",
    "\n",
    "# uniform\n",
    "plt.plot(ticks, df['Uniform'], color='green', linestyle='--', label='Uniform')\n",
    "\n",
    "plt.xticks(ticks, labels, rotation=90)\n",
    "plt.ylabel(\"Defect Frequency\")\n",
    "plt.title(\"Defect Distribution: Sample vs Global vs Uniform\")\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(f'images/sampled_defect_distribution.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_freq = log.loc[sample]['item'].value_counts(normalize=True)\n",
    "global_freq = log['item'].value_counts(normalize=True)\n",
    "uniform = pd.Series(1 / log['item'].nunique(), index=log['item'].unique())\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Sample': sample_freq,\n",
    "    'Global': global_freq,\n",
    "    'Uniform': uniform\n",
    "}).sort_values(by='Global', ascending=False)\n",
    "\n",
    "labels = [items['name'].loc[idx][:20] for idx in df.index]\n",
    "ticks = np.arange(len(df))\n",
    "\n",
    "bar_width = 0.4\n",
    "plt.figure(figsize=(13, 5), layout='constrained')\n",
    "\n",
    "# sample vs global\n",
    "plt.bar(ticks - bar_width/2, df['Global'], width=bar_width, label='Global', color='lightgray')\n",
    "plt.bar(ticks + bar_width/2, df['Sample'], width=bar_width, label='Sample', color='steelblue')\n",
    "\n",
    "# uniform\n",
    "plt.plot(ticks, df['Uniform'], color='green', linestyle='--', label='Uniform')\n",
    "\n",
    "plt.xticks(ticks, labels, rotation=90)\n",
    "plt.tick_params(axis='x', labelsize=7)\n",
    "plt.ylabel(\"Task Frequency\")\n",
    "plt.title(\"Task Distribution: Sample vs Global vs Uniform\")\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(f'images/sampled_task_distribution.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_counts = pd.DataFrame({\n",
    "    name: pd.Series(df.values.flatten()).value_counts()\n",
    "    for name, df in features.items()\n",
    "}).fillna(0).astype(int)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 5), layout='constrained')\n",
    "feature_counts.T.plot(kind='bar', stacked=True, colormap='tab10', ax=ax)\n",
    "\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Number of Occurances\")\n",
    "plt.title(\"Stacked Bar Plot of Feature Values in the Sample\")\n",
    "plt.legend(title=\"Values\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.savefig(f'images/sampled_feature_distribution.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_export_dataframes(indexes, log, items, defects, defect_log, features):\n",
    "    \"\"\"Create submission and defect dataframes for export from given indexes and feature dictionaries.\"\"\"\n",
    "    submission_df = []\n",
    "    defect_df = []\n",
    "\n",
    "    for idx in indexes:\n",
    "        row = log.loc[idx]\n",
    "        submission_df.append({\n",
    "            'submission': row['answer'],\n",
    "            'task name': items.loc[row['item']]['name'],\n",
    "            'instructions': items.loc[row['item']]['instructions']\n",
    "        })\n",
    "\n",
    "        # For each defect present in the submission\n",
    "        for defect in defect_log.loc[idx][defect_log.loc[idx] > 0].index:\n",
    "            defect_entry = {\n",
    "                'submission id': idx,\n",
    "                'defect id': defect,\n",
    "                'name': defects.loc[defect]['defect name'],\n",
    "                'description': defects.loc[defect]['description'],\n",
    "                'code example': defects.loc[defect]['code example'],\n",
    "                'code fix example': defects.loc[defect]['code fix example'],\n",
    "                'additional context': ''\n",
    "            }\n",
    "\n",
    "            for feature, df in features.items():\n",
    "                value = df.at[idx, defect]\n",
    "                defect_entry['additional context'] += feature_embeddings[feature][value] + '\\n'\n",
    "\n",
    "            defect_df.append(defect_entry)\n",
    "\n",
    "    return pd.DataFrame(submission_df, index=indexes), pd.DataFrame(defect_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    sampled_submissions, sampled_defects = create_export_dataframes(sample, log, items, defects, defect_log, features)\n",
    "    sampled_submissions.to_csv('data/export/sampled_submissions.csv', sep=';', index_label='index')\n",
    "    sampled_defects.to_csv('data/export/sampled_defects.csv', sep=';', index_label='index')\n",
    "else:\n",
    "    sampled_submissions = pd.read_csv('data/export/sampled_submissions.csv', sep=';', index_col=0)\n",
    "    sampled_defects = pd.read_csv('data/export/sampled_defects.csv', sep=';', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# survey results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = pd.read_csv('data/responses.csv', sep=';')\n",
    "responses = responses[responses['submission id'].isin(sampled_submissions.index)] # some submissions were manually removed during the survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_counts = responses.groupby(['submission id', 'answer']).size().reset_index(name='count')\n",
    "ties = vote_counts.groupby('submission id', group_keys=False).apply(lambda x: (x['count'] == x['count'].max()).sum() > 1, include_groups=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of respondents:', responses['respondent'].nunique())\n",
    "print('Average number of responses:', responses.groupby('respondent').count()['answer'].mean())\n",
    "print('Average number of answers per submission:', responses.groupby('submission id').count()['answer'].mean())\n",
    "print('Percentage of tied results:', np.round(ties.mean() * 100, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_counts = responses['submission id'].value_counts().value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=small_figsize, layout='constrained')\n",
    "plt.bar(submission_counts.index.astype(str), submission_counts.values)\n",
    "plt.title(\"Distribution of Responses per Submission\")\n",
    "plt.xlabel(\"Number of Responses\")\n",
    "plt.ylabel(\"Number of Submissions\")\n",
    "\n",
    "plt.savefig(f'images/submission_response_distribution.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pick_rates = (responses.groupby('answer').size() / defect_log.loc[responses['submission id']].sum()).sort_values(ascending=False)\n",
    "pick_rates = pick_rates[~pick_rates.isna()]\n",
    "\n",
    "severities = (defects.loc[pick_rates.index]['severity'] - 2) / 3\n",
    "\n",
    "plt.figure(figsize=long_figsize, layout='constrained')\n",
    "\n",
    "ticks = np.arange(len(pick_rates))\n",
    "width = 0.4\n",
    "\n",
    "plt.bar(ticks - width / 2, pick_rates.values, width=width)\n",
    "plt.bar(ticks + width / 2, severities.values, width=width)\n",
    "plt.title(\"Defect Pick Rates vs Normalized Severity\")\n",
    "plt.xlabel(\"Defect\")\n",
    "plt.ylabel(\"Pick Rate\")\n",
    "plt.xticks(ticks, [defects['defect name'][idx][:20] for idx in pick_rates.index], rotation=90)\n",
    "\n",
    "plt.savefig(f'images/defect_pick_rates.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defect_log.loc[responses['submission id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winners = vote_counts.loc[vote_counts.groupby('submission id')['count'].idxmax()]\n",
    "winners.set_index('submission id', inplace=True)\n",
    "\n",
    "winners['consensus'] = (winners['count'] / responses.groupby('submission id').size()).sort_values()\n",
    "\n",
    "consensus = winners['consensus'].sort_values()\n",
    "\n",
    "ticks = np.arange(len(consensus))\n",
    "\n",
    "plt.figure(figsize=(13, 4), layout=\"constrained\")\n",
    "\n",
    "plt.bar(ticks, consensus.values)\n",
    "\n",
    "plt.title(\"Entry Consensus (Votes for Winner / Total Votes)\")\n",
    "plt.xlabel(\"Entry\")\n",
    "plt.ylabel(\"Consensus\")\n",
    "plt.tick_params(axis='x', labelsize=7)\n",
    "\n",
    "if save:\n",
    "    plt.savefig('images/entry_consensus.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(generate_submission_html(consensus.index[0], 'severity')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(generate_submission_html(consensus.index[1], 'severity')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(generate_submission_html(consensus.index[-1], 'severity')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(generate_submission_html(consensus.index[-2], 'severity')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_one_group_out_for_model(model, X, y, groups):\n",
    "    \"\"\"Train the model using leave-one-out cross-validation.\"\"\"\n",
    "    logo = LeaveOneGroupOut()\n",
    "    predictions = []\n",
    "    ground_truth = []\n",
    "\n",
    "    for train_idx, test_idx in logo.split(X, y, groups=groups):\n",
    "        model.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
    "        preds = model.predict(X.iloc[test_idx])\n",
    "        predictions.extend(preds)\n",
    "        ground_truth.extend(y.iloc[test_idx])\n",
    "\n",
    "    return predictions, ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_model_performance(y_true, y_pred, model_name='Model'):\n",
    "    \"\"\"Summarize the results of leave-one-out cross-validation with accuracy, precision, recall, and F1 score.\"\"\"\n",
    "    summary = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': np.round(accuracy_score(y_true, y_pred), 2),\n",
    "        'Precision': np.round(precision_score(y_true, y_pred, zero_division=0), 2),\n",
    "        'Recall': np.round(recall_score(y_true, y_pred, zero_division=0), 2),\n",
    "        'F1': np.round(f1_score(y_true, y_pred, zero_division=0), 2),\n",
    "    }\n",
    "    return pd.DataFrame([summary])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## differential features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add losing defects\n",
    "df = responses.merge(sampled_defects[['submission id', 'defect id']], on='submission id', how='left')\n",
    "df = df[~(df['answer'] == df['defect id'])]\n",
    "\n",
    "# add differential features\n",
    "for feature, values in feature_values.items():\n",
    "    df[feature] = np.zeros(len(df))\n",
    "    for idx, row in df.iterrows():\n",
    "        df.at[idx, feature] = values.loc[row['submission id'], row['answer']] - values.loc[row['submission id'], row['defect id']]\n",
    "\n",
    "differential_features = features.keys()\n",
    "\n",
    "# pairwise encoding\n",
    "pairwise_df = []\n",
    "\n",
    "# keep track of the original responses for cross validation\n",
    "for index, (_, group) in enumerate(df.groupby(['submission id', 'respondent'])):\n",
    "    for _, row in group.iterrows():    \n",
    "        pairwise_df.append({\n",
    "            'response id': index,\n",
    "            'defect1': row['answer'],\n",
    "            'defect2': row['defect id'],\n",
    "            'first chosen': 1,\n",
    "            **{feature: row[feature] for feature in differential_features}\n",
    "        })\n",
    "\n",
    "        # also add the reverse\n",
    "        pairwise_df.append({\n",
    "            'response id': index,\n",
    "            'defect1': row['defect id'],\n",
    "            'defect2': row['answer'],\n",
    "            'first chosen': 0,\n",
    "            **{feature: -row[feature] for feature in differential_features}\n",
    "        })\n",
    "\n",
    "pairwise_df = pd.DataFrame(pairwise_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for feature in differential_features:\n",
    "    X = pairwise_df[[feature]]\n",
    "    y = pairwise_df['first chosen']\n",
    "    groups = pairwise_df['response id']\n",
    "\n",
    "    if training:\n",
    "        X, _, y, _, groups, _ = train_test_split(\n",
    "            X, y, groups, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    preds, truths = leave_one_group_out_for_model(model, X, y, groups)\n",
    "\n",
    "    summary_df = summarize_model_performance(truths, preds, model_name=feature)\n",
    "    results.append(summary_df)\n",
    "\n",
    "final_results = pd.concat(results).reset_index(drop=True)\n",
    "print(final_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "X = pairwise_df[features.keys()]\n",
    "y = pairwise_df['first chosen']\n",
    "\n",
    "# convert into binary itemsets\n",
    "itemsets = X.apply(lambda x: [f\"{col}>\" if x[col] > 0 else f\"{col}<=\" for col in X.columns], axis=1)\n",
    "\n",
    "# encode\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(itemsets).transform(itemsets)\n",
    "encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "encoded['winner'] = y.values.astype(bool)\n",
    "\n",
    "# run apriori\n",
    "frequent_itemsets = apriori(encoded, min_support=0.1, use_colnames=True)\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.6)\n",
    "\n",
    "# filter rules\n",
    "# predicting the output variable\n",
    "rules = rules[rules['consequents'].apply(lambda x: 'winner' in x)]\n",
    "# sufficient confidence and support\n",
    "rules = rules[\n",
    "    (rules['confidence'] > 0.7) & \n",
    "    (rules['support'] > 0.15)\n",
    "]\n",
    "# sort\n",
    "rules = rules.sort_values(by='lift', ascending=False)\n",
    "# only one rule per antecedent\n",
    "rules = rules.drop_duplicates(subset=['antecedents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rules = rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(5)\n",
    "\n",
    "# generated by chatGPT\n",
    "html = f\"\"\"\n",
    "<div style=\"background-color: #121212; color: #f0f0f0; padding: 15px; font-family: 'Segoe UI', sans-serif; max-width: 900px; margin: 20px auto; border-radius: 6px;\">\n",
    "    <h2 style=\"text-align: center; margin-bottom: 15px;\">Top Unique Association Rules Predicting Target=1</h2>\n",
    "    {best_rules.to_html(index=False, border=0, classes='rules-table')}\n",
    "</div>\n",
    "<style>\n",
    "    .rules-table {{\n",
    "        width: 100%;\n",
    "        border-collapse: collapse;\n",
    "        color: #dcdcdc;\n",
    "    }}\n",
    "    .rules-table th {{\n",
    "        background-color: #2b2b2b;\n",
    "        padding: 8px;\n",
    "        text-align: left;\n",
    "    }}\n",
    "    .rules-table td {{\n",
    "        background-color: #1e1e1e;\n",
    "        padding: 8px;\n",
    "    }}\n",
    "    .rules-table tr:hover td {{\n",
    "        background-color: #3a3a3a;\n",
    "    }}\n",
    "</style>\n",
    "\"\"\"\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derived_df = {}\n",
    "\n",
    "for antecedents in best_rules['antecedents']:\n",
    "    def match_rule(row):  # noqa: D103\n",
    "        for cond in antecedents:\n",
    "            if '>' in cond:\n",
    "                feat = cond.split('>')[0]\n",
    "                if not row[feat] > 0:\n",
    "                    return 0\n",
    "            else:\n",
    "                feat = cond.split('<=')[0]\n",
    "                if not row[feat] <= 0:\n",
    "                    return 0\n",
    "        return 1\n",
    "    derived_df[\" & \".join(sorted(antecedents))] = pairwise_df.apply(match_rule, axis=1)\n",
    "\n",
    "derived_rules = derived_df.keys()\n",
    "derived_df = pd.DataFrame(derived_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for derived_rule in derived_rules:\n",
    "    X = derived_df[[derived_rule]]\n",
    "    y = pairwise_df['first chosen'].astype(bool)\n",
    "    groups = pairwise_df['response id']\n",
    "\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    preds, truths = leave_one_group_out_for_model(model, X, y, groups)\n",
    "\n",
    "    summary_df = summarize_model_performance(truths, preds, model_name=derived_rule)\n",
    "    results.append(summary_df)\n",
    "\n",
    "final_results = pd.concat(results).reset_index(drop=True)\n",
    "print(final_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discretized_df = responses.merge(sampled_defects[['submission id', 'defect id']], on='submission id', how='left')\n",
    "discretized_df = discretized_df[~(discretized_df['answer'] == discretized_df['defect id'])]\n",
    "\n",
    "defect1_features = []\n",
    "defect2_features = []\n",
    "\n",
    "# add categorical features\n",
    "for feature, discretized_values in features.items():\n",
    "    defect1_name = f'defect1 {feature}'\n",
    "    defect2_name = f'defect2 {feature}'\n",
    "\n",
    "    defect1_features.append(defect1_name)\n",
    "    defect2_features.append(defect2_name)\n",
    "\n",
    "    discretized_df[defect1_name] = np.zeros(len(discretized_df))\n",
    "    discretized_df[defect2_name] = np.zeros(len(discretized_df))\n",
    "\n",
    "    for idx, row in discretized_df.iterrows():\n",
    "        discretized_df.at[idx, defect1_name] = discretized_values.loc[row['submission id'], row['answer']]\n",
    "        discretized_df.at[idx, defect2_name] = discretized_values.loc[row['submission id'], row['defect id']]\n",
    "\n",
    "discretized_pairwise_df = []\n",
    "\n",
    "for index, (_, group) in enumerate(discretized_df.groupby(['submission id', 'respondent'])):\n",
    "    for _, row in group.iterrows():    \n",
    "        discretized_pairwise_df.append({\n",
    "            'response id': index,\n",
    "            'defect1': row['answer'],\n",
    "            'defect2': row['defect id'],\n",
    "            'first chosen': 1,\n",
    "            **{defect1_features[i]: row[defect2_features[i]] for i in range(len(defect1_features))},\n",
    "            **{defect2_features[i]: row[defect1_features[i]] for i in range(len(defect2_features))}\n",
    "        })\n",
    "\n",
    "        # also add the reverse\n",
    "        discretized_pairwise_df.append({\n",
    "            'response id': index,\n",
    "            'defect1': row['defect id'],\n",
    "            'defect2': row['answer'],\n",
    "            'first chosen': 0,\n",
    "            **{defect1_features[i]: row[defect1_features[i]] for i in range(len(defect1_features))},\n",
    "            **{defect2_features[i]: row[defect2_features[i]] for i in range(len(defect2_features))}\n",
    "        })\n",
    "\n",
    "discretized_pairwise_df = pd.DataFrame(discretized_pairwise_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "X = discretized_pairwise_df[defect1_features + defect2_features]\n",
    "y = discretized_pairwise_df['first chosen']\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "tree.fit(X, y)\n",
    "\n",
    "plt.figure(figsize=big_figsize, layout=\"constrained\")\n",
    "plot_tree(tree, feature_names=X.columns, filled=True, rounded=True, class_names=['Chosen Second','Chosen First'])\n",
    "plt.title(f\"Shallow Decision Tree for Feature Interactions (ACC: {tree.score(X, y):.2f})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "redundant with the previous results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = tree.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': importances\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "print(importance_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combination model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([pairwise_df[differential_features], discretized_pairwise_df[defect1_features + defect2_features], derived_df[derived_rules]], axis=1)\n",
    "y = pairwise_df['first chosen'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([pairwise_df[differential_features], discretized_pairwise_df[defect1_features + defect2_features], derived_df[derived_rules]], axis=1)\n",
    "y = pairwise_df['first chosen'].astype(bool)\n",
    "\n",
    "if training:\n",
    "    X, _, y, _, groups, _ = train_test_split(\n",
    "        X, y, groups, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "models = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(max_depth=5, random_state=4444),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Naive Bayes\": GaussianNB()\n",
    "}\n",
    "\n",
    "results_df = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    preds, truths = leave_one_group_out_for_model(model, X, y, groups=pairwise_df['response id'])\n",
    "    summary_df = summarize_model_performance(truths, preds, model_name=name)\n",
    "    results_df.append(summary_df)\n",
    "\n",
    "final_results_df = pd.concat(results_df).reset_index(drop=True)\n",
    "print(final_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise RuntimeError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "non_whitespace_defects = defects[\n",
    "    ~defects['defect name'].str.lower().str.contains('whitespace', na=False)\n",
    "].index\n",
    "\n",
    "filtered_defect_log = defect_log[non_whitespace_defects.intersection(defect_log.columns)]\n",
    "\n",
    "candidates = filtered_defect_log[filtered_defect_log.sum(axis=1) >= 2].index\n",
    "\n",
    "filtered_features = {k: v.loc[candidates] for k, v in features.items()}\n",
    "\n",
    "evaluation_sample = greedy_sample(filtered_features, log.loc[candidates], n_samples=7, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, evaluation_defects = create_export_dataframes(evaluation_sample, log, items, defects, filtered_defect_log, filtered_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_defect_pair(index, defect1, defect2):  # noqa: D103\n",
    "    differential = pd.Series(0., index=differential_features)\n",
    "    for feature, values in feature_values.items():\n",
    "        differential[feature] = values.loc[index, defect1] - values.loc[index, defect2]\n",
    "    derived = pd.Series(0., index=derived_rules)\n",
    "    for antecedents in best_rules['antecedents']:\n",
    "        def match_rule(row):  # noqa: D103\n",
    "            for cond in antecedents:\n",
    "                if '>' in cond:\n",
    "                    feat = cond.split('>')[0]\n",
    "                    if not row[feat] > 0:\n",
    "                        return 0\n",
    "                else:\n",
    "                    feat = cond.split('<=')[0]\n",
    "                    if not row[feat] <= 0:\n",
    "                        return 0\n",
    "            return 1\n",
    "        derived[\" & \".join(sorted(antecedents))] = match_rule(differential)\n",
    "    derived = pd.Series(derived)\n",
    "    discretized = pd.Series(0., index=defect1_features + defect2_features)\n",
    "    for feature, values in features.items():\n",
    "        discretized[f'defect1 {feature}'] = values.loc[index, defect1]\n",
    "        discretized[f'defect2 {feature}'] = values.loc[index, defect2]\n",
    "    return models['Decision Tree'].predict(pd.DataFrame([pd.concat([differential, discretized, derived])]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 10\n",
    "ordered_defects = {}\n",
    "\n",
    "for index, group in evaluation_defects.groupby('submission id'):\n",
    "    found_defects = group['defect id'].tolist()\n",
    "\n",
    "    iteration = 0\n",
    "    changed = True\n",
    "\n",
    "    while changed and iteration < max_iters:\n",
    "        changed = False\n",
    "        iteration += 1\n",
    "\n",
    "        for i in range(len(found_defects)):\n",
    "            for j in range(i + 1, len(found_defects)):\n",
    "                if predict_defect_pair(index, found_defects[i], found_defects[j]) == False:\n",
    "                    found_defects[i], found_defects[j] = found_defects[j], found_defects[i]\n",
    "                    changed = True\n",
    "\n",
    "    if iteration == max_iters:\n",
    "        print(f\"Warning: max iterations reached for submission_id {index}. Result may be unstable.\")\n",
    "\n",
    "    ordered_defects[index] = found_defects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unordered_defects = {}\n",
    "\n",
    "for k, v in ordered_defects.items():\n",
    "    if len(v) == 2:\n",
    "        # swap the two items\n",
    "        unordered_defects[k] = [v[1], v[0]]\n",
    "    else:\n",
    "        # shuffle for other lengths\n",
    "        unordered_defects[k] = random.sample(v, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Only relevant CSS\n",
    "css = \"\"\"\n",
    "<style>\n",
    ".survey-container {\n",
    "    display: flex;\n",
    "    flex-direction: column;\n",
    "    align-items: center;\n",
    "    width: 100%;\n",
    "    margin: auto;\n",
    "    font-family: Arial, sans-serif;\n",
    "    color: #333;\n",
    "    background-color: #f4f4f4;\n",
    "    padding: 20px;\n",
    "    box-sizing: border-box;\n",
    "}\n",
    "\n",
    ".survey-content {\n",
    "    display: flex;\n",
    "    justify-content: space-between;\n",
    "    width: 100%;\n",
    "    flex-wrap: wrap;\n",
    "    gap: 20px;\n",
    "}\n",
    "\n",
    ".task-section, .defects-section {\n",
    "    width: 48%;\n",
    "    padding: 15px;\n",
    "    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);\n",
    "    border-radius: 8px;\n",
    "    background-color: #f5f5f5;\n",
    "}\n",
    "\n",
    ".code-block {\n",
    "    background-color: #f0f0f0;\n",
    "    padding: 10px;\n",
    "    border-radius: 5px;\n",
    "    overflow-x: auto;\n",
    "    font-family: monospace;\n",
    "}\n",
    "\n",
    ".defect-button {\n",
    "    background-color: #dcdcdc;\n",
    "    border: none;\n",
    "    padding: 15px;\n",
    "    text-align: left;\n",
    "    width: 100%;\n",
    "    margin-bottom: 10px;\n",
    "    box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1);\n",
    "    display: flex;\n",
    "    flex-direction: column;\n",
    "    align-items: flex-start;\n",
    "    border-radius: 8px;\n",
    "    cursor: default;\n",
    "}\n",
    "\n",
    ".defect-content {\n",
    "    width: 100%;\n",
    "}\n",
    "\n",
    ".defect-context {\n",
    "    margin-top: 10px;\n",
    "    padding: 10px;\n",
    "    background-color: #f8f8f8;\n",
    "    border-top: 1px solid #ddd;\n",
    "    font-style: italic;\n",
    "}\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "for task_number, idx in enumerate(evaluation_sample):\n",
    "    row = log.loc[idx]\n",
    "    task = items.loc[row['item']]\n",
    "\n",
    "    html = \"\"\"\n",
    "    <div class=\"survey-container\">\n",
    "        <div class=\"survey-content\" style=\"display: flex; gap: 20px;\">\n",
    "            <!-- Left column: ordered defects -->\n",
    "            <div class=\"defects-section ordered-defects\" style=\"flex: 1;\">\n",
    "    \"\"\"\n",
    "\n",
    "    for defect_id in ordered_defects[idx]:\n",
    "        defect = defects.loc[defect_id]\n",
    "        html += f\"\"\"\n",
    "            <button class=\"defect-button unclickable\">\n",
    "                <div class=\"defect-content\">\n",
    "                    <p><strong>{defect['defect name']}</strong>: {defect['description']}</p>\n",
    "        \"\"\"\n",
    "        if defect.get('code example'):\n",
    "            html += f\"\"\"\n",
    "                    <pre class=\"code-block\"><strong>Example:</strong>\\n{defect['code example']}</pre>\n",
    "            \"\"\"\n",
    "        if defect.get('code fix example'):\n",
    "            html += f\"\"\"\n",
    "                    <pre class=\"code-block\"><strong>Fix:</strong>\\n{defect['code fix example']}</pre>\n",
    "            \"\"\"\n",
    "        if defect.get('additional context'):\n",
    "            html += f\"\"\"\n",
    "                    <div class=\"defect-context\">\n",
    "                        <strong>Additional Context:</strong> {defect['additional context']}\n",
    "                    </div>\n",
    "            \"\"\"\n",
    "\n",
    "        html += \"\"\"\n",
    "                </div>\n",
    "            </button>\n",
    "        \"\"\"\n",
    "\n",
    "    html += f\"\"\"\n",
    "            </div>\n",
    "            <!-- Center column: task section -->\n",
    "            <div class=\"task-section\" style=\"flex: 2;\">\n",
    "                <h3>Task {task_number + 1}: {task['name']}</h3>\n",
    "                <p><strong>Instructions:</strong> {task['instructions']}</p>\n",
    "                <h4>Student Submission:</h4>\n",
    "                <pre class=\"code-block\">{row['answer']}</pre>\n",
    "            </div>\n",
    "            <!-- Right column: unordered defects -->\n",
    "            <div class=\"defects-section unordered-defects\" style=\"flex: 1;\">\n",
    "    \"\"\"\n",
    "\n",
    "    for defect_id in unordered_defects[idx]:\n",
    "        defect = defects.loc[defect_id]\n",
    "        html += f\"\"\"\n",
    "            <button class=\"defect-button unclickable\">\n",
    "                <div class=\"defect-content\">\n",
    "                    <p><strong>{defect['defect name']}</strong>: {defect['description']}</p>\n",
    "        \"\"\"\n",
    "        if defect.get('code example'):\n",
    "            html += f\"\"\"\n",
    "                    <pre class=\"code-block\"><strong>Example:</strong>\\n{defect['code example']}</pre>\n",
    "            \"\"\"\n",
    "        if defect.get('code fix example'):\n",
    "            html += f\"\"\"\n",
    "                    <pre class=\"code-block\"><strong>Fix:</strong>\\n{defect['code fix example']}</pre>\n",
    "            \"\"\"\n",
    "        if defect.get('additional context'):\n",
    "            html += f\"\"\"\n",
    "                    <div class=\"defect-context\">\n",
    "                        <strong>Additional Context:</strong> {defect['additional context']}\n",
    "                    </div>\n",
    "            \"\"\"\n",
    "\n",
    "        html += \"\"\"\n",
    "                </div>\n",
    "            </button>\n",
    "        \"\"\"\n",
    "\n",
    "    html += \"\"\"\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    filename = f\"data/export/tasks/task{task_number + 1}.html\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(css + html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
