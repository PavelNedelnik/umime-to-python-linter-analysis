{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from src.prioritization import *\n",
    "from src.utils import gini\n",
    "\n",
    "RETRAIN_MODELS = True\n",
    "\n",
    "MIN_TASK_DEFECT_SUBMISSIONS = 5  # task-defect pairs below this threshold are left out for some experiments\n",
    "CLOSE_TIE_THRESHOLD = 0.05  # threshold for the <close ties> metric as a percentage of the max score\n",
    "BINARY_CMAP = ListedColormap(['red', 'green'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CONFIG_ENV\"] = \"debug\"\n",
    "if False:\n",
    "    os.environ[\"CONFIG_ENV\"] = \"production\"\n",
    "\n",
    "from config import load_config\n",
    "config = load_config()\n",
    "\n",
    "DEBUG = config[\"DEBUG\"]\n",
    "RESOLUTION = config['DEFAULTS']['resolution']\n",
    "\n",
    "# input data\n",
    "TRAINING_DATA_PATH = config['PATHS']['development_set']\n",
    "VALIDATION_DATA_PATH = config['PATHS']['evaluation_set']\n",
    "STORAGE_PATH = config['PATHS']['storage']\n",
    "\n",
    "# output data\n",
    "MODEL_OUTPUT_PATH = config['PATHS']['evaluation_trained_heuristics']\n",
    "EVALUATION_PRIORITIZATIONS_CACHE = config['PATHS']['evaluation_prioritizations']\n",
    "MODEL_METRICS_PATH = config['PATHS']['model_metrics']\n",
    "IMAGE_DIR = config['PATHS']['images'] / 'heuristics'\n",
    "\n",
    "os.makedirs(MODEL_OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(EVALUATION_PRIORITIZATIONS_CACHE, exist_ok=True)\n",
    "os.makedirs(MODEL_METRICS_PATH, exist_ok=True)\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_and_defect_description(task, defect, items, defects, log, defect_log):\n",
    "    \"\"\"Generate an HTML display for a specific task and defect.\"\"\"\n",
    "    task_row = items.loc[task]\n",
    "    defect_row = defects.loc[defect]\n",
    "    submissions = log[(log[\"item\"] == task) & (defect_log[defect])]\n",
    "    \n",
    "    return f\"\"\"\n",
    "    <div style=\"display: flex; justify-content: space-between; gap: 20px;\">\n",
    "        \n",
    "        <div style=\"width: 48%; border: 1px solid #ccc; padding: 10px; border-radius: 5px;\">\n",
    "            <h3>{task_row[\"name\"]}</h3>\n",
    "            <div><strong>Instructions:</strong><br>{task_row[\"instructions\"]}</div>\n",
    "            <div><strong>Solution:</strong><br>\n",
    "                <pre style=\"background-color: #2e2e2e; color: #f5f5f5; padding: 10px; border-radius: 5px; font-family: monospace;\">{task_row[\"solution\"]}</pre>\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        \n",
    "        <div style=\"width: 48%; border: 1px solid #ccc; padding: 10px; border-radius: 5px;\">\n",
    "            <h3>{defect_row[\"defect name\"]}</h3>\n",
    "            <div><strong>Defect Type:</strong> {defect_row[\"defect type\"]}</div>\n",
    "            <div><strong>Severity:</strong> {defect_row[\"severity\"]}</div>\n",
    "            <div><strong>Description:</strong><br>{defect_row[\"description\"]}</div>\n",
    "            \n",
    "            <div style=\"display: flex; justify-content: space-between; margin-top: 20px;\">\n",
    "                <div style=\"width: 48%; padding: 10px;\">\n",
    "                    <strong>Code Example:</strong><br>\n",
    "                    <pre style=\"background-color: #2e2e2e; color: #f5f5f5; padding: 10px; border-radius: 5px; font-family: monospace;\">{defect_row[\"code example\"]}</pre>\n",
    "                </div>\n",
    "                <div style=\"width: 48%; padding: 10px;\">\n",
    "                    <strong>Code Fix Example:</strong><br>\n",
    "                    <pre style=\"background-color: #2e2e2e; color: #f5f5f5; padding: 10px; border-radius: 5px; font-family: monospace;\">{defect_row[\"code fix example\"]}</pre>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "    \n",
    "    \n",
    "    <div style=\"border: 1px solid #ccc; padding: 10px; margin-top: 20px; border-radius: 5px;\">\n",
    "        <strong>Example Submission:</strong><br>\n",
    "        <pre style=\"background-color: #2e2e2e; color: #f5f5f5; padding: 10px; border-radius: 5px; font-family: monospace;\">{submissions[\"answer\"].iloc[random.randint(0, len(submissions) - 1)] if len(submissions) else 'No submissions found'}</pre>\n",
    "    </div>\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv(STORAGE_PATH / 'items.csv', index_col=0)\n",
    "defects = pd.read_csv(STORAGE_PATH / f'defects.csv', index_col=0)\n",
    "\n",
    "train_log = pd.read_csv(TRAINING_DATA_PATH / 'log.csv', index_col=0, parse_dates=['time'])\n",
    "train_defect_log = pd.read_csv(TRAINING_DATA_PATH / 'defect_log.csv', index_col=0)\n",
    "train_defect_log.columns = train_defect_log.columns.astype(int)\n",
    "\n",
    "test_log = pd.read_csv(VALIDATION_DATA_PATH / 'log.csv', index_col=0, parse_dates=['time'])\n",
    "test_defect_log = pd.read_csv(VALIDATION_DATA_PATH / 'defect_log.csv', index_col=0)\n",
    "test_defect_log.columns = test_defect_log.columns.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Task filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-defect pairs without minimal support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insufficient_support = (train_defect_log > 0).groupby(train_log[\"item\"]).sum() < MIN_TASK_DEFECT_SUBMISSIONS\n",
    "insufficient_support = insufficient_support.reindex(items.index).reindex(defects.index, axis=1).astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = items, defects\n",
    "\n",
    "models = [\n",
    "    TaskCommonModel(*data),\n",
    "    TaskCharacteristicModel(*data),\n",
    "    StudentCommonModel(*data),\n",
    "    StudentCharacteristicModel(*data),\n",
    "    StudentEncounteredBeforeModel(*data),\n",
    "    DefectMultiplicityModel(*data),\n",
    "    SeverityModel(*data),\n",
    "]\n",
    "\n",
    "models = {model.get_model_name(): model for model in models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in (pbar :=tqdm(models.items(), desc=\"Training Models\")):\n",
    "    pbar.set_description(f\"Training {name}\")\n",
    "    if RETRAIN_MODELS:\n",
    "        model.update(train_log, train_defect_log)\n",
    "    else:\n",
    "        models[name] = model.load(MODEL_OUTPUT_PATH / f\"{name}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_MODELS:\n",
    "    test_log = test_log.sort_values(by=['time'])\n",
    "\n",
    "    model_prioritizations = []\n",
    "\n",
    "    for idx, submission in tqdm(test_log.iterrows(), total=test_log.shape[0], desc=\"Calculating prioritizations\"):\n",
    "        defect_counts = test_defect_log.loc[idx]\n",
    "\n",
    "        model_scores = {name: model._calculate_scores(submission, defect_counts) for name, model in models.items()}\n",
    "        \n",
    "        for defect in defect_counts[defect_counts > 0].index:\n",
    "            row = {\"submission id\": idx, \"defect id\": defect}\n",
    "            for name, scores in model_scores.items():\n",
    "                row[name] = scores[defect]\n",
    "            model_prioritizations.append(row)\n",
    "\n",
    "        for model in models.values():\n",
    "            model.update(submission, defect_counts)\n",
    "\n",
    "    model_prioritizations = pd.DataFrame(model_prioritizations)\n",
    "\n",
    "    model_prioritizations.to_csv(EVALUATION_PRIORITIZATIONS_CACHE / 'model_scores.csv', index=False)\n",
    "else:\n",
    "    model_prioritizations = pd.read_csv(EVALUATION_PRIORITIZATIONS_CACHE / 'model_scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Weight histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_weight_histogram(model_name, model, image_dir, bins=60):\n",
    "    \"\"\"Plot a histogram of model weights.\"\"\"\n",
    "    values = model.get_model_weights()\n",
    "\n",
    "    if values is None:\n",
    "        print(f\"Model {model_name} has no weights. Skipping...\")\n",
    "        return\n",
    "\n",
    "    values = values.values.flatten()\n",
    "    thresholds = model.get_model_thresholds()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8), layout=\"constrained\")\n",
    "\n",
    "    n_unique = len(np.unique(values))\n",
    "    discrete = n_unique <= 10\n",
    "\n",
    "    sns.histplot(data=values, bins=bins if not discrete else n_unique, ax=ax, discrete=discrete, shrink=0.8)\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        ax.axvline(threshold, color='r', linestyle='--', linewidth=2)\n",
    "    \n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel(model.get_measure_name())\n",
    "    ax.set_ylabel('Frequency (Log Scale)')\n",
    "    ax.set_title(f'Distribution of {model.get_measure_name()}')\n",
    "    \n",
    "    plt.savefig(image_dir / model_name.lower().replace(\" \", \"_\"), dpi=RESOLUTION)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weight_histograms_dir = IMAGE_DIR / 'model_weight_histograms'\n",
    "os.makedirs(model_weight_histograms_dir, exist_ok=True)\n",
    "\n",
    "for model_name, model in tqdm(models.items(), desc=\"Plotting Model Weight Histograms\"):\n",
    "    if model_name == 'Defect Multiplicity':\n",
    "        # The histogram of the model weights is not well-defined for the Defect Multiplicity model. Instead, we use a simple approximation.\n",
    "        defect_counts = [1, 2, 3]\n",
    "        values = pd.DataFrame(\n",
    "            [model._calculate_scores(None, pd.Series(i, index=model.defects.index)) for i in defect_counts],\n",
    "            index=defect_counts,\n",
    "        )\n",
    "\n",
    "        values = values.values.flatten()\n",
    "        thresholds = model.get_model_thresholds()\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12, 8), layout=\"constrained\")\n",
    "\n",
    "        n_unique = len(np.unique(values))\n",
    "        discrete = n_unique <= 10\n",
    "\n",
    "        sns.histplot(data=values, bins=60, ax=ax, shrink=0.8)\n",
    "\n",
    "        for threshold in thresholds:\n",
    "            ax.axvline(threshold, color='r', linestyle='--', linewidth=2)\n",
    "        \n",
    "        ax.set_yscale('log')\n",
    "        ax.set_xlabel(model.get_measure_name())\n",
    "        ax.set_ylabel('Frequency (Log Scale)')\n",
    "        ax.set_title(f'Approximate Distribution of {model.get_measure_name()}')\n",
    "        \n",
    "        plt.savefig(model_weight_histograms_dir / model_name.lower().replace(\" \", \"_\"), dpi=RESOLUTION)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plot_model_weight_histogram(model_name, model, model_weight_histograms_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_score_histogram(\n",
    "    model_name, \n",
    "    values,\n",
    "    thresholds,\n",
    "    image_dir,\n",
    "    bins=60,\n",
    "):\n",
    "    \"\"\"Plot a histogram of model scores (from cached prioritizations).\"\"\"\n",
    "    values = np.asarray(values)\n",
    "\n",
    "    if values.size == 0:\n",
    "        print(f\"Model {model_name}: no values available — skipping.\")\n",
    "        return\n",
    "\n",
    "    # Decide whether discrete or continuous histogram\n",
    "    n_unique = len(np.unique(values))\n",
    "    discrete = n_unique <= 10  # same logic as your original\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8), layout=\"constrained\")\n",
    "\n",
    "    sns.histplot(\n",
    "        data=values,\n",
    "        bins=n_unique if discrete else bins,\n",
    "        ax=ax,\n",
    "        discrete=discrete,\n",
    "        shrink=0.8\n",
    "    )\n",
    "\n",
    "    # Add thresholds\n",
    "    for treshold in thresholds:\n",
    "        ax.axvline(treshold, color=\"r\", linestyle=\"--\", linewidth=2)\n",
    "\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel(model_name)\n",
    "    ax.set_ylabel(\"Frequency (log scale)\")\n",
    "    ax.set_title(f\"Distribution of Scores for {model_name}\")\n",
    "\n",
    "    save_path = image_dir / f\"{model_name.lower().replace(' ', '_')}.png\"\n",
    "    plt.savefig(save_path, dpi=RESOLUTION)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_score_histograms_dir = IMAGE_DIR / 'model_score_histograms'\n",
    "os.makedirs(model_score_histograms_dir, exist_ok=True)\n",
    "\n",
    "for model_name in models.keys():\n",
    "    values = model_prioritizations[model_name].values\n",
    "\n",
    "    thresholds = models[model_name].get_model_thresholds()\n",
    "\n",
    "    plot_model_score_histogram(\n",
    "        model_name=model_name,\n",
    "        values=values,\n",
    "        thresholds=thresholds,\n",
    "        image_dir=model_score_histograms_dir,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-defect weight maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_task_weight_heatmap(model_name, model, image_dir, defects, items, mask=None, normalize_cmap=False):\n",
    "    \"\"\"Plot a heatmap of task-defect model weights.\"\"\"\n",
    "    model_weights = model.get_model_weights().copy()\n",
    "\n",
    "    if model_weights is None:\n",
    "        print(f\"Model {model_name} has no weights. Skipping...\")\n",
    "        return\n",
    "\n",
    "    defect_names = defects['display name'].loc[model_weights.columns]\n",
    "    task_names = items['display name'].loc[model_weights.index]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 17), layout=\"constrained\")\n",
    "    \n",
    "    if normalize_cmap:\n",
    "        lim = max(np.abs(model_weights.values).max(), 1)\n",
    "        kwargs = {'cmap': 'coolwarm', 'vmin':-lim, 'vmax':lim}\n",
    "    else:\n",
    "        kwargs = {'cmap': 'Reds'}\n",
    "\n",
    "    model_weights[mask] = np.nan\n",
    "        \n",
    "    sns.heatmap(\n",
    "        model_weights, xticklabels=defect_names, yticklabels=task_names, cbar=True,\n",
    "        cbar_kws={'label': model.get_measure_name()}, **kwargs\n",
    "    )\n",
    "\n",
    "    ax.tick_params(axis='x', labelsize=7)\n",
    "    ax.tick_params(axis='y', labelsize=8)\n",
    "    plt.title(model.get_measure_name())\n",
    "    plt.xlabel(\"Defects\")\n",
    "    plt.ylabel(\"Tasks\")\n",
    "    title = model_name.lower().replace(\" \", \"_\")\n",
    "    plt.savefig(image_dir / (title if mask is None else title + \"_masked\"), dpi=RESOLUTION)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_defect_maps_dir = IMAGE_DIR / 'task_defect_weight_maps'\n",
    "os.makedirs(task_defect_maps_dir, exist_ok=True)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    if model.get_context_type() == \"task\":\n",
    "        normalize_cmap = model_name == \"Task Characteristic\"\n",
    "        plot_task_weight_heatmap(model_name, model, task_defect_maps_dir, defects, items, normalize_cmap=normalize_cmap)\n",
    "    else:\n",
    "        print(f\"Model {model_name} is not a task model. Skipping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    if model.get_context_type() == \"task\":\n",
    "        normalize_cmap = model_name == \"Task Characteristic\"\n",
    "        plot_task_weight_heatmap(model_name, model, task_defect_maps_dir, defects, items, normalize_cmap=normalize_cmap, mask=insufficient_support)\n",
    "    else:\n",
    "        print(f\"Model {model_name} is not a task model. Skipping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics_dir = IMAGE_DIR / 'model_metrics'\n",
    "os.makedirs(model_metrics_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Quantitative analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_MODELS:\n",
    "    # Prepare metric containers\n",
    "    metrics = {\n",
    "        'exact_ties':  {name: [] for name in models.keys()},\n",
    "        'close_ties':  {name: [] for name in models.keys()},\n",
    "        'gini':        {name: [] for name in models.keys()},\n",
    "    }\n",
    "\n",
    "    model_correlations = {\n",
    "        name1: {name2: [] for name2 in models.keys()}\n",
    "        for name1 in models.keys()\n",
    "    }\n",
    "\n",
    "    # Group by submission id to reconstruct each submission's defect set\n",
    "    submission_groups = model_prioritizations.groupby(\"submission id\")\n",
    "\n",
    "    for submission_id, group in tqdm(submission_groups, desc=\"Computing metrics from cached prioritizations\"):\n",
    "        if group.shape[0] < 2:\n",
    "            # Too few defects to compute prioritization metrics. Skip or append zeros—keeping skip to match your original logic.\n",
    "            continue\n",
    "\n",
    "        # Reconstruct score vectors\n",
    "        model_scores = {\n",
    "            name: group[name].values.astype(float)\n",
    "            for name in models.keys()\n",
    "        }\n",
    "\n",
    "        # Per-model metrics\n",
    "        for name, scores in model_scores.items():\n",
    "            max_score = scores.max()\n",
    "\n",
    "            # Exact ties\n",
    "            metrics['exact_ties'][name].append(int((scores == max_score).sum() > 1))\n",
    "\n",
    "            # Close ties\n",
    "            close_threshold_value = max_score * CLOSE_TIE_THRESHOLD\n",
    "            metrics['close_ties'][name].append(\n",
    "                int((scores >= (max_score - close_threshold_value)).sum() > 1)\n",
    "            )\n",
    "\n",
    "            # Gini coefficient\n",
    "            metrics['gini'][name].append(gini(scores))\n",
    "\n",
    "        # Inter-model Spearman correlations\n",
    "        for i, name_i in enumerate(models.keys()):\n",
    "            for j, name_j in enumerate(models.keys()):\n",
    "                if i >= j:\n",
    "                    continue\n",
    "\n",
    "                arr_i = model_scores[name_i]\n",
    "                arr_j = model_scores[name_j]\n",
    "\n",
    "                # Handle constant arrays (undefined Spearman)\n",
    "                if np.all(arr_i == arr_i[0]) or np.all(arr_j == arr_j[0]):\n",
    "                    rho = np.nan\n",
    "                else:\n",
    "                    rho, _ = spearmanr(arr_i, arr_j)\n",
    "\n",
    "                model_correlations[name_i][name_j].append(rho)\n",
    "                model_correlations[name_j][name_i].append(rho)\n",
    "\n",
    "    # Save results\n",
    "    with open(MODEL_METRICS_PATH / \"metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics, f)\n",
    "\n",
    "    with open(MODEL_METRICS_PATH / \"model_correlations.json\", \"w\") as f:\n",
    "        json.dump(model_correlations, f)\n",
    "else:\n",
    "    with open(MODEL_METRICS_PATH / \"metrics.json\", \"r\") as f:\n",
    "        metrics = json.load(f)\n",
    "    with open(MODEL_METRICS_PATH / \"model_correlations.json\", \"r\") as f:\n",
    "        model_correlations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final Aggregation and Formatting ---\n",
    "results = {}\n",
    "for metric_name, data in metrics.items():\n",
    "    avg_values = {name: np.mean(values) for name, values in data.items()}\n",
    "    results[f'avg_{metric_name}'] = pd.Series(avg_values)\n",
    "\n",
    "print(f\"Average Exact Ties: \\n{results['avg_exact_ties']}\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"Average Close Ties: \\n{results['avg_close_ties']}\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"Average Gini Coefficients: \\n{results['avg_gini']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decisivness - Ties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = pd.DataFrame({\n",
    "    'Exact Ties': results['avg_exact_ties'],\n",
    "    'Close Ties': results['avg_close_ties'] - results['avg_exact_ties']\n",
    "}).T.reset_index()\n",
    "\n",
    "plot_data = plot_data.rename(columns={'index': 'Tie Type'})\n",
    "\n",
    "plot_data_melted = plot_data.melt(id_vars='Tie Type', var_name='Model', value_name='Average Count')\n",
    "\n",
    "ax = plot_data.set_index('Tie Type').T.plot(\n",
    "    kind='bar', \n",
    "    stacked=True, \n",
    "    figsize=(12, 8),\n",
    "    colormap='tab10'\n",
    ")\n",
    "\n",
    "plt.title(\"Average Number of Exact and Close Ties per Model\", fontsize=16)\n",
    "plt.ylabel(\"Average Count\", fontsize=12)\n",
    "plt.xlabel(\"Prioritization Model\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "plt.legend(title='Tie Type')\n",
    "plt.tight_layout()\n",
    "plt.savefig(model_metrics_dir / 'ties_bar_plot.png', dpi=RESOLUTION)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decisivness - Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = []\n",
    "for model_name, gini_list in metrics['gini'].items():\n",
    "    for gini_value in gini_list:\n",
    "        plot_data.append({'Model': model_name, 'Gini Coefficient': gini_value})\n",
    "\n",
    "plot_data = pd.DataFrame(plot_data)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "sns.boxplot(\n",
    "    x='Model',\n",
    "    y='Gini Coefficient',\n",
    "    data=plot_data,\n",
    "    notch=True,\n",
    "    palette='viridis',\n",
    "    hue='Model',\n",
    "    legend=False\n",
    ")\n",
    "\n",
    "plt.title(\"Distribution of Gini Coefficients by Prioritization Model\", fontsize=16)\n",
    "plt.xlabel(\"Prioritization Model\", fontsize=12)\n",
    "plt.ylabel(\"Gini Coefficient\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(model_metrics_dir / 'gini_box_plot.png', dpi=RESOLUTION)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inter-Model Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = np.empty((len(models.keys()), len(models.keys())))\n",
    "correlation_matrix[:] = np.nan\n",
    "\n",
    "for i, first in enumerate(models.keys()):\n",
    "    for j, second in enumerate(models.keys()):\n",
    "        if i >= j:\n",
    "            continue\n",
    "        correlation_matrix[i, j] = np.nanmean(model_correlations[first][second])\n",
    "        correlation_matrix[j, i] = correlation_matrix[i, j]\n",
    "\n",
    "correlation_matrix = pd.DataFrame(correlation_matrix, index=models.keys(), columns=models.keys())\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "sns.heatmap(\n",
    "    correlation_matrix,\n",
    "    annot=True,\n",
    "    cmap='coolwarm',\n",
    "    fmt=\".2f\",\n",
    "    linewidths=.5,\n",
    "    cbar_kws={'label': \"Average Spearman's Correlation\"},\n",
    "    vmin=-1,\n",
    "    vmax=1\n",
    ")\n",
    "\n",
    "plt.title(\"Inter-Model Prioritization Agreement (Spearman's Rho)\", fontsize=16)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(model_metrics_dir / 'model_correlation_heatmap.png', dpi=RESOLUTION)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agreement with baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = []\n",
    "for model, corr_list in model_correlations['Naive Severity'].items():\n",
    "    for corr in corr_list:\n",
    "        plot_data.append({'Model': model, 'Spearman_Rho': corr})\n",
    "\n",
    "plot_data = pd.DataFrame(plot_data)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "sns.boxplot(\n",
    "    x='Model',\n",
    "    y='Spearman_Rho',\n",
    "    data=plot_data,\n",
    "    palette='rocket',\n",
    "    hue='Model',\n",
    "    legend=False,\n",
    "    notch=True\n",
    ")\n",
    "\n",
    "plt.title(\"Distribution of Spearman's Rho by Prioritization Model\", fontsize=16)\n",
    "plt.xlabel(\"Prioritization Model\", fontsize=12)\n",
    "plt.ylabel(\"Spearman's Rho\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(model_metrics_dir / 'agreement_with_baseline_box_plot.png', dpi=RESOLUTION)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Calculating correlation matrix for {name}\")\n",
    "    \n",
    "    weights = model.get_model_weights()\n",
    "\n",
    "    # Handle single-submission and context-less models\n",
    "    if weights is None or weights.shape[0] == 1:\n",
    "        matrices[name] = pd.DataFrame(1.0, index=items.index, columns=items.index)\n",
    "        continue\n",
    "\n",
    "    # Filter out constant rows (for spearman correlation)\n",
    "    is_constant_row = weights.apply(lambda row: np.all(row == row.iloc[0]), axis=1)\n",
    "    filtered_weights = weights[~is_constant_row]\n",
    "\n",
    "    correlation_matrix = filtered_weights.T.corr(method='spearman')\n",
    "\n",
    "    # Fill NaNs (which correspond to constant rows or missing tasks) with 1.0 (perfect agreement)\n",
    "    correlation_matrix = correlation_matrix.reindex(index=weights.index, columns=weights.index)\n",
    "    np.fill_diagonal(correlation_matrix.values, 1.0)\n",
    "    \n",
    "    matrices[name] = correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_matrices = {\n",
    "    name: matrix for name, matrix in matrices.items() if models[name].get_context_type() == 'task'\n",
    "}\n",
    "\n",
    "ncols = 2\n",
    "nrows = (len(task_matrices) + ncols - 1) // ncols\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=nrows, ncols=ncols, figsize=(10 * ncols, 9 * nrows),\n",
    "    sharex=True, sharey=True,\n",
    "    gridspec_kw={'wspace': 0.05, 'hspace': 0.05} # Fine-tune the spacing\n",
    ")\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (name, correlation_matrix) in enumerate(task_matrices.items()):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    sns.heatmap(\n",
    "        correlation_matrix.astype(float),\n",
    "        ax=ax,\n",
    "        cmap='coolwarm',\n",
    "        vmin=-1,\n",
    "        vmax=1,\n",
    "        linewidths=.5,\n",
    "        cbar=False,\n",
    "    )\n",
    "\n",
    "    ax.set_title(f\"Task-to-Task Agreement for {name}\", fontsize=18)\n",
    "    if i in [2, 3]:\n",
    "        ax.set_xlabel('Task ID', fontsize=12)\n",
    "    else:\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_xticklabels([])\n",
    "    if i in [0, 2]:\n",
    "        ax.set_ylabel('Task ID', fontsize=12)\n",
    "    else:\n",
    "        ax.set_ylabel('')\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "# Create a single color bar for the entire figure\n",
    "fig.subplots_adjust(right=0.85)\n",
    "cbar_ax = fig.add_axes([0.88, 0.15, 0.02, 0.7])\n",
    "last_heatmap = axes[-1].collections[0] if len(task_matrices) % 2 == 0 else axes[-2].collections[0]\n",
    "cbar = fig.colorbar(last_heatmap, cax=cbar_ax)\n",
    "cbar.set_label(\"Spearman's Correlation\", fontsize=16)\n",
    "\n",
    "plt.savefig(model_metrics_dir / 'task_model_sensitivity_heatmaps.png', dpi=RESOLUTION)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_devs = {}\n",
    "for name, correlation_matrix in matrices.items():\n",
    "    upper_triangle_mask = np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)\n",
    "    off_diagonal_values = correlation_matrix.where(upper_triangle_mask).stack().values\n",
    "    off_diagonal_std = off_diagonal_values.std()\n",
    "    \n",
    "    std_devs[name] = off_diagonal_std\n",
    "\n",
    "std_df = pd.DataFrame(std_devs.items(), columns=['Model', 'Standard Deviation'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Model', y='Standard Deviation', data=std_df, palette='viridis', hue='Model', legend=False)\n",
    "\n",
    "plt.title(\"Standard Deviation of Inter-Task Correlation\", fontsize=16)\n",
    "plt.ylabel(\"Standard Deviation of Spearman's Rho\", fontsize=12)\n",
    "plt.xlabel(\"Prioritization Model\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(model_metrics_dir / 'sensitivity_bar_plot.png', dpi=RESOLUTION)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Export models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_MODELS:\n",
    "    for name, model in (pbar :=tqdm(models.items(), desc=\"Training Models\")):\n",
    "        pbar.set_description(f\"Training {name}\")\n",
    "        model.save(MODEL_OUTPUT_PATH / f\"{name}.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
