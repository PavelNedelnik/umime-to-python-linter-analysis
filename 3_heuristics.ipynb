{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "from matplotlib.colors import ListedColormap\n",
    "from IPython.display import display, HTML\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "import src.ipython_loader as loader\n",
    "from src.prioritization import *\n",
    "\n",
    "RESOLUTION = 300\n",
    "VERSION = '0.0.0'\n",
    "DATASET_PATH = Path('data') / 'datasets' / f'ipython_{VERSION}'\n",
    "OUTPUT_PATH = DATASET_PATH / 'heuristics'\n",
    "BINARY_CMAP = ListedColormap(['red', 'green'])\n",
    "\n",
    "IMAGE_DIR = Path('images') / \"heuristics\"\n",
    "\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "\n",
    "MIN_TASK_DEFECT_SUBMISSIONS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_text(text: str, max_length: int=20):\n",
    "    \"\"\"Truncate text if it exceeds the maximum length.\"\"\"\n",
    "    if len(text) > max_length:\n",
    "        return text[:max_length - 3] + '...'\n",
    "    return text\n",
    "\n",
    "def abbreviate_and_truncate_text(text: str, ordered_abbreviations: dict | None=None, max_length: int=20):\n",
    "    \"\"\"Shorten text by applying a list of abbreviations and truncating if necessary.\"\"\"\n",
    "    current_text = text\n",
    "\n",
    "    if ordered_abbreviations:\n",
    "        for full_word, abbr in ordered_abbreviations.items():\n",
    "            # Use regex with word boundaries to ensure we replace full words only\n",
    "            pattern = re.escape(full_word)\n",
    "            current_text = re.sub(pattern, abbr, current_text, flags=re.IGNORECASE)\n",
    "\n",
    "            if len(current_text) <= max_length:\n",
    "                return current_text\n",
    "    \n",
    "    return truncate_text(current_text)\n",
    "\n",
    "ordered_abbreviations = {\n",
    "    'whitespace': 'ws',\n",
    "    'constant': 'const',\n",
    "    'variable': 'var',\n",
    "    'function': 'func',\n",
    "    'parameter': 'param',\n",
    "    'expression': 'expr',\n",
    "    'argument': 'arg',\n",
    "    'operator': 'op',\n",
    "    'operation': 'op',\n",
    "    'augmentable': 'aug',\n",
    "    'assignment': 'assign',\n",
    "    'container': 'cont',\n",
    "    'statement': 'stmt',\n",
    "    'arithmetic': 'arith',\n",
    "    'condition': 'cond',\n",
    "    'identifier': 'identif',\n",
    "    'multiple': 'multi',\n",
    "    'redundant': 'redun',\n",
    "    'necessary': 'necces',\n",
    "    'comparison': 'compar',\n",
    "    'negated': 'neg',\n",
    "    'unreachable': 'unreach',\n",
    "    'inappropriate': 'inapp',\n",
    "    'parenthesis': '()',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_and_defect_description(task, defect, items, defects, log, defect_log):\n",
    "    \"\"\"Generate an HTML display for a specific task and defect.\"\"\"\n",
    "    task_row = items.loc[task]\n",
    "    defect_row = defects.loc[defect]\n",
    "    submissions = log[(log[\"item\"] == task) & (defect_log[defect])]\n",
    "    \n",
    "    return f\"\"\"\n",
    "    <div style=\"display: flex; justify-content: space-between; gap: 20px;\">\n",
    "        \n",
    "        <div style=\"width: 48%; border: 1px solid #ccc; padding: 10px; border-radius: 5px;\">\n",
    "            <h3>{task_row[\"name\"]}</h3>\n",
    "            <div><strong>Instructions:</strong><br>{task_row[\"instructions\"]}</div>\n",
    "            <div><strong>Solution:</strong><br>\n",
    "                <pre style=\"background-color: #2e2e2e; color: #f5f5f5; padding: 10px; border-radius: 5px; font-family: monospace;\">{task_row[\"solution\"]}</pre>\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        \n",
    "        <div style=\"width: 48%; border: 1px solid #ccc; padding: 10px; border-radius: 5px;\">\n",
    "            <h3>{defect_row[\"defect name\"]}</h3>\n",
    "            <div><strong>Defect Type:</strong> {defect_row[\"defect type\"]}</div>\n",
    "            <div><strong>Severity:</strong> {defect_row[\"severity\"]}</div>\n",
    "            <div><strong>Description:</strong><br>{defect_row[\"description\"]}</div>\n",
    "            \n",
    "            <div style=\"display: flex; justify-content: space-between; margin-top: 20px;\">\n",
    "                <div style=\"width: 48%; padding: 10px;\">\n",
    "                    <strong>Code Example:</strong><br>\n",
    "                    <pre style=\"background-color: #2e2e2e; color: #f5f5f5; padding: 10px; border-radius: 5px; font-family: monospace;\">{defect_row[\"code example\"]}</pre>\n",
    "                </div>\n",
    "                <div style=\"width: 48%; padding: 10px;\">\n",
    "                    <strong>Code Fix Example:</strong><br>\n",
    "                    <pre style=\"background-color: #2e2e2e; color: #f5f5f5; padding: 10px; border-radius: 5px; font-family: monospace;\">{defect_row[\"code fix example\"]}</pre>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "    \n",
    "    \n",
    "    <div style=\"border: 1px solid #ccc; padding: 10px; margin-top: 20px; border-radius: 5px;\">\n",
    "        <strong>Example Submission:</strong><br>\n",
    "        <pre style=\"background-color: #2e2e2e; color: #f5f5f5; padding: 10px; border-radius: 5px; font-family: monospace;\">{submissions[\"answer\"].iloc[random.randint(0, len(submissions) - 1)] if len(submissions) else 'No submissions found'}</pre>\n",
    "    </div>\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_weight_histogram(values, x_label, title, save_path, bins=80):\n",
    "    \"\"\"Plot a basic histogram.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 8), layout=\"constrained\")\n",
    "\n",
    "    n_unique = len(np.unique(values))\n",
    "\n",
    "    sns.histplot(data=values, bins=bins if values.size > 2 else n_unique, shrink=0.8, ax=ax)\n",
    "    \n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel('Frequency (Log Scale)')\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    plt.savefig(IMAGE_DIR / save_path, dpi=RESOLUTION)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_task_weight_heatmap(model_weights, defects, items, title, save_path, mask=None, cmap='viridis'):\n",
    "    \"\"\"Plot a heatmap of task-context model weights.\"\"\"\n",
    "    defect_names = defects['display name'].loc[model_weights.columns]\n",
    "    task_names = items['display name'].loc[model_weights.index]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 17), layout=\"constrained\")\n",
    "    \n",
    "    if cmap == 'coolwarm':\n",
    "        lim = np.abs(model_weights.values).max()\n",
    "        sns.heatmap(model_weights, xticklabels=defect_names, yticklabels=task_names, cbar=True, cmap=cmap, vmin=-lim, vmax=lim, mask=mask)\n",
    "    else:\n",
    "        sns.heatmap(model_weights, xticklabels=defect_names, yticklabels=task_names, cbar=True, cmap=cmap, mask=mask)\n",
    "\n",
    "    ax.tick_params(axis='x', labelsize=7)\n",
    "    ax.tick_params(axis='y', labelsize=8)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Defects\")\n",
    "    plt.ylabel(\"Tasks\")\n",
    "    plt.savefig(IMAGE_DIR / save_path, dpi=RESOLUTION)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_context(log, defect_log, defects, items, pct=0.8):\n",
    "    \"\"\"Define train/test split for task-context models.\"\"\"\n",
    "    train_pivot = log.iloc[int(log.shape[0] * pct)][\"time\"]\n",
    "\n",
    "    train_context = ContextProvider(log[log[\"time\"] < train_pivot], defect_log[log[\"time\"] < train_pivot], defects, items)\n",
    "    test_context = ContextProvider(log[log[\"time\"] > train_pivot], defect_log[log[\"time\"] > train_pivot], defects, items)\n",
    "\n",
    "    return train_context, test_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(array):\n",
    "    \"\"\"Compute the Gini coefficient of a sorted numpy array.\"\"\"\n",
    "    array = array.flatten()\n",
    "    # avoid zero division\n",
    "    if array.sum() == 0:\n",
    "        return 0.0\n",
    "    # ensure all values are non-negative\n",
    "    if np.amin(array) < 0:\n",
    "        array -= np.amin(array)\n",
    "    # order values for computation\n",
    "    array = np.sort(array)\n",
    "    index = np.arange(1, array.shape[0] + 1)\n",
    "    # gini formula\n",
    "    return ((np.sum((2 * index - array.shape[0] - 1) * array)) / (array.shape[0] * np.sum(array)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv(DATASET_PATH / f'items_{VERSION}.csv', index_col=0)\n",
    "log = pd.read_csv(DATASET_PATH / f'log_{VERSION}.csv', index_col=0, parse_dates=['time'])\n",
    "defects = pd.read_csv(DATASET_PATH / f'defects_{VERSION}.csv', index_col=0)\n",
    "defect_log = pd.read_csv(DATASET_PATH / f'defect_log_{VERSION}.csv', index_col=0)\n",
    "defect_log.columns = defect_log.columns.astype(int)\n",
    "code_to_defect_id = json.load(open(DATASET_PATH / f'code_to_defect_id_{VERSION}.json', \"r\"))\n",
    "defect_presence = defect_log > 0\n",
    "\n",
    "full_context = ContextProvider(log, defect_log, defects, items)\n",
    "\n",
    "defects['display name'] = defects['defect name'].apply(lambda x: abbreviate_and_truncate_text(x, ordered_abbreviations))\n",
    "items['display name'] = items['name'].apply(truncate_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Task filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-defect pairs without minimal support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insufficient_support = defect_presence.groupby(log[\"item\"]).sum() < MIN_TASK_DEFECT_SUBMISSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Task-Context Heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_context, test_context = get_train_test_context(log, defect_log, defects, items, pct=0.8)\n",
    "\n",
    "task_models = {\n",
    "    \"Task Common\": TaskCommonModel().train(train_context),\n",
    "    \"Task Characteristic\": TaskCharacteristicModel().train(train_context),\n",
    "    \"Currently Taught\": CurrentlyTaughtPrioritizer('data/currently_taught.txt').train(train_context),\n",
    "    \"Severity Baseline\": SeverityModel().train(train_context)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submissions meaningful for prioritization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_defects = test_context.get_defect_log() > 0\n",
    "print(\"Fraction of Submissions with Multiple Defects:\", (test_defects.sum(axis=1) > 1).mean())\n",
    "print(\"Total Number\", (test_defects.sum(axis=1) > 1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Weight Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_weight_histogram(\n",
    "    task_models[\"Task Common\"].get_model_weights().values.flatten(),\n",
    "    x_label=\"Task-Relative Defect Frequency\",\n",
    "    title=\"Distribution of Task-Defect Commonality\",\n",
    "    save_path=\"task_common_distribution.png\",\n",
    "    bins=80\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_weight_histogram(\n",
    "    task_models[\"Task Characteristic\"].get_model_weights().values.flatten(),\n",
    "    x_label=\"Z-Score (Absolute Value)\",\n",
    "    title=\"Distribution of Characteristic Task-Defect Scores\",\n",
    "    save_path=\"task_characteristic_distribution.png\",\n",
    "    bins=80\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_weight_histogram(\n",
    "    task_models[\"Currently Taught\"].get_model_weights().values.flatten(),\n",
    "    x_label=\"LLM Judgment (0 = No, 1 = Yes)\",\n",
    "    title=\"Distribution of LLM Judgments on Currently Taught Defects\",\n",
    "    save_path=\"currently_taught_distribution.png\",\n",
    "    bins=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-Defect Weight Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_task_weight_heatmap(\n",
    "    task_models[\"Task Common\"].get_model_weights(),\n",
    "    defects,\n",
    "    items,\n",
    "    \"Task-Defect Commonality\",\n",
    "    \"task_common_heatmap.png\",\n",
    "    mask=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_task_weight_heatmap(\n",
    "    task_models[\"Task Characteristic\"].get_model_weights(),\n",
    "    defects,\n",
    "    items,\n",
    "    \"Characteristic Task-Defect Scores\",\n",
    "    \"task_characteristic_heatmap.png\",\n",
    "    mask=None,\n",
    "    cmap='coolwarm'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_task_weight_heatmap(\n",
    "    task_models[\"Currently Taught\"].get_model_weights(),\n",
    "    defects,\n",
    "    items,\n",
    "    \"LLM Judgments on Currently Taught Defects\",\n",
    "    \"currently_taught_heatmap.png\",\n",
    "    mask=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked Task-Defect Weight Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_task_weight_heatmap(\n",
    "    task_models[\"Task Common\"].get_model_weights(),\n",
    "    defects,\n",
    "    items,\n",
    "    \"Task-Defect Commonality\",\n",
    "    \"task_common_masked_heatmap.png\",\n",
    "    mask=insufficient_support\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_task_weight_heatmap(\n",
    "    task_models[\"Task Characteristic\"].get_model_weights(),\n",
    "    defects,\n",
    "    items,\n",
    "    \"Characteristic Task-Defect Scores\",\n",
    "    \"task_characteristic_masked_heatmap.png\",\n",
    "    mask=insufficient_support,\n",
    "    cmap='coolwarm'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_task_weight_heatmap(\n",
    "    task_models[\"Currently Taught\"].get_model_weights(),\n",
    "    defects,\n",
    "    items,\n",
    "    \"LLM Judgments on Currently Taught Defects\",\n",
    "    \"currently_taught_masked_heatmap.png\",\n",
    "    mask=insufficient_support\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLOSE_TIE_THRESHOLD = 0.05  # threshold for close ties as a percentage of the max score\n",
    "\n",
    "metrics = {\n",
    "    'exact_ties': {name: [] for name in task_models.keys()},\n",
    "    'close_ties': {name: [] for name in task_models.keys()},\n",
    "    'gini': {name: [] for name in task_models.keys()}\n",
    "}\n",
    "\n",
    "model_correlations = {\n",
    "    name1: {name2: [] for name2 in task_models.keys()}\n",
    "    for name1 in task_models.keys()\n",
    "}\n",
    "\n",
    "test_log = test_context.get_log()\n",
    "for _, submission_log in tqdm(test_log.iterrows(), total=test_log.shape[0]):\n",
    "    defect_counts = test_context.get_defect_log().loc[submission_log.name]\n",
    "    present_defects = defect_counts[defect_counts > 0]\n",
    "    \n",
    "    # Skip submissions with too few issues to prioritize\n",
    "    if len(present_defects) < 2:\n",
    "        continue\n",
    "\n",
    "    prioritizations = []\n",
    "\n",
    "    for name, model in task_models.items():\n",
    "        scores = model.prioritize(submission_log, present_defects)\n",
    "        \n",
    "        # --- Exact Ties ---\n",
    "        # Count items with the same score as the max\n",
    "        max_score = scores.max()\n",
    "        exact_ties = (scores == max_score).sum() > 1\n",
    "        metrics['exact_ties'][name].append(exact_ties)\n",
    "\n",
    "        # --- Close Ties ---\n",
    "        # Count items within a relative threshold of the max score\n",
    "        close_tie_threshold_value = max_score * CLOSE_TIE_THRESHOLD\n",
    "        close_ties = (scores >= (max_score - close_tie_threshold_value)).sum() > 1\n",
    "        metrics['close_ties'][name].append(close_ties)\n",
    "\n",
    "        # --- Gini Coefficient ---\n",
    "        metrics['gini'][name].append(gini(scores.values))\n",
    "\n",
    "        # Store the prioritization to calculate intra-model agreement\n",
    "        prioritizations.append(scores.values)\n",
    "    \n",
    "    # --- Agreement ---\n",
    "    for i, first in enumerate(task_models.keys()):\n",
    "        for j, second in enumerate(task_models.keys()):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            first_score = prioritizations[i]\n",
    "            second_score = prioritizations[j]\n",
    "            # Check for constant arrays\n",
    "            if np.all(first_score == first_score[0]) or np.all(second_score == second_score[0]):\n",
    "                rho = np.nan\n",
    "            else:\n",
    "                rho, _ = spearmanr(prioritizations[i], prioritizations[j])\n",
    "            model_correlations[first][second].append(rho)\n",
    "            model_correlations[second][first].append(rho)\n",
    "\n",
    "# --- Final Aggregation and Formatting ---\n",
    "results = {}\n",
    "for metric_name, data in metrics.items():\n",
    "    avg_values = {name: np.mean(values) for name, values in data.items()}\n",
    "    results[f'avg_{metric_name}'] = pd.Series(avg_values)\n",
    "\n",
    "print(f\"Average Exact Ties: \\n{results['avg_exact_ties']}\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"Average Close Ties: \\n{results['avg_close_ties']}\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"Average Gini Coefficients: \\n{results['avg_gini']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decisivness - Ties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = pd.DataFrame({\n",
    "    'Exact Ties': results['avg_exact_ties'],\n",
    "    'Close Ties': results['avg_close_ties'] - results['avg_exact_ties']\n",
    "}).T.reset_index()\n",
    "\n",
    "plot_data = plot_data.rename(columns={'index': 'Tie Type'})\n",
    "\n",
    "plot_data_melted = plot_data.melt(id_vars='Tie Type', var_name='Model', value_name='Average Count')\n",
    "\n",
    "ax = plot_data.set_index('Tie Type').T.plot(\n",
    "    kind='bar', \n",
    "    stacked=True, \n",
    "    figsize=(12, 8),\n",
    "    colormap='tab10'\n",
    ")\n",
    "\n",
    "plt.title(\"Average Number of Exact and Close Ties per Model\", fontsize=16)\n",
    "plt.ylabel(\"Average Count\", fontsize=12)\n",
    "plt.xlabel(\"Prioritization Model\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "plt.legend(title='Tie Type')\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / 'task_models_tie_bar_plot.png', dpi=RESOLUTION)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decisivness - Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = []\n",
    "for model_name, gini_list in metrics['gini'].items():\n",
    "    for gini_value in gini_list:\n",
    "        plot_data.append({'Model': model_name, 'Gini Coefficient': gini_value})\n",
    "\n",
    "plot_data = pd.DataFrame(plot_data)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "sns.boxplot(\n",
    "    x='Model',\n",
    "    y='Gini Coefficient',\n",
    "    data=plot_data,\n",
    "    notch=True,\n",
    "    palette='viridis',\n",
    "    hue='Model',\n",
    "    legend=False\n",
    ")\n",
    "\n",
    "plt.title(\"Distribution of Gini Coefficients by Prioritization Model\", fontsize=16)\n",
    "plt.xlabel(\"Prioritization Model\", fontsize=12)\n",
    "plt.ylabel(\"Gini Coefficient\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(IMAGE_DIR / 'task_models_gini_box_plot.png', dpi=RESOLUTION)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inter-Model Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = np.empty((len(task_models.keys()), len(task_models.keys())))\n",
    "correlation_matrix[:] = np.nan\n",
    "\n",
    "for i, first in enumerate(task_models.keys()):\n",
    "    for j, second in enumerate(task_models.keys()):\n",
    "        if i >= j:\n",
    "            continue\n",
    "        correlation_matrix[i, j] = np.nanmean(model_correlations[first][second])\n",
    "        correlation_matrix[j, i] = correlation_matrix[i, j]\n",
    "\n",
    "correlation_matrix = pd.DataFrame(correlation_matrix, index=task_models.keys(), columns=task_models.keys())\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "sns.heatmap(\n",
    "    correlation_matrix,\n",
    "    annot=True,\n",
    "    cmap='coolwarm',\n",
    "    fmt=\".2f\",\n",
    "    linewidths=.5,\n",
    "    cbar_kws={'label': \"Average Spearman's Correlation\"},\n",
    "    vmin=-1,\n",
    "    vmax=1\n",
    ")\n",
    "\n",
    "plt.title(\"Inter-Model Prioritization Agreement (Spearman's Rho)\", fontsize=16)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(IMAGE_DIR / 'task_models_correlation_heatmap.png', dpi=RESOLUTION)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agreement with baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = []\n",
    "for model, corr_list in model_correlations['Severity Baseline'].items():\n",
    "    for corr in corr_list:\n",
    "        plot_data.append({'Model': model, 'Spearman_Rho': corr})\n",
    "\n",
    "plot_data = pd.DataFrame(plot_data)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "sns.boxplot(\n",
    "    x='Model',\n",
    "    y='Spearman_Rho',\n",
    "    data=plot_data,\n",
    "    palette='rocket',\n",
    "    hue='Model',\n",
    "    legend=False,\n",
    "    notch=True\n",
    ")\n",
    "\n",
    "plt.title(\"Distribution of Spearman's Rho by Prioritization Model\", fontsize=16)\n",
    "plt.xlabel(\"Prioritization Model\", fontsize=12)\n",
    "plt.ylabel(\"Spearman's Rho\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(IMAGE_DIR / 'task_models_baseline_correlation_box_plot.png', dpi=RESOLUTION)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_task_models = {\n",
    "    name: model.train(full_context) for name, model in task_models.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices = {}\n",
    "for name, model in tqdm(full_task_models.items(), desc=\"Analyzing Task Models\"):\n",
    "    weights = model.get_model_weights()\n",
    "\n",
    "    correlation_matrix = pd.DataFrame(index=items.index, columns=items.index)\n",
    "\n",
    "    if weights is None:\n",
    "        matrix_values = np.ones((len(items.index), len(items.index)))\n",
    "        np.fill_diagonal(matrix_values, 0)\n",
    "        correlation_matrix = pd.DataFrame(matrix_values, index=items.index, columns=items.index)\n",
    "        matrices[name] = correlation_matrix\n",
    "        continue\n",
    "\n",
    "    for first in items.index:\n",
    "        for second in items.index:\n",
    "            if first >= second:\n",
    "                continue\n",
    "            weights_first = weights.loc[first]\n",
    "            weights_second = weights.loc[second]\n",
    "\n",
    "            if np.all(weights_first == weights_first.iloc[0]) or np.all(weights_second == weights_second.iloc[0]):\n",
    "                rho = np.nan\n",
    "            else:\n",
    "                rho, _ = spearmanr(weights_first, weights_second)\n",
    "\n",
    "            correlation_matrix.loc[first, second] = rho\n",
    "            correlation_matrix.loc[second, first] = rho\n",
    "    matrices[name] = correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    nrows=2, ncols=2, figsize=(20, 18),\n",
    "    sharex=True, sharey=True,\n",
    "    gridspec_kw={'wspace': 0.05, 'hspace': 0.05} # Fine-tune the spacing\n",
    ")\n",
    "axes = axes.flatten()\n",
    "\n",
    "models = [\"Task Common\", \"Task Characteristic\", \"Currently Taught\", \"Severity Baseline\"]\n",
    "cmaps = ['coolwarm', 'coolwarm', 'coolwarm', 'coolwarm']\n",
    "\n",
    "for i, name in enumerate(models):\n",
    "    ax = axes[i]\n",
    "    correlation_matrix = matrices[name]\n",
    "    \n",
    "    sns.heatmap(\n",
    "        correlation_matrix.astype(float),\n",
    "        ax=ax,\n",
    "        cmap=cmaps[i],\n",
    "        vmin=-1,\n",
    "        vmax=1,\n",
    "        linewidths=.5,\n",
    "        cbar=False,\n",
    "    )\n",
    "\n",
    "    ax.set_title(f\"Task-to-Task Agreement for {name}\", fontsize=18)\n",
    "    if i in [2, 3]:\n",
    "        ax.set_xlabel('Task ID', fontsize=12)\n",
    "    else:\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_xticklabels([])\n",
    "    if i in [0, 2]:\n",
    "        ax.set_ylabel('Task ID', fontsize=12)\n",
    "    else:\n",
    "        ax.set_ylabel('')\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "# Create a single color bar for the entire figure\n",
    "fig.subplots_adjust(right=0.85)\n",
    "cbar_ax = fig.add_axes([0.88, 0.15, 0.02, 0.7])\n",
    "last_heatmap = axes[-1].collections[0]\n",
    "cbar = fig.colorbar(last_heatmap, cax=cbar_ax)\n",
    "cbar.set_label(\"Spearman's Correlation\", fontsize=16)\n",
    "\n",
    "plt.savefig(IMAGE_DIR / 'task_to_task_correlation_heatmaps.png', dpi=RESOLUTION)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_devs = {}\n",
    "for name, correlation_matrix in matrices.items():\n",
    "    upper_triangle_mask = np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)\n",
    "    off_diagonal_values = correlation_matrix.where(upper_triangle_mask).stack().values\n",
    "    off_diagonal_std = off_diagonal_values.std()\n",
    "    \n",
    "    std_devs[name] = off_diagonal_std\n",
    "\n",
    "std_df = pd.DataFrame(std_devs.items(), columns=['Model', 'Standard Deviation'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Model', y='Standard Deviation', data=std_df, palette='viridis', hue='Model', legend=False)\n",
    "\n",
    "plt.title(\"Standard Deviation of Inter-Task Correlation\", fontsize=16)\n",
    "plt.ylabel(\"Standard Deviation of Spearman's Rho\", fontsize=12)\n",
    "plt.xlabel(\"Prioritization Model\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(IMAGE_DIR / 'task_models_sensitivity_bar_plot.png', dpi=RESOLUTION)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Student-Context Heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_models = {\n",
    "    \"Student Frequency\": StudentFrequencyModel(),\n",
    "    \"Student Characteristic\": StudentCharacteristicModel(),\n",
    "    \"Student Encountered\": StudentEncounteredBeforeModel(),\n",
    "    \"Defect Multiplicity\": DefectMultiplicityModel(),\n",
    "    \"Severity Baseline\": SeverityModel().train(full_context)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_weight_histogram(\n",
    "    student_models[\"Student Frequency\"].get_model_weights().values.flatten(),\n",
    "    x_label=\"Frequency\",\n",
    "    title=\"Distribution of Student-Specific Frequency\",\n",
    "    save_path=\"student_frequency_distribution.png\",\n",
    "    bins=80\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_weight_histogram(\n",
    "    student_models[\"Student Characteristic\"].get_model_weights().values.flatten(),\n",
    "    x_label=\"Z-Score (Absolute Value)\",\n",
    "    title=\"Distribution of Characteristic Student-Defect Scores\",\n",
    "    save_path=\"student_characteristic_distribution.png\",\n",
    "    bins=80\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pilot Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
