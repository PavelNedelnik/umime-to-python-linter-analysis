{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from IPython.display import display, HTML\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.prioritization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CONFIG_ENV\"] = \"debug\"\n",
    "if False:\n",
    "    os.environ[\"CONFIG_ENV\"] = \"production\"\n",
    "\n",
    "from config import load_config\n",
    "config = load_config()\n",
    "\n",
    "DEBUG = config[\"DEBUG\"]\n",
    "\n",
    "# input data\n",
    "TRAINING_DATA_PATH = config['PATHS']['development_set']\n",
    "VALIDATION_DATA_PATH = config['PATHS']['evaluation_set']\n",
    "# storage\n",
    "STORAGE = config['PATHS']['storage']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv(STORAGE / 'items.csv', index_col=0)\n",
    "defects = pd.read_csv(STORAGE / f'defects.csv', index_col=0)\n",
    "\n",
    "train_log = pd.read_csv(TRAINING_DATA_PATH / 'log.csv', index_col=0, parse_dates=['time'])\n",
    "train_defect_log = pd.read_csv(TRAINING_DATA_PATH / 'defect_log.csv', index_col=0)\n",
    "train_defect_log.columns = train_defect_log.columns.astype(int)\n",
    "\n",
    "test_log = pd.read_csv(VALIDATION_DATA_PATH / 'log.csv', index_col=0, parse_dates=['time'])\n",
    "test_defect_log = pd.read_csv(VALIDATION_DATA_PATH / 'defect_log.csv', index_col=0)\n",
    "test_defect_log.columns = test_defect_log.columns.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = items, defects\n",
    "\n",
    "models = [\n",
    "    TaskCommonModel(*data),\n",
    "    TaskCharacteristicModel(*data),\n",
    "    StudentCommonModel(*data),\n",
    "    StudentCharacteristicModel(*data),\n",
    "    StudentEncounteredBeforeModel(*data),\n",
    "    DefectMultiplicityModel(*data),\n",
    "    SeverityModel(*data),\n",
    "]\n",
    "\n",
    "models = {model.get_model_name(): model for model in models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in (pbar :=tqdm(models.items(), desc=\"Training Models\")):\n",
    "    pbar.set_description(f\"Training {name}\")\n",
    "    model.update(train_log, train_defect_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Pilot testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Written with ChatGPT and Gemini\n",
    "\n",
    "def _generate_submission_html(sub_num: int, submission: pd.Series,\n",
    "                                         defect_counts: pd.Series, # Pass original counts\n",
    "                                         active_models: dict, items: pd.DataFrame, defects: pd.DataFrame,\n",
    "                                         show_scores: bool, show_probabilities: bool, show_levels: bool) -> str:\n",
    "    \"\"\"Generate HTML for a submission, displaying basic information about the submission and the selected tables with model outputs.\"\"\"\n",
    "    task_id = submission['item']\n",
    "    task_row = items.loc[task_id]\n",
    "    present_mask = defect_counts > 0\n",
    "    present_defects_index = present_mask[present_mask].index\n",
    "\n",
    "    # --- Calculate necessary outputs ---\n",
    "    raw_scores_df, prioritized_scores_df, discrete_levels_df, rank_series = None, None, None, None\n",
    "\n",
    "    # Calculate raw scores if needed for display or sorting\n",
    "    if show_scores or show_probabilities or show_levels:\n",
    "        temp_raw_scores = {name: model._calculate_scores(submission, defect_counts).loc[present_defects_index]\n",
    "                           for name, model in active_models.items()}\n",
    "        raw_scores_df = pd.DataFrame(temp_raw_scores)\n",
    "        if not raw_scores_df.empty:\n",
    "            numeric_cols = raw_scores_df.select_dtypes(include=np.number).columns\n",
    "            raw_scores_df['SUM_RANK'] = raw_scores_df[numeric_cols].rank(ascending=False, axis=0).sum(axis=1)\n",
    "            rank_series = raw_scores_df['SUM_RANK']\n",
    "\n",
    "    # Calculate prioritized scores if requested\n",
    "    if show_probabilities and not present_defects_index.empty:\n",
    "        temp_prio_scores = {name: model.prioritize(submission, defect_counts).loc[present_defects_index]\n",
    "                            for name, model in active_models.items()}\n",
    "        prioritized_scores_df = pd.DataFrame(temp_prio_scores)\n",
    "\n",
    "    # Calculate discrete levels if requested\n",
    "    if show_levels and not present_defects_index.empty:\n",
    "        temp_disc_levels = {name: model.discretize(submission, defect_counts).loc[present_defects_index]\n",
    "                            for name, model in active_models.items()}\n",
    "        discrete_levels_df = pd.DataFrame(temp_disc_levels)\n",
    "\n",
    "    # --- HTML Structure ---\n",
    "    css = \"\"\"\n",
    "<style>\n",
    ".tooltip { position: relative; display: inline-block; cursor: help; border-bottom: 1px dashed #fff; }\n",
    ".tooltip .tooltiptext { visibility: hidden; width: 400px; background-color: #1e1e1e; color: #f5f5f5; text-align: left; border-radius: 6px; padding: 10px; position: absolute; z-index: 1000; top: 50%; left: 100%; transform: translateY(-50%) translateX(10px); opacity: 0; transition: opacity 0.3s; font-family: monospace; white-space: normal; box-shadow: 0 4px 8px rgba(0,0,0,0.5); }\n",
    ".tooltip:hover .tooltiptext { visibility: visible; opacity: 1; }\n",
    ".tooltiptext strong { color: #FFD700; } .tooltiptext pre { background-color: #333; padding: 5px; border-radius: 3px; overflow-x: auto; }\n",
    ".panel { background-color: #333 !important; color: #f5f5f5 !important; border: 1px solid #555 !important; padding: 10px; border-radius: 5px; }\n",
    ".hdr { background-color: #444 !important; color: #f5f5f5 !important; }\n",
    ".cell { border: 1px solid #555; padding: 8px; text-align: center; background-color: #333; color: #f5f5f5; }\n",
    ".cell-left { text-align: left; font-weight: bold; }\n",
    ".level-cell { font-weight: bold; font-size: 0.9em; }\n",
    ".score-cell { font-size: 0.85em; color: #bbb; }\n",
    "</style>\n",
    "    \"\"\"\n",
    "    html_output = css + f\"\"\"\n",
    "    <div style=\"border: 2px solid #555; margin: 20px 0; padding: 15px; border-radius: 8px; background-color: #222; color: #f5f5f5;\">\n",
    "        <div style=\"background-color: #4CAF50; color: white; padding: 8px; border-radius: 5px; margin-bottom: 15px;\">\n",
    "            <h3 style=\"margin: 0;\">SUB {sub_num + 1} | Task: {task_id} ({task_row['display name']})</h3>\n",
    "            <p style=\"margin: 0; font-size: 0.9em;\">At: {submission['time']}</p>\n",
    "        </div>\n",
    "        <div style=\"display: flex; gap: 15px; margin-bottom: 15px;\">\n",
    "             <div class=\"panel instructions-panel\" style=\"flex: 1;\"><strong>Instructions:</strong> <p style=\"font-size: 0.9em; max-height: 100px; overflow-y: auto;\">{task_row['instructions']}</p></div>\n",
    "             <div style=\"flex: 1.5; border: 1px solid #ddd; padding: 10px; border-radius: 5px; background-color: #2e2e1e; color: #f5f5f5; font-family: monospace;\"><strong>Code:</strong> <pre style=\"margin: 0; max-height: 150px; overflow-y: auto; white-space: pre-wrap;\">{submission['answer']}</pre></div>\n",
    "        </div>\n",
    "    \"\"\"\n",
    "\n",
    "    def _generate_table_html(scores_df, title, mode):\n",
    "        \"\"\"Generate the HTML table for the model output tables.\"\"\"\n",
    "        if scores_df is None or scores_df.empty: return f\"<h4 style='margin-top: 20px;'>{title}</h4><p>No defects present or table not shown.</p>\"\n",
    "\n",
    "        lmap15 = { 0: (\"Abs\", \"#404040\"), 1: (\"VL\", \"#d9f0a3\"), 2: (\"L\", \"#addd8e\"), 3: (\"M\", \"#fdae6b\"), 4: (\"H\", \"#f46d43\"), 5: (\"VH\", \"#d73027\") }\n",
    "        lmapZ = { 0: (\"Avg\", \"#ffffbf\"), 1: (\"HF\", \"#fdae61\"), 2: (\"VHF\", \"#d7191c\"), -1: (\"LF\", \"#abd9e9\"), -2: (\"VLF\", \"#2c7bb6\"), \"abs\": (\"Abs\", \"#404040\")}\n",
    "        tc15 = { 0: \"#ccc\", 1: \"#000\", 2: \"#000\", 3: \"#000\", 4: \"#fff\", 5: \"#fff\" }\n",
    "        tcZ = { 0: \"#000\", 1: \"#000\", 2: \"#fff\", -1: \"#000\", -2: \"#fff\", \"abs\": \"#ccc\" }\n",
    "\n",
    "        tbl = f\"<h4 style='margin-top: 20px;'>{title}</h4><table style='width:100%; border-collapse:collapse; font-size:0.9em;'><thead><tr class='hdr'><th class='cell cell-left' style='width:25%;'>Defect</th>\"\n",
    "        for name in active_models.keys(): tbl += f\"<th class='cell' style='width:15%;'>{name}<br><span style='font-size:0.8em;'></span></th>\"\n",
    "        tbl += \"</tr></thead><tbody>\"\n",
    "\n",
    "        idx_to_sort = rank_series.sort_values(ascending=True).index if rank_series is not None else scores_df.index\n",
    "\n",
    "        for defect_id in idx_to_sort:\n",
    "            if defect_id not in scores_df.index: continue\n",
    "            dr = defects.loc[defect_id]\n",
    "\n",
    "            # Tooltip\n",
    "            tt_cont = f\"<strong>T:</strong> {dr['defect type']}<br><strong>D:</strong> {dr['description']}<br>\"\n",
    "            def add_code(t, c): return f\"<strong>{t}:</strong><pre>{str(c).replace('<','&lt;').replace('>','&gt;')}</pre>\" if pd.notna(c) and c!=\"\" else \"\"\n",
    "            tt_cont += add_code(\"Ex\", dr['code example'])\n",
    "            tt_cont += add_code(\"Fix\", dr['code fix example'])\n",
    "            tbl += f\"<tr><td class='cell data-cell-left'><div class='tooltip'>{dr['display name']}<span class='tooltiptext'>{tt_cont}</span></div></td>\"\n",
    "\n",
    "            # Cells\n",
    "            for name, model in active_models.items():\n",
    "                val = scores_df.loc[defect_id, name]\n",
    "                cell_html = \"<td class='cell data-cell'>\" # Default\n",
    "\n",
    "                if mode == 'discrete':\n",
    "                    lvl = int(val); isZ = model.get_discretization_scale() == '-2-2'\n",
    "                    cmap = lmapZ if isZ else lmap15; tmap = tcZ if isZ else tc15\n",
    "                    key = lvl if isZ else lvl; key = \"abs\" if lvl==0 and isZ else key\n",
    "                    txt, bg = cmap.get(key, (\"?\", \"#888\")); tc = tmap.get(key, \"#000\")\n",
    "                    disp = f\"{key if isZ and key!='abs' else lvl}({txt})\"\n",
    "                    cell_html = f\"<td class='cell level-cell' style='background-color:{bg}; color:{tc};'>{disp}</td>\"\n",
    "                elif mode == 'prioritized' or mode == 'raw':\n",
    "                    score = val\n",
    "                    if mode == 'prioritized': min_s, max_s = 0.0, 1.0\n",
    "                    else: s_col = scores_df[name]; min_s, max_s = s_col.min(), s_col.max()\n",
    "                    norm = 0.5 if max_s == min_s or pd.isna(min_s) or pd.isna(max_s) else np.interp(score, [min_s, max_s], [0.1, 1.0])\n",
    "                    r,g,b = 255, 255-int(230*norm), 255-int(230*norm)\n",
    "                    color = f\"rgb({max(0,r)},{max(0,g)},{max(0,b)})\"; tc = \"#fff\" if norm > 0.6 else \"#000\"\n",
    "                    cell_html = f\"<td class='cell score-cell' style='background-color:{color}; color:{tc};'>{score:.3f}</td>\"\n",
    "\n",
    "                tbl += cell_html\n",
    "            tbl += \"</tr>\"\n",
    "        tbl += \"</tbody></table>\"\n",
    "        return tbl\n",
    "\n",
    "    # --- Conditionally Add Aditional Tables ---\n",
    "    if present_defects_index.empty:\n",
    "         html_output += \"<h4 style='margin-top: 20px;'>No Defects Present</h4>\"\n",
    "    else:\n",
    "        if show_scores: html_output += _generate_table_html(raw_scores_df.drop(columns=['SUM_RANK'], errors='ignore'), \"Raw Scores (_calculate_scores)\", 'raw')\n",
    "        if show_probabilities: html_output += _generate_table_html(prioritized_scores_df, \"Prioritized Weights (Softmax)\", 'prioritized')\n",
    "        if show_levels: html_output += _generate_table_html(discrete_levels_df, \"Discrete Levels\", 'discrete')\n",
    "\n",
    "    html_output += \"</div>\" # Close main container div\n",
    "    return html_output\n",
    "\n",
    "def pilot_test_user(user_id: int, all_models: dict, log: pd.DataFrame, defect_log: pd.DataFrame, items: pd.DataFrame, defects: pd.DataFrame,\n",
    "                    show_scores: bool = False, show_probabilities: bool = False, show_levels: bool = False):\n",
    "    \"\"\"Analyze a user's submission history, displaying selected views based on flags.\"\"\"\n",
    "    user_log = log[log['user'] == user_id].sort_values('time')\n",
    "    user_defect_log = defect_log.loc[user_log.index]\n",
    "\n",
    "    if user_log.empty: display(HTML(f\"<h2>No submissions for User ID: {user_id}</h2>\")); return\n",
    "\n",
    "    active_models = {name: deepcopy(model) for name, model in all_models.items()}\n",
    "    for sub_num, (index, submission) in enumerate(tqdm(user_log.iterrows(), total=len(user_log), desc=f\"User {user_id}\")):\n",
    "        defect_counts = user_defect_log.loc[index]\n",
    "\n",
    "        html_output = _generate_submission_html(\n",
    "            sub_num, submission, defect_counts, active_models, items, defects,\n",
    "            show_scores, show_probabilities, show_levels\n",
    "        )\n",
    "        display(HTML(html_output))\n",
    "\n",
    "        for model in active_models.values(): model.update(submission, defect_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defect scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pilot_test_user(test_log.iloc[10]['user'], models, test_log, test_defect_log, items, defects, show_scores=True, show_levels=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
