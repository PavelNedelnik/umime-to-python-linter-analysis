{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import wilcoxon\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "from src.prioritization import *\n",
    "\n",
    "RETRAIN_MODELS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CONFIG_ENV\"] = \"debug\"\n",
    "if False:\n",
    "    os.environ[\"CONFIG_ENV\"] = \"production\"\n",
    "\n",
    "from config import load_config\n",
    "config = load_config()\n",
    "\n",
    "RESOLUTION = config['DEFAULTS']['resolution']\n",
    "SEED = config['DEFAULTS']['random_seed']\n",
    "\n",
    "# input data\n",
    "BENCHMARK_PATH = config['PATHS']['benchmark_dataset']\n",
    "STORAGE_PATH = config['PATHS']['storage']\n",
    "\n",
    "# output data\n",
    "IMAGE_DIR = config['PATHS']['images'] / 'modelling'\n",
    "FINAL_MODEL_PATH = BENCHMARK_PATH / \"final_teacher_model.pkl\"\n",
    "FINAL_FEATURES_PATH = BENCHMARK_PATH / \"final_selected_features.pkl\"\n",
    "FINAL_HEURISTIC_MODEL_PATH = BENCHMARK_PATH / \"final_heuristic_model.pkl\"\n",
    "FINAL_HEURISTIC_FEATURES_PATH = BENCHMARK_PATH / \"final_heuristic_selected_features.pkl\"\n",
    "BENCHMARK_CACHE_PATH = BENCHMARK_PATH / \"benchmark_cache\"\n",
    "\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "os.makedirs(BENCHMARK_CACHE_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv(STORAGE_PATH / 'items.csv', index_col=0)\n",
    "defects = pd.read_csv(STORAGE_PATH / f'defects.csv', index_col=0)\n",
    "\n",
    "df = pd.read_csv(BENCHMARK_PATH / 'benchmark_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_discrete_features = [col for col in df.columns if col.endswith('(Left Discrete)')]\n",
    "right_discrete_features = [col for col in df.columns if col.endswith('(Right Discrete)')]\n",
    "left_continuous_features = [col for col in df.columns if col.endswith('(Left Continuous)')]\n",
    "right_continuous_features = [col for col in df.columns if col.endswith('(Right Continuous)')]\n",
    "\n",
    "if any(map(lambda x: len(x) == 0, [left_discrete_features, right_discrete_features, left_continuous_features, right_continuous_features])):\n",
    "    raise ValueError('Some of the feature sets are empty')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_suffix(col: str) -> str:\n",
    "    \"\"\"Return base name before the first ' (' occurrence; fall back to original.\"\"\"\n",
    "    if not isinstance(col, str):\n",
    "        return col\n",
    "    idx = col.find(\" (\")\n",
    "    return col[:idx] if idx != -1 else col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_discrete_values = df[left_discrete_features].rename(columns=remove_suffix)\n",
    "right_discrete_values = df[right_discrete_features].rename(columns=remove_suffix)\n",
    "\n",
    "left_continuous_values = df[left_continuous_features].rename(columns=remove_suffix)\n",
    "right_continuous_values = df[right_continuous_features].rename(columns=remove_suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_diff = left_discrete_values - right_discrete_values\n",
    "discrete_diff = discrete_diff.add_suffix(' (Discrete Diff)')\n",
    "\n",
    "continuous_diff = left_continuous_values - right_continuous_values\n",
    "continuous_diff = continuous_diff.add_suffix(' (Continuous Diff)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_is_larger = left_discrete_values > right_discrete_values\n",
    "discrete_is_larger = discrete_is_larger.add_suffix(' (Discrete >)')\n",
    "\n",
    "continuous_is_larger = left_continuous_values > right_continuous_values\n",
    "continuous_is_larger = continuous_is_larger.add_suffix(' (Continuous >)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_is_extreme_max = left_discrete_values == 5\n",
    "left_is_extreme_max = left_is_extreme_max.add_suffix(' (Left Max)')\n",
    "left_is_extreme_min = left_discrete_values == 1\n",
    "left_is_extreme_min = left_is_extreme_min.add_suffix(' (Left Min)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item and defect metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_type = defects['defect type'].loc[df['left']].reset_index(drop=True).rename('left')\n",
    "right_type = defects['defect type'].loc[df['right']].reset_index(drop=True).rename('right')\n",
    "\n",
    "item_topic = items['topic'].loc[df['item']].reset_index(drop=True).rename('item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_encoder = OneHotEncoder()\n",
    "\n",
    "metadata = metadata_encoder.fit_transform(pd.concat([\n",
    "    left_type,\n",
    "    right_type,\n",
    "    item_topic\n",
    "], axis=1))\n",
    "\n",
    "metadata = pd.DataFrame(metadata.toarray(), columns=metadata_encoder.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature groups and combined dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_df = pd.concat([\n",
    "    df[left_discrete_features],\n",
    "    df[right_discrete_features],\n",
    "    discrete_diff,\n",
    "    df[left_continuous_features],\n",
    "    df[right_continuous_features],\n",
    "    continuous_diff,\n",
    "    discrete_is_larger,\n",
    "    continuous_is_larger,\n",
    "    left_is_extreme_max,\n",
    "    left_is_extreme_min,\n",
    "    metadata,\n",
    "], axis=1)\n",
    "\n",
    "print(\"Final engineered dataframe shape:\", engineered_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_groups = {\n",
    "    \"Left Discrete\": left_discrete_features,\n",
    "    \"Right Discrete\": right_discrete_features,\n",
    "    \"Discrete Diff\": discrete_diff.columns.tolist(),\n",
    "    \"Left+Right Continuous\": left_continuous_features + right_continuous_features,\n",
    "    \"Continuous Diff\": continuous_diff.columns.tolist(),\n",
    "    \"Derived Rules\": discrete_is_larger.columns.tolist() + continuous_is_larger.columns.tolist(),\n",
    "    \"Additional Rules\": left_is_extreme_max.columns.tolist() + left_is_extreme_min.columns.tolist(),\n",
    "    \"Metadata\": metadata.columns.tolist(),\n",
    "    \"All Features\": engineered_df.columns.tolist()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['left won']\n",
    "\n",
    "sample_weights = df.loc[engineered_df.index]['weight']\n",
    "groups = df['submission id']\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "folds = list(enumerate(logo.split(engineered_df, y, groups)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single feature models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(max_iter=5000, solver=\"lbfgs\", random_state=SEED))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heuristic_feature_names = list(map(remove_suffix, left_discrete_features))\n",
    "X_full = engineered_df[feature_groups['Left Discrete'] + feature_groups['Discrete Diff']]\n",
    "\n",
    "if RETRAIN_MODELS:\n",
    "    single_feature_results = []\n",
    "    single_feature_fold_predictions = {}\n",
    "\n",
    "    for fold_idx, (train_idx, test_idx) in tqdm(folds, desc=\"Iterating over folds\", total=groups.nunique()):\n",
    "        X_train_full, X_test_full = engineered_df.iloc[train_idx], engineered_df.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        train_weights, test_weights = sample_weights.iloc[train_idx], sample_weights.iloc[test_idx]\n",
    "\n",
    "        # Single-feature models\n",
    "        for value_type in ['Discrete', 'Continuous']:\n",
    "            for feature_name in heuristic_feature_names:\n",
    "                # Collect data\n",
    "                cols = [f'{feature_name} (Left {value_type})', f'{feature_name} ({value_type} Diff)']\n",
    "                X_train = X_train_full[cols]\n",
    "                X_test = X_test_full[cols]\n",
    "\n",
    "                # Train and predict\n",
    "                model = deepcopy(base_model)\n",
    "\n",
    "                # Pass sample weights to the pipeline\n",
    "                if isinstance(model, Pipeline):\n",
    "                    model.fit(X_train, y_train, clf__sample_weight=train_weights)\n",
    "                else:\n",
    "                    model.fit(X_train, y_train, sample_weight=train_weights)\n",
    "\n",
    "                y_pred = model.predict(X_test)\n",
    "                y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "                # Evaluate\n",
    "                acc = accuracy_score(y_test, y_pred, sample_weight=test_weights)\n",
    "                try:\n",
    "                    auc_score = roc_auc_score(y_test, y_proba, sample_weight=test_weights)\n",
    "                except ValueError:\n",
    "                    # y_test might only contain one class\n",
    "                    auc_score = np.nan\n",
    "\n",
    "                # Store results\n",
    "                model_name = \"LogisticRegression\"\n",
    "                group_name = f\"{feature_name} - {value_type}\"\n",
    "\n",
    "                single_feature_results.append({\n",
    "                    \"fold\": fold_idx,\n",
    "                    \"model\": model_name,\n",
    "                    \"group\": group_name,\n",
    "                    \"accuracy\": acc,\n",
    "                    \"auc\": auc_score\n",
    "                })\n",
    "\n",
    "                single_feature_fold_predictions[(fold_idx, model_name, group_name)] = pd.DataFrame({\n",
    "                    \"y_true\": y_test.values,\n",
    "                    \"y_pred\": y_pred,\n",
    "                    \"y_proba\": y_proba\n",
    "                }, index=y_test.index)\n",
    "\n",
    "    single_feature_results = pd.DataFrame(single_feature_results)\n",
    "    single_feature_results.to_csv(BENCHMARK_CACHE_PATH / 'single_feature_results.csv', index=False)\n",
    "    pickle.dump(single_feature_fold_predictions, open(BENCHMARK_CACHE_PATH / 'single_feature_fold_predictions.pkl', 'wb'))\n",
    "else:\n",
    "    single_feature_results = pd.read_csv(BENCHMARK_CACHE_PATH / 'single_feature_results.csv')\n",
    "    single_feature_fold_predictions = pickle.load(open(BENCHMARK_CACHE_PATH / 'single_feature_fold_predictions.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by model and feature group\n",
    "single_feature_summary = single_feature_results.groupby([\"model\", \"group\"])[[\"accuracy\", \"auc\"]].agg([\"mean\", \"std\"]).reset_index()\n",
    "# flatten multiindex\n",
    "single_feature_summary.columns = [\" \".join(col).strip() for col in single_feature_summary.columns.values]\n",
    "single_feature_summary['ranking_stability_score'] = single_feature_summary['auc mean'] - 0.5 * single_feature_summary['auc std']\n",
    "single_feature_summary.sort_values('ranking_stability_score', ascending=False, inplace=True)\n",
    "display(single_feature_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        random_state=SEED,\n",
    "        max_depth=3\n",
    "    ),\n",
    "\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(\n",
    "        random_state=SEED,\n",
    "        max_depth=3\n",
    "    ),\n",
    "\n",
    "    \"Logistic Regression\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            random_state=SEED,\n",
    "            max_iter=5000,\n",
    "            solver=\"lbfgs\"\n",
    "        ))\n",
    "    ]),\n",
    "    \"LASSO Logistic\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            random_state=SEED,\n",
    "            max_iter=5000,\n",
    "            solver=\"liblinear\",\n",
    "            penalty=\"l1\"\n",
    "        ))\n",
    "    ])\n",
    "}\n",
    "\n",
    "if RETRAIN_MODELS:\n",
    "    results = []\n",
    "    fold_predictions = {}\n",
    "\n",
    "    for fold_idx, (train_idx, test_idx) in tqdm(enumerate(logo.split(engineered_df, y, groups=groups)), desc=\"Iterating over folds\", total=groups.nunique()):\n",
    "        # Select indices\n",
    "        X_train_full, X_test_full = engineered_df.iloc[train_idx], engineered_df.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        train_weights, test_weights = sample_weights.iloc[train_idx], sample_weights.iloc[test_idx]\n",
    "\n",
    "        # Iterate over models and feature groups\n",
    "        for model_name, model in models.items():\n",
    "            for group_name, cols in feature_groups.items():\n",
    "                # Select columns\n",
    "                X_train = X_train_full[cols]\n",
    "                X_test = X_test_full[cols]\n",
    "        \n",
    "                # Train and predict\n",
    "                model = deepcopy(model)\n",
    "\n",
    "                # Pass sample weights to the pipeline\n",
    "                if isinstance(model, Pipeline):\n",
    "                    model.fit(X_train, y_train, clf__sample_weight=train_weights)\n",
    "                else:\n",
    "                    model.fit(X_train, y_train, sample_weight=train_weights)\n",
    "                    \n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "                # Evaluate\n",
    "                # fallback for models without predict_proba\n",
    "                if hasattr(model, \"predict_proba\"):\n",
    "                    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "                else:\n",
    "                    y_proba = np.nan\n",
    "\n",
    "                acc = accuracy_score(y_test, y_pred, sample_weight=test_weights)\n",
    "                try:\n",
    "                    auc_score = roc_auc_score(y_test, y_proba, sample_weight=test_weights)\n",
    "                except ValueError:\n",
    "                    # y_test might only contain one class\n",
    "                    auc = np.nan\n",
    "\n",
    "                # Store results\n",
    "                results.append({\n",
    "                    \"fold\": fold_idx,\n",
    "                    \"model\": model_name,\n",
    "                    \"group\": group_name,\n",
    "                    \"accuracy\": acc,\n",
    "                    \"auc\": auc_score\n",
    "                })\n",
    "\n",
    "                fold_predictions[(fold_idx, model_name, group_name)] = pd.DataFrame({\n",
    "                    \"y_true\": y_test.values,\n",
    "                    \"y_pred\": y_pred,\n",
    "                    \"y_proba\": y_proba\n",
    "                }, index=y_test.index)\n",
    "\n",
    "    ablation_study_results = pd.DataFrame(results)\n",
    "    ablation_study_results.to_csv(BENCHMARK_CACHE_PATH / 'ablation_study_results.csv', index=False)\n",
    "    pickle.dump(fold_predictions, open(BENCHMARK_CACHE_PATH / 'ablation_study_fold_predictions.pkl', 'wb'))\n",
    "else:\n",
    "    ablation_study_results = pd.read_csv(BENCHMARK_CACHE_PATH / 'ablation_study_results.csv')\n",
    "    fold_predictions = pickle.load(open(BENCHMARK_CACHE_PATH / 'ablation_study_fold_predictions.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by model and feature group\n",
    "ablation_study_summary = ablation_study_results.groupby([\"model\", \"group\"])[[\"accuracy\", \"auc\"]].agg([\"mean\", \"std\"]).reset_index()\n",
    "# flatten multiindex\n",
    "ablation_study_summary.columns = [\" \".join(col).strip() for col in ablation_study_summary.columns.values]\n",
    "ablation_study_summary['ranking_stability_score'] = ablation_study_summary['auc mean'] - 0.5 * ablation_study_summary['auc std']\n",
    "ablation_study_summary.sort_values('ranking_stability_score', ascending=False, inplace=True)\n",
    "display(ablation_study_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Train the final model with the best performing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparams = ablation_study_summary.iloc[0]\n",
    "best_model_name, best_feature_group = best_hyperparams[\"model\"], best_hyperparams[\"group\"]\n",
    "print(f\"Selected model: {best_model_name}; feature group: {best_feature_group}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_MODELS:\n",
    "    best_model = models[best_model_name]\n",
    "    selected_cols = feature_groups.get(best_feature_group)\n",
    "\n",
    "    X_full = engineered_df[selected_cols]\n",
    "    y_full = y\n",
    "\n",
    "    best_model.fit(X_full, y_full)\n",
    "\n",
    "    joblib.dump(best_model, FINAL_MODEL_PATH)\n",
    "    joblib.dump(selected_cols, FINAL_FEATURES_PATH)\n",
    "else:\n",
    "    best_model = joblib.load(FINAL_MODEL_PATH)\n",
    "    selected_cols = joblib.load(FINAL_FEATURES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_single_heuristic_model = single_feature_summary.iloc[0]\n",
    "best_single_heuristic_name, best_single_heuristic_group = best_single_heuristic_model[\"model\"], best_single_heuristic_model[\"group\"]\n",
    "print(f\"Selected feature group: {best_single_heuristic_group}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_MODELS:\n",
    "    best_single_feature_model = base_model\n",
    "    feature_name, value_type = best_single_heuristic_group.split(\" - \")\n",
    "    selected_cols = [f'{feature_name} (Left {value_type})', f'{feature_name} ({value_type} Diff)']\n",
    "\n",
    "    X_full = engineered_df[selected_cols]\n",
    "    y_full = y\n",
    "\n",
    "    best_single_feature_model.fit(X_full, y_full)\n",
    "\n",
    "    joblib.dump(best_single_feature_model, FINAL_HEURISTIC_MODEL_PATH)\n",
    "    joblib.dump(selected_cols, FINAL_HEURISTIC_FEATURES_PATH)\n",
    "else:\n",
    "    best_single_feature_model = joblib.load(FINAL_HEURISTIC_MODEL_PATH)\n",
    "    selected_cols = joblib.load(FINAL_HEURISTIC_FEATURES_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single feature models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(data=single_feature_summary, x=\"group\", y=\"ranking_stability_score\", hue=\"group\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.title(\"Single feature models by stability score\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"single_feature_ranking_stability_score.png\", dpi=RESOLUTION)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_feature_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "ax = sns.barplot(\n",
    "    data=single_feature_results,\n",
    "    x=\"group\",\n",
    "    y=\"accuracy\",\n",
    "    errorbar=(\"ci\", 95),\n",
    "    hue=\"group\"\n",
    ")\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Single-Feature Heuristics vs Full Model (Accuracy)\")\n",
    "plt.xlabel(\"Heuristic (Single Feature)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"single_feature_accuracy.png\", dpi=RESOLUTION)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wilcoxon vs best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add best performing model\n",
    "combined_df = pd.concat([\n",
    "    single_feature_results,\n",
    "    ablation_study_results[(ablation_study_results[\"model\"] == best_model_name) & (ablation_study_results[\"group\"] == best_feature_group)]\n",
    "], axis=0)\n",
    "\n",
    "combined_df['model+group'] = combined_df.apply(lambda row: f\"{row['group']} ({row['model']})\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_tests = []\n",
    "\n",
    "best_model_df = ablation_study_results[(ablation_study_results[\"model\"] == best_model_name) & (ablation_study_results[\"group\"] == best_feature_group)]\n",
    "\n",
    "for i, heuristic_group in enumerate(single_feature_results[\"group\"].unique()):\n",
    "    heuristic_df = single_feature_results[\n",
    "        (single_feature_results['group'] == heuristic_group)\n",
    "    ].sort_values(\"fold\")\n",
    "\n",
    "    stat, p = wilcoxon(heuristic_df[\"accuracy\"], best_model_df[\"accuracy\"])\n",
    "    pairwise_tests.append({\n",
    "        \"model_1\": heuristic_group,\n",
    "        \"model_2\": f\"{best_model_name} ({best_feature_group})\",\n",
    "        \"wilcoxon_stat\": stat,\n",
    "        \"p_value\": p\n",
    "    })\n",
    "\n",
    "pd.DataFrame(pairwise_tests).sort_values(\"p_value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top model-feature pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10 = ablation_study_summary.head(10)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(data=top10, x=\"group\", y=\"ranking_stability_score\", hue=\"model\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.title(\"Top model+feature combinations by stability score\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"top_models.png\", dpi=RESOLUTION)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wilcoxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group_to_compare = \"All Features\"\n",
    "\n",
    "pairwise_tests = []\n",
    "models_list = ablation_study_results[\"model\"].unique()\n",
    "\n",
    "for i, m1 in enumerate(models_list):\n",
    "    for m2 in models_list[i+1:]:\n",
    "        df1 = ablation_study_results[\n",
    "            (ablation_study_results[\"model\"] == m1) & \n",
    "            (ablation_study_results[\"group\"] == feature_group_to_compare)\n",
    "        ].sort_values(\"fold\")\n",
    "\n",
    "        df2 = ablation_study_results[\n",
    "            (ablation_study_results[\"model\"] == m2) & \n",
    "            (ablation_study_results[\"group\"] == feature_group_to_compare)\n",
    "        ].sort_values(\"fold\")\n",
    "\n",
    "        if len(df1) == len(df2) and len(df1) > 0:\n",
    "            stat, p = wilcoxon(df1[\"accuracy\"], df2[\"accuracy\"])\n",
    "            pairwise_tests.append({\n",
    "                \"model_1\": m1,\n",
    "                \"model_2\": m2,\n",
    "                \"wilcoxon_stat\": stat,\n",
    "                \"p_value\": p\n",
    "            })\n",
    "\n",
    "pd.DataFrame(pairwise_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostic plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "for model_name in ablation_study_results[\"model\"].unique():\n",
    "    y_true_all = []\n",
    "    y_proba_all = []\n",
    "    \n",
    "    for key, pred_df in fold_predictions.items():\n",
    "        _, mname, group = key\n",
    "        if mname == model_name and group == \"All Features\":\n",
    "            y_true_all.extend(pred_df[\"y_true\"])\n",
    "            y_proba_all.extend(pred_df[\"y_proba\"])\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true_all, y_proba_all)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.plot(fpr, tpr, lw=2, label=f\"{model_name} (AUC={roc_auc:.3f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve (Aggregated Across Folds)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(IMAGE_DIR / \"roc_curve.png\", dpi=RESOLUTION)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "for model_name in ablation_study_results[\"model\"].unique():\n",
    "    y_true_all = []\n",
    "    y_proba_all = []\n",
    "    \n",
    "    for key, pred_df in fold_predictions.items():\n",
    "        _, mname, group = key\n",
    "        if mname == model_name and group == \"All Features\":\n",
    "            y_true_all.extend(pred_df[\"y_true\"])\n",
    "            y_proba_all.extend(pred_df[\"y_proba\"])\n",
    "\n",
    "    prob_true, prob_pred = calibration_curve(\n",
    "        y_true_all, y_proba_all, n_bins=10, strategy=\"quantile\"\n",
    "    )\n",
    "\n",
    "    plt.plot(prob_pred, prob_true, marker=\"o\", label=model_name)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"Perfect\")\n",
    "plt.xlabel(\"Predicted Probability\")\n",
    "plt.ylabel(\"Empirical Win Rate\")\n",
    "plt.title(\"Calibration Plot\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(IMAGE_DIR / \"calibration_curve.png\", dpi=RESOLUTION)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "for model_name in ablation_study_results[\"model\"].unique():\n",
    "    y_proba_all = []\n",
    "    \n",
    "    for key, pred_df in fold_predictions.items():\n",
    "        _, mname, group = key\n",
    "        if mname == model_name and group == \"All Features\":\n",
    "            y_proba_all.extend(pred_df[\"y_proba\"])\n",
    "\n",
    "    sns.kdeplot(y_proba_all, label=model_name, fill=True, alpha=0.3)\n",
    "\n",
    "plt.title(\"Distribution of Predicted Probabilities\")\n",
    "plt.xlabel(\"p(left wins)\")\n",
    "plt.legend()\n",
    "plt.savefig(IMAGE_DIR / \"predicted_probabilities_hist.png\", dpi=RESOLUTION)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature group comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharex=False)\n",
    "\n",
    "# --- Accuracy ---\n",
    "sns.barplot(\n",
    "    data=ablation_study_results,\n",
    "    x=\"group\",\n",
    "    y=\"accuracy\",\n",
    "    errorbar=\"sd\",\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(\"Ablation Study: Accuracy by Feature Group\")\n",
    "axes[0].set_ylabel(\"Accuracy\")\n",
    "axes[0].set_xlabel(\"Feature Group\")\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# --- AUC ---\n",
    "sns.barplot(\n",
    "    data=ablation_study_results,\n",
    "    x=\"group\",\n",
    "    y=\"auc\",\n",
    "    errorbar=\"sd\",\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(\"Ablation Study: AUC by Feature Group\")\n",
    "axes[1].set_ylabel(\"AUC\")\n",
    "axes[1].set_xlabel(\"Feature Group\")\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"feature_group_stats.png\", dpi=RESOLUTION)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_importances = []\n",
    "\n",
    "for group_name, cols in feature_groups.items():\n",
    "    X_full = engineered_df[cols]\n",
    "    model = deepcopy(best_model)\n",
    "    model.fit(X_full, y)\n",
    "    \n",
    "    df_imp = pd.DataFrame({\n",
    "        \"group\": group_name,\n",
    "        \"feature\": cols,\n",
    "        \"importance\": model.feature_importances_\n",
    "    })\n",
    "    all_importances.append(df_imp)\n",
    "\n",
    "importance_df = pd.concat(all_importances, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_df = importance_df.pivot_table(\n",
    "    index=\"feature\", columns=\"group\", values=\"importance\", fill_value=0\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12,heatmap_df.shape[0]/5))\n",
    "sns.heatmap(heatmap_df, cmap=\"viridis\", linewidths=0.5)\n",
    "plt.title(\"Feature Importance Heatmap Across Groups\")\n",
    "plt.xlabel(\"Feature Group\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"feature_importance.png\", dpi=RESOLUTION)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_ranking = importance_df.groupby(\"feature\")[\"importance\"].mean().sort_values(ascending=False).head(20)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(x=global_ranking.values, y=global_ranking.index, hue=global_ranking.index, palette=\"viridis\")\n",
    "plt.title(\"Top 20 Features Overall\")\n",
    "plt.xlabel(\"Mean Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"top_20_features.png\", dpi=RESOLUTION)\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
