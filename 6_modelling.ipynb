{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "from src.prioritization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CONFIG_ENV\"] = \"debug\"\n",
    "if False:\n",
    "    os.environ[\"CONFIG_ENV\"] = \"production\"\n",
    "\n",
    "from config import load_config\n",
    "config = load_config()\n",
    "\n",
    "RESOLUTION = config['DEFAULTS']['resolution']\n",
    "SEED = config['DEFAULTS']['random_seed']\n",
    "\n",
    "# input data\n",
    "BENCHMARK_PATH = config['PATHS']['benchmark_dataset']\n",
    "STORAGE_PATH = config['PATHS']['storage']\n",
    "\n",
    "# output data\n",
    "IMAGE_DIR = config['PATHS']['images'] / 'modelling'\n",
    "\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv(STORAGE_PATH / 'items.csv', index_col=0)\n",
    "defects = pd.read_csv(STORAGE_PATH / f'defects.csv', index_col=0)\n",
    "\n",
    "df = pd.read_csv(BENCHMARK_PATH / 'benchmark_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_discrete_features = [col for col in df.columns if col.endswith('(Left Discrete)')]\n",
    "right_discrete_features = [col for col in df.columns if col.endswith('(Right Discrete)')]\n",
    "left_continuous_features = [col for col in df.columns if col.endswith('(Left Continuous)')]\n",
    "right_continuous_features = [col for col in df.columns if col.endswith('(Right Continuous)')]\n",
    "\n",
    "if any(map(lambda x: len(x) == 0, [left_discrete_features, right_discrete_features, left_continuous_features, right_continuous_features])):\n",
    "    raise ValueError('Some of the feature sets are empty')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_suffix(col):\n",
    "    \"\"\"Get to the original column name.\"\"\"\n",
    "    return col[:col.find(' (')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_discrete_values = df[left_discrete_features].rename(columns=remove_suffix)\n",
    "right_discrete_values = df[right_discrete_features].rename(columns=remove_suffix)\n",
    "\n",
    "left_continuous_values = df[left_continuous_features].rename(columns=remove_suffix)\n",
    "right_continuous_values = df[right_continuous_features].rename(columns=remove_suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_diff = left_discrete_values - right_discrete_values\n",
    "discrete_diff = discrete_diff.add_suffix(' (Discrete Diff)')\n",
    "\n",
    "continuous_diff = left_continuous_values - right_continuous_values\n",
    "continuous_diff = continuous_diff.add_suffix(' (Continuous Diff)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_is_larger = left_discrete_values > right_discrete_values\n",
    "discrete_is_larger = discrete_is_larger.add_suffix(' (Discrete >)')\n",
    "\n",
    "continuous_is_larger = left_continuous_values > right_continuous_values\n",
    "continuous_is_larger = continuous_is_larger.add_suffix(' (Continuous >)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_is_extreme_max = left_discrete_values == 5\n",
    "left_is_extreme_max = left_is_extreme_max.add_suffix(' (Left Max)')\n",
    "left_is_extreme_min = left_discrete_values == 1\n",
    "left_is_extreme_min = left_is_extreme_min.add_suffix(' (Left Min)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item and defect metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_type = defects['defect type'].loc[df['left']].reset_index(drop=True).rename('left')\n",
    "right_type = defects['defect type'].loc[df['right']].reset_index(drop=True).rename('right')\n",
    "\n",
    "item_topic = items['topic'].loc[df['item']].reset_index(drop=True).rename('item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_encoder = OneHotEncoder()\n",
    "\n",
    "metadata = metadata_encoder.fit_transform(pd.concat([\n",
    "    left_type,\n",
    "    right_type,\n",
    "    item_topic\n",
    "], axis=1))\n",
    "\n",
    "metadata = pd.DataFrame(metadata.toarray(), columns=metadata_encoder.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature groups and combined dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_df = pd.concat([\n",
    "    df[left_discrete_features],\n",
    "    df[right_discrete_features],\n",
    "    discrete_diff,\n",
    "    df[left_continuous_features],\n",
    "    df[right_continuous_features],\n",
    "    continuous_diff,\n",
    "    discrete_is_larger,\n",
    "    continuous_is_larger,\n",
    "    left_is_extreme_max,\n",
    "    left_is_extreme_min,\n",
    "    metadata,\n",
    "], axis=1)\n",
    "\n",
    "print(\"Final engineered dataframe shape:\", engineered_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_groups = {\n",
    "    \"Left Discrete\": left_discrete_features,\n",
    "    \"Right Discrete\": right_discrete_features,\n",
    "    \"Discrete Diff\": discrete_diff.columns.tolist(),\n",
    "    \"Left+Right Continuous\": left_continuous_features + right_continuous_features,\n",
    "    \"Continuous Diff\": continuous_diff.columns.tolist(),\n",
    "    \"Derived Rules\": discrete_is_larger.columns.tolist() + continuous_is_larger.columns.tolist(),\n",
    "    \"Additional Rules\": left_is_extreme_max.columns.tolist() + left_is_extreme_min.columns.tolist(),\n",
    "    \"Metadata\": metadata.columns.tolist(),\n",
    "    \"All Features\": engineered_df.columns.tolist()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=SEED, max_depth=3),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(max_depth=3, random_state=SEED),\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=SEED),\n",
    "}\n",
    "\n",
    "y = df['left won']\n",
    "\n",
    "groups = df['submission id']\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "results = []\n",
    "fold_predictions = {}\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in tqdm(enumerate(logo.split(engineered_df, y, groups=groups)), desc=\"Iterating over folds\", total=groups.nunique()):\n",
    "    X_train_full, X_test_full = engineered_df.iloc[train_idx], engineered_df.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        for group_name, cols in feature_groups.items():\n",
    "            X_train = X_train_full[cols]\n",
    "            X_test = X_test_full[cols]\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            if hasattr(model, \"predict_proba\"):\n",
    "                y_proba = model.predict_proba(X_test)[:, 1]\n",
    "            else:\n",
    "                # fallback for models without predict_proba\n",
    "                y_proba = y_pred  \n",
    "\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "            results.append({\n",
    "                \"fold\": fold_idx,\n",
    "                \"model\": model_name,\n",
    "                \"group\": group_name,\n",
    "                \"accuracy\": acc,\n",
    "                \"auc\": auc\n",
    "            })\n",
    "\n",
    "            fold_predictions[(fold_idx, model_name, group_name)] = pd.DataFrame({\n",
    "                \"y_true\": y_test.values,\n",
    "                \"y_pred\": y_pred,\n",
    "                \"y_proba\": y_proba\n",
    "            }, index=y_test.index)\n",
    "\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Aggregate by model and feature group\n",
    "summary = results_df.groupby([\"model\", \"group\"])[[\"accuracy\", \"auc\"]].agg([\"mean\", \"std\"]).reset_index()\n",
    "display(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***\n",
    "# Plot results\n",
    "# ***\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(data=results_df, x='group', y='accuracy', ci='sd')\n",
    "plt.title(\"Ablation Study: Accuracy by Feature Group\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Feature Group\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(data=results_df, x='group', y='auc', ci='sd')\n",
    "plt.title(\"Ablation Study: AUC by Feature Group\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.xlabel(\"Feature Group\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***\n",
    "# Optional: Inspect feature importances per group (for interpretability)\n",
    "# ***\n",
    "\n",
    "for group_name, cols in feature_groups.items():\n",
    "    print(f\"\\n=== Feature importances: {group_name} ===\")\n",
    "    \n",
    "    # retrain on full dataset for interpretability\n",
    "    X_full = df[cols]\n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "    model.fit(X_full, y)\n",
    "    \n",
    "    importances = pd.DataFrame({\n",
    "        \"feature\": cols,\n",
    "        \"importance\": model.feature_importances_\n",
    "    }).sort_values(\"importance\", ascending=False)\n",
    "    \n",
    "    display(importances.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
