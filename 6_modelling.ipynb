{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import wilcoxon\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "from src.prioritization import *\n",
    "from src.feature_engineering import select_features, BASE_VALUES\n",
    "\n",
    "RETRAIN_MODELS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CONFIG_ENV\"] = \"debug\"\n",
    "if False:\n",
    "    os.environ[\"CONFIG_ENV\"] = \"production\"\n",
    "\n",
    "from config import load_config\n",
    "config = load_config()\n",
    "\n",
    "DEBUG = config[\"DEBUG\"]\n",
    "\n",
    "RESOLUTION = config['DEFAULTS']['resolution']\n",
    "SEED = config['DEFAULTS']['random_seed']\n",
    "\n",
    "# input data\n",
    "BENCHMARK_PATH = config['PATHS']['benchmark_dataset']\n",
    "STORAGE_PATH = config['PATHS']['storage']\n",
    "\n",
    "# output data\n",
    "IMAGE_DIR = config['PATHS']['images'] / 'modelling'\n",
    "FINAL_MODEL_PATH = BENCHMARK_PATH / \"final_teacher_model.pkl\"\n",
    "FINAL_MODEL_FEATURES_PATH = BENCHMARK_PATH / \"final_selected_features.pkl\"\n",
    "FINAL_BASELINE_PATH = BENCHMARK_PATH / \"baseline_models\"\n",
    "FINAL_BASELINE_FEATURES_PATH = BENCHMARK_PATH / \"baseline_features\"\n",
    "BENCHMARK_CACHE_PATH = BENCHMARK_PATH / \"benchmark_cache\"\n",
    "\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "os.makedirs(FINAL_BASELINE_PATH, exist_ok=True)\n",
    "os.makedirs(FINAL_BASELINE_FEATURES_PATH, exist_ok=True)\n",
    "os.makedirs(BENCHMARK_CACHE_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv(STORAGE_PATH / 'items.csv', index_col=0)\n",
    "defects = pd.read_csv(STORAGE_PATH / f'defects.csv', index_col=0)\n",
    "\n",
    "df = pd.read_csv(BENCHMARK_PATH / 'benchmark_dataset.csv')\n",
    "catalog = joblib.load(BENCHMARK_PATH / 'feature_catalog.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['left won']\n",
    "sample_weights = df['weight']\n",
    "groups = df['submission id']\n",
    "X = df.drop(columns=['left won', 'submission id', 'weight', 'left', 'right', 'item'])\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "folds = list(enumerate(logo.split(df, y, groups)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single feature models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heuristic_groups = {\n",
    "    f\"{base} - {value_type}\": select_features(catalog, base=base, kind='Original', dtype=value_type)\n",
    "        for base in BASE_VALUES for value_type in ['Discrete', 'Continuous']\n",
    "        if base != 'Metadata'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(max_iter=5000, solver=\"lbfgs\", random_state=SEED))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_MODELS:\n",
    "    single_feature_results = []\n",
    "    single_feature_fold_predictions = {}\n",
    "\n",
    "    for fold_idx, (train_idx, test_idx) in tqdm(folds, desc=\"Iterating over folds\", total=groups.nunique()):\n",
    "\n",
    "        # Select index\n",
    "        X_train_full, X_test_full = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        train_weights, test_weights = sample_weights.iloc[train_idx], sample_weights.iloc[test_idx]\n",
    "\n",
    "        # Select column\n",
    "        for group_name, cols in heuristic_groups.items():\n",
    "            X_train = X_train_full[cols]\n",
    "            X_test = X_test_full[cols]\n",
    "\n",
    "            # Train and predict\n",
    "            model = deepcopy(base_model)\n",
    "\n",
    "            # Pass sample weights to the pipeline\n",
    "            model.fit(X_train, y_train, clf__sample_weight=train_weights)\n",
    "\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            # Evaluate\n",
    "            acc = accuracy_score(y_test, y_pred, sample_weight=test_weights)\n",
    "            try:\n",
    "                auc_score = roc_auc_score(y_test, y_proba, sample_weight=test_weights)\n",
    "            except ValueError:\n",
    "                # y_test might only contain one class\n",
    "                auc_score = np.nan\n",
    "\n",
    "            # Store results\n",
    "            model_name = \"LogisticRegression\"\n",
    "\n",
    "            single_feature_results.append({\n",
    "                \"fold\": fold_idx,\n",
    "                \"model\": model_name,\n",
    "                \"group\": group_name,\n",
    "                \"accuracy\": acc,\n",
    "                \"auc\": auc_score\n",
    "            })\n",
    "\n",
    "            single_feature_fold_predictions[(fold_idx, model_name, group_name)] = pd.DataFrame({\n",
    "                \"y_true\": y_test.values,\n",
    "                \"y_pred\": y_pred,\n",
    "                \"y_proba\": y_proba\n",
    "            }, index=y_test.index)\n",
    "\n",
    "    single_feature_results = pd.DataFrame(single_feature_results)\n",
    "    single_feature_results.to_csv(BENCHMARK_CACHE_PATH / 'single_feature_results.csv', index=False)\n",
    "    pickle.dump(single_feature_fold_predictions, open(BENCHMARK_CACHE_PATH / 'single_feature_fold_predictions.pkl', 'wb'))\n",
    "else:\n",
    "    single_feature_results = pd.read_csv(BENCHMARK_CACHE_PATH / 'single_feature_results.csv')\n",
    "    single_feature_fold_predictions = pickle.load(open(BENCHMARK_CACHE_PATH / 'single_feature_fold_predictions.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by model and feature group\n",
    "single_feature_summary = single_feature_results.groupby([\"model\", \"group\"])[[\"accuracy\", \"auc\"]].agg([\"mean\", \"std\"]).reset_index()\n",
    "# flatten multiindex\n",
    "single_feature_summary.columns = [\" \".join(col).strip() for col in single_feature_summary.columns.values]\n",
    "single_feature_summary['ranking_stability_score'] = single_feature_summary['auc mean'] - 0.5 * single_feature_summary['auc std']\n",
    "single_feature_summary.sort_values('ranking_stability_score', ascending=False, inplace=True)\n",
    "single_feature_summary.to_html(IMAGE_DIR / \"single_feature_summary.html\", index=False)\n",
    "display(single_feature_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_groups = {\n",
    "    \"Original (Left Only)\": select_features(\n",
    "        catalog, kind=\"Original\", side=\"Left\"\n",
    "    ),\n",
    "\n",
    "    \"Original (Right Only)\": select_features(\n",
    "        catalog, kind=\"Original\", side=\"Right\"\n",
    "    ),\n",
    "\n",
    "    \"Original (Left+Right)\": select_features(\n",
    "        catalog, kind=\"Original\"\n",
    "    ),\n",
    "\n",
    "    \"Diff Only\": select_features(\n",
    "        catalog, kind=\"Difference\"\n",
    "    ),\n",
    "\n",
    "    \"Binary Comparisons\": select_features(\n",
    "        catalog, kind=\"Binary\"\n",
    "    ),\n",
    "\n",
    "    \"Metadata\": select_features(\n",
    "        catalog, kind=\"Metadata\"\n",
    "    ),\n",
    "\n",
    "    \"All Heuristic-Derived\": select_features(\n",
    "        catalog, kind=None\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        random_state=SEED,\n",
    "        max_depth=3\n",
    "    ),\n",
    "\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(\n",
    "        random_state=SEED,\n",
    "        max_depth=3\n",
    "    ),\n",
    "\n",
    "    \"Logistic Regression\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            random_state=SEED,\n",
    "            max_iter=5000,\n",
    "            solver=\"lbfgs\"\n",
    "        ))\n",
    "    ]),\n",
    "    \"LASSO Logistic\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            random_state=SEED,\n",
    "            max_iter=5000,\n",
    "            solver=\"liblinear\",\n",
    "            penalty=\"l1\"\n",
    "        ))\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_MODELS:\n",
    "    results = []\n",
    "    fold_predictions = {}\n",
    "\n",
    "    for fold_idx, (train_idx, test_idx) in tqdm(folds, desc=\"Iterating over folds\", total=groups.nunique()):\n",
    "\n",
    "        # Select indices\n",
    "        X_train_full, X_test_full = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        train_weights, test_weights = sample_weights.iloc[train_idx], sample_weights.iloc[test_idx]\n",
    "\n",
    "        # Iterate over models and feature groups\n",
    "        for model_name, model in models.items():\n",
    "            for group_name, cols in feature_groups.items():\n",
    "                # Select columns\n",
    "                X_train = X_train_full[cols]\n",
    "                X_test = X_test_full[cols]\n",
    "        \n",
    "                # Train and predict\n",
    "                model = deepcopy(model)\n",
    "\n",
    "                # Pass sample weights to the pipeline\n",
    "                if isinstance(model, Pipeline):\n",
    "                    model.fit(X_train, y_train, clf__sample_weight=train_weights)\n",
    "                else:\n",
    "                    model.fit(X_train, y_train, sample_weight=train_weights)\n",
    "                    \n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "                # Evaluate\n",
    "                # fallback for models without predict_proba\n",
    "                if hasattr(model, \"predict_proba\"):\n",
    "                    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "                else:\n",
    "                    y_proba = np.nan\n",
    "\n",
    "                acc = accuracy_score(y_test, y_pred, sample_weight=test_weights)\n",
    "                try:\n",
    "                    auc_score = roc_auc_score(y_test, y_proba, sample_weight=test_weights)\n",
    "                except ValueError:\n",
    "                    # y_test might only contain one class\n",
    "                    auc = np.nan\n",
    "\n",
    "                # Store results\n",
    "                results.append({\n",
    "                    \"fold\": fold_idx,\n",
    "                    \"model\": model_name,\n",
    "                    \"group\": group_name,\n",
    "                    \"accuracy\": acc,\n",
    "                    \"auc\": auc_score\n",
    "                })\n",
    "\n",
    "                fold_predictions[(fold_idx, model_name, group_name)] = pd.DataFrame({\n",
    "                    \"y_true\": y_test.values,\n",
    "                    \"y_pred\": y_pred,\n",
    "                    \"y_proba\": y_proba\n",
    "                }, index=y_test.index)\n",
    "\n",
    "    ablation_study_results = pd.DataFrame(results)\n",
    "    ablation_study_results.to_csv(BENCHMARK_CACHE_PATH / 'ablation_study_results.csv', index=False)\n",
    "    pickle.dump(fold_predictions, open(BENCHMARK_CACHE_PATH / 'ablation_study_fold_predictions.pkl', 'wb'))\n",
    "else:\n",
    "    ablation_study_results = pd.read_csv(BENCHMARK_CACHE_PATH / 'ablation_study_results.csv')\n",
    "    fold_predictions = pickle.load(open(BENCHMARK_CACHE_PATH / 'ablation_study_fold_predictions.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by model and feature group\n",
    "ablation_study_summary = ablation_study_results.groupby([\"model\", \"group\"])[[\"accuracy\", \"auc\"]].agg([\"mean\", \"std\"]).reset_index()\n",
    "# flatten multiindex\n",
    "ablation_study_summary.columns = [\" \".join(col).strip() for col in ablation_study_summary.columns.values]\n",
    "ablation_study_summary['ranking_stability_score'] = ablation_study_summary['auc mean'] - 0.5 * ablation_study_summary['auc std']\n",
    "ablation_study_summary.sort_values('ranking_stability_score', ascending=False, inplace=True)\n",
    "ablation_study_summary.to_html(IMAGE_DIR / \"ablation_study_summary.html\", index=False)\n",
    "display(ablation_study_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Train the final model with the best performing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    best_hyperparams = ablation_study_summary[ablation_study_summary['model'] == \"LASSO Logistic\"].iloc[0]\n",
    "else:\n",
    "    best_hyperparams = ablation_study_summary.iloc[0]\n",
    "best_model_name, best_feature_group = best_hyperparams[\"model\"], best_hyperparams[\"group\"]\n",
    "print(f\"Selected model: {best_model_name}; feature group: {best_feature_group}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_MODELS:\n",
    "    best_model = deepcopy(models[best_model_name])\n",
    "    selected_cols = feature_groups.get(best_feature_group)\n",
    "\n",
    "    X_full = X[selected_cols]\n",
    "    y_full = y\n",
    "\n",
    "    best_model.fit(X_full, y_full)\n",
    "\n",
    "    joblib.dump(best_model, FINAL_MODEL_PATH)\n",
    "    joblib.dump(selected_cols, FINAL_MODEL_FEATURES_PATH)\n",
    "else:\n",
    "    best_model = joblib.load(FINAL_MODEL_PATH)\n",
    "    selected_cols = joblib.load(FINAL_MODEL_FEATURES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_heuristic = single_feature_summary[single_feature_summary['group'] == 'Naive Severity - Discrete'].iloc[0]\n",
    "secondary_heuristic = single_feature_summary[single_feature_summary['group'] == 'Task Common - Continuous'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_MODELS:\n",
    "    primary_model = deepcopy(base_model)\n",
    "    secondary_model = deepcopy(base_model)\n",
    "\n",
    "    primary_cols = heuristic_groups['Naive Severity - Discrete']\n",
    "    secondary_cols = heuristic_groups['Task Common - Continuous']\n",
    "\n",
    "    primary_model.fit(X[primary_cols], y_full)\n",
    "    secondary_model.fit(X[secondary_cols], y_full)\n",
    "\n",
    "    joblib.dump(primary_model, FINAL_BASELINE_PATH / 'primary_model.joblib')\n",
    "    joblib.dump(secondary_model, FINAL_BASELINE_PATH / 'secondary_model.joblib')\n",
    "    joblib.dump(primary_cols, FINAL_BASELINE_FEATURES_PATH / 'primary_features.joblib')\n",
    "    joblib.dump(secondary_cols, FINAL_BASELINE_FEATURES_PATH / 'secondary_features.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single feature models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(data=single_feature_summary, x=\"group\", y=\"ranking_stability_score\", hue=\"group\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.title(\"Single feature models by stability score\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"single_feature_ranking_stability_score.png\", dpi=RESOLUTION)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = single_feature_results.copy()\n",
    "sorted_df['mean_accuracy'] = sorted_df.groupby('group')['accuracy'].transform('mean')\n",
    "sorted_df = sorted_df.sort_values('mean_accuracy', ascending=False)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "ax = sns.barplot(\n",
    "    data=sorted_df,\n",
    "    x=\"group\",\n",
    "    y=\"accuracy\",\n",
    "    errorbar=(\"ci\", 95),\n",
    "    hue=\"group\"\n",
    ")\n",
    "\n",
    "ax.axhline(0.5, linestyle=\"--\", color=\"red\", linewidth=1.2, alpha=0.8)\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Single-Feature Heuristics vs Full Model (Accuracy)\")\n",
    "plt.xlabel(\"Heuristic (Single Feature)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"single_feature_accuracy.png\", dpi=RESOLUTION)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete vs continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select discrete and continuous feature rows\n",
    "discrete_rows = single_feature_summary[\n",
    "    single_feature_summary['group'].map(lambda x: x.endswith(' - Discrete'))\n",
    "]\n",
    "\n",
    "continuous_rows = single_feature_summary[\n",
    "    single_feature_summary['group'].map(lambda x: x.endswith(' - Continuous'))\n",
    "]\n",
    "\n",
    "# Merge them by base feature name\n",
    "base_name = lambda group_name: group_name.replace(' - Discrete', '').replace(' - Continuous', '')\n",
    "discrete_rows = discrete_rows.assign(feature=discrete_rows['group'].map(base_name))\n",
    "continuous_rows = continuous_rows.assign(feature=continuous_rows['group'].map(base_name))\n",
    "\n",
    "merged = pd.merge(\n",
    "    discrete_rows[['feature', 'auc mean', 'accuracy mean']],\n",
    "    continuous_rows[['feature', 'auc mean', 'accuracy mean']],\n",
    "    on='feature',\n",
    "    suffixes=('_discrete', '_continuous')\n",
    ")\n",
    "\n",
    "# Add difference columns\n",
    "merged['delta_accuracy'] = merged['accuracy mean_discrete'] - merged['accuracy mean_continuous']\n",
    "merged['delta_auc'] = merged['auc mean_discrete'] - merged['auc mean_continuous']\n",
    "\n",
    "merged.sort_values('delta_accuracy', ascending=False, inplace=True)\n",
    "merged.to_html(IMAGE_DIR / \"continuous_discrete_delta.html\", index=False)\n",
    "display(merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average delta in accuracy and AUC (Discrete - Continuous):\", merged['delta_accuracy'].mean(), merged['delta_auc'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wilcoxon vs best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add best performing model\n",
    "combined_df = pd.concat([\n",
    "    single_feature_results,\n",
    "    ablation_study_results[(ablation_study_results[\"model\"] == best_model_name) & (ablation_study_results[\"group\"] == best_feature_group)]\n",
    "], axis=0)\n",
    "\n",
    "combined_df['model+group'] = combined_df.apply(lambda row: f\"{row['group']} ({row['model']})\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_tests = []\n",
    "\n",
    "best_model_df = ablation_study_results[(ablation_study_results[\"model\"] == best_model_name) & (ablation_study_results[\"group\"] == best_feature_group)]\n",
    "\n",
    "for i, heuristic_group in enumerate(single_feature_results[\"group\"].unique()):\n",
    "    heuristic_df = single_feature_results[\n",
    "        (single_feature_results['group'] == heuristic_group)\n",
    "    ].sort_values(\"fold\")\n",
    "\n",
    "    stat, p = wilcoxon(heuristic_df[\"accuracy\"], best_model_df[\"accuracy\"])\n",
    "    pairwise_tests.append({\n",
    "        \"model_1\": heuristic_group,\n",
    "        \"model_2\": f\"{best_model_name} ({best_feature_group})\",\n",
    "        \"wilcoxon_stat\": stat,\n",
    "        \"p_value\": p\n",
    "    })\n",
    "\n",
    "pairwise_tests = pd.DataFrame(pairwise_tests).sort_values(\"p_value\")\n",
    "pairwise_tests.to_html(IMAGE_DIR / \"single_feature_vs_best_wilcoxon.html\", index=False)\n",
    "display(pairwise_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top model-feature pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10 = ablation_study_summary.head(10)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(data=top10, x=\"group\", y=\"ranking_stability_score\", hue=\"model\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.title(\"Top model+feature combinations by stability score\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"top_models.png\", dpi=RESOLUTION)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_to_compare = \"Naive Severity - Discrete\"\n",
    "severity_summary_row = single_feature_summary[single_feature_summary[\"group\"] == group_to_compare]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_severity = pd.concat([\n",
    "    ablation_study_summary.head(10),\n",
    "    severity_summary_row\n",
    "], axis=0)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(data=with_severity, x=\"group\", y=\"ranking_stability_score\", hue=\"model\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.title(\"Top model+feature combinations and the severity baseline by stability score\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"top_models_with_severity.png\", dpi=RESOLUTION)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_tests = []\n",
    "\n",
    "severity_model, severity_group = severity_summary_row[\"model\"].iloc[0], severity_summary_row[\"group\"].iloc[0]\n",
    "severity_df = single_feature_results[\n",
    "    (single_feature_results[\"group\"] == severity_group) &\n",
    "    (single_feature_results[\"model\"] == severity_model)\n",
    "].sort_values(\"fold\")\n",
    "\n",
    "for _, model_row in top10.iterrows():\n",
    "    model_name, model_group = model_row[\"model\"], model_row[\"group\"]\n",
    "    model_df = ablation_study_results[\n",
    "        (ablation_study_results[\"model\"] == model_name) & \n",
    "        (ablation_study_results[\"group\"] == model_group)\n",
    "    ].sort_values(\"fold\")\n",
    "\n",
    "    stat, p = wilcoxon(model_df[\"accuracy\"], severity_df[\"accuracy\"])\n",
    "    pairwise_tests.append({\n",
    "        \"model_1\": f\"{model_name} ({model_group})\",\n",
    "        \"model_2\": f\"{severity_model} ({severity_group})\",\n",
    "        \"wilcoxon_stat\": stat,\n",
    "        \"p_value\": p\n",
    "    })\n",
    "\n",
    "pairwise_tests = pd.DataFrame(pairwise_tests)\n",
    "pairwise_tests.to_html(IMAGE_DIR / \"top_models_vs_severity_wilcoxon.html\", index=False)\n",
    "display(pairwise_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wilcoxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group_to_compare = \"All Heuristic-Derived\"\n",
    "\n",
    "pairwise_tests = []\n",
    "models_list = ablation_study_results[\"model\"].unique()\n",
    "\n",
    "for i, m1 in enumerate(models_list):\n",
    "    for m2 in models_list[i+1:]:\n",
    "        df1 = ablation_study_results[\n",
    "            (ablation_study_results[\"model\"] == m1) & \n",
    "            (ablation_study_results[\"group\"] == feature_group_to_compare)\n",
    "        ].sort_values(\"fold\")\n",
    "\n",
    "        df2 = ablation_study_results[\n",
    "            (ablation_study_results[\"model\"] == m2) & \n",
    "            (ablation_study_results[\"group\"] == feature_group_to_compare)\n",
    "        ].sort_values(\"fold\")\n",
    "\n",
    "        if len(df1) == len(df2) and len(df1) > 0:\n",
    "            stat, p = wilcoxon(df1[\"accuracy\"], df2[\"accuracy\"])\n",
    "            pairwise_tests.append({\n",
    "                \"model_1\": m1,\n",
    "                \"model_2\": m2,\n",
    "                \"wilcoxon_stat\": stat,\n",
    "                \"p_value\": p\n",
    "            })\n",
    "\n",
    "pairwise_tests = pd.DataFrame(pairwise_tests)\n",
    "pairwise_tests.to_html(IMAGE_DIR / \"ml_model_wilcoxon.html\", index=False)\n",
    "display(pairwise_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostic plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "for model_name in ablation_study_results[\"model\"].unique():\n",
    "    y_true_all = []\n",
    "    y_proba_all = []\n",
    "    \n",
    "    for key, pred_df in fold_predictions.items():\n",
    "        _, mname, group = key\n",
    "        if mname == model_name and group == \"All Heuristic-Derived\":\n",
    "            y_true_all.extend(pred_df[\"y_true\"])\n",
    "            y_proba_all.extend(pred_df[\"y_proba\"])\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true_all, y_proba_all)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.plot(fpr, tpr, lw=2, label=f\"{model_name} (AUC={roc_auc:.3f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve (Aggregated Across Folds)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(IMAGE_DIR / \"roc_curve.png\", dpi=RESOLUTION)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "for model_name in ablation_study_results[\"model\"].unique():\n",
    "    y_true_all = []\n",
    "    y_proba_all = []\n",
    "    \n",
    "    for key, pred_df in fold_predictions.items():\n",
    "        _, mname, group = key\n",
    "        if mname == model_name and group == \"All Heuristic-Derived\":\n",
    "            y_true_all.extend(pred_df[\"y_true\"])\n",
    "            y_proba_all.extend(pred_df[\"y_proba\"])\n",
    "\n",
    "    prob_true, prob_pred = calibration_curve(\n",
    "        y_true_all, y_proba_all, n_bins=10, strategy=\"quantile\"\n",
    "    )\n",
    "\n",
    "    plt.plot(prob_pred, prob_true, marker=\"o\", label=model_name)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"Perfect\")\n",
    "plt.xlabel(\"Predicted Probability\")\n",
    "plt.ylabel(\"Empirical Win Rate\")\n",
    "plt.title(\"Calibration Plot\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(IMAGE_DIR / \"calibration_curve.png\", dpi=RESOLUTION)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "for model_name in ablation_study_results[\"model\"].unique():\n",
    "    y_proba_all = []\n",
    "    \n",
    "    for key, pred_df in fold_predictions.items():\n",
    "        _, mname, group = key\n",
    "        if mname == model_name and group == \"All Heuristic-Derived\":\n",
    "            y_proba_all.extend(pred_df[\"y_proba\"])\n",
    "\n",
    "    sns.kdeplot(y_proba_all, label=model_name, fill=True, alpha=0.3)\n",
    "\n",
    "plt.title(\"Distribution of Predicted Probabilities\")\n",
    "plt.xlabel(\"p(left wins)\")\n",
    "plt.legend()\n",
    "plt.savefig(IMAGE_DIR / \"predicted_probabilities_hist.png\", dpi=RESOLUTION)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature group comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharex=False)\n",
    "\n",
    "# --- Accuracy ---\n",
    "sns.barplot(\n",
    "    data=ablation_study_results,\n",
    "    x=\"group\",\n",
    "    y=\"accuracy\",\n",
    "    errorbar=\"sd\",\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(\"Ablation Study: Accuracy by Feature Group\")\n",
    "axes[0].set_ylabel(\"Accuracy\")\n",
    "axes[0].set_xlabel(\"Feature Group\")\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# --- AUC ---\n",
    "sns.barplot(\n",
    "    data=ablation_study_results,\n",
    "    x=\"group\",\n",
    "    y=\"auc\",\n",
    "    errorbar=\"sd\",\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(\"Ablation Study: AUC by Feature Group\")\n",
    "axes[1].set_ylabel(\"AUC\")\n",
    "axes[1].set_xlabel(\"Feature Group\")\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"feature_group_stats.png\", dpi=RESOLUTION)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tree_params = ablation_study_summary[ablation_study_summary[\"model\"].isin([\"Random Forest\", \"GradientBoostingClassifier\"])].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_importances = []\n",
    "\n",
    "for group_name, cols in feature_groups.items():\n",
    "    X_full = X[cols]\n",
    "    model = deepcopy(models[best_tree_params[\"model\"]])\n",
    "    model.fit(X_full, y)\n",
    "    \n",
    "    all_importances.append(pd.DataFrame({\n",
    "        \"group\": group_name,\n",
    "        \"feature\": cols,\n",
    "        \"importance\": model.feature_importances_\n",
    "    }))\n",
    "\n",
    "importance_df = pd.concat(all_importances, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_df = importance_df.pivot_table(\n",
    "    index=\"feature\", columns=\"group\", values=\"importance\", fill_value=0\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12,heatmap_df.shape[0]/5))\n",
    "sns.heatmap(heatmap_df, cmap=\"viridis\", linewidths=0.5)\n",
    "plt.title(\"Feature Importance Heatmap Across Groups\")\n",
    "plt.xlabel(\"Feature Group\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"feature_importance.png\", dpi=RESOLUTION)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_ranking = importance_df.groupby(\"feature\")[\"importance\"].mean().sort_values(ascending=False).head(20)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(x=global_ranking.values, y=global_ranking.index, hue=global_ranking.index, palette=\"viridis\")\n",
    "plt.title(\"Top 20 Features Overall\")\n",
    "plt.xlabel(\"Mean Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"top_20_features.png\", dpi=RESOLUTION)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation importance\n",
    "\n",
    "TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
