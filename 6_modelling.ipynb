{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from src.prioritization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CONFIG_ENV\"] = \"debug\"\n",
    "if False:\n",
    "    os.environ[\"CONFIG_ENV\"] = \"production\"\n",
    "\n",
    "from config import load_config\n",
    "config = load_config()\n",
    "\n",
    "RESOLUTION = config['DEFAULTS']['resolution']\n",
    "SEED = config['DEFAULTS']['random_seed']\n",
    "\n",
    "# input data\n",
    "BENCHMARK_PATH = config['PATHS']['benchmark_dataset']\n",
    "STORAGE_PATH = config['PATHS']['storage']\n",
    "\n",
    "# output data\n",
    "IMAGE_DIR = config['PATHS']['images'] / 'modelling'\n",
    "FINAL_MODEL_PATH = BENCHMARK_PATH / \"final_teacher_model.pkl\"\n",
    "FINAL_FEATURES_PATH = BENCHMARK_PATH / \"final_selected_features.pkl\"\n",
    "\n",
    "\n",
    "\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv(STORAGE_PATH / 'items.csv', index_col=0)\n",
    "defects = pd.read_csv(STORAGE_PATH / f'defects.csv', index_col=0)\n",
    "\n",
    "df = pd.read_csv(BENCHMARK_PATH / 'benchmark_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCHMARK_PATH / 'benchmark_dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_discrete_features = [col for col in df.columns if col.endswith('(Left Discrete)')]\n",
    "right_discrete_features = [col for col in df.columns if col.endswith('(Right Discrete)')]\n",
    "left_continuous_features = [col for col in df.columns if col.endswith('(Left Continuous)')]\n",
    "right_continuous_features = [col for col in df.columns if col.endswith('(Right Continuous)')]\n",
    "\n",
    "if any(map(lambda x: len(x) == 0, [left_discrete_features, right_discrete_features, left_continuous_features, right_continuous_features])):\n",
    "    raise ValueError('Some of the feature sets are empty')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_suffix(col: str) -> str:\n",
    "    \"\"\"Return base name before the first ' (' occurrence; fall back to original.\"\"\"\n",
    "    if not isinstance(col, str):\n",
    "        return col\n",
    "    idx = col.find(\" (\")\n",
    "    return col[:idx] if idx != -1 else col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_discrete_values = df[left_discrete_features].rename(columns=remove_suffix)\n",
    "right_discrete_values = df[right_discrete_features].rename(columns=remove_suffix)\n",
    "\n",
    "left_continuous_values = df[left_continuous_features].rename(columns=remove_suffix)\n",
    "right_continuous_values = df[right_continuous_features].rename(columns=remove_suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_diff = left_discrete_values - right_discrete_values\n",
    "discrete_diff = discrete_diff.add_suffix(' (Discrete Diff)')\n",
    "\n",
    "continuous_diff = left_continuous_values - right_continuous_values\n",
    "continuous_diff = continuous_diff.add_suffix(' (Continuous Diff)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_is_larger = left_discrete_values > right_discrete_values\n",
    "discrete_is_larger = discrete_is_larger.add_suffix(' (Discrete >)')\n",
    "\n",
    "continuous_is_larger = left_continuous_values > right_continuous_values\n",
    "continuous_is_larger = continuous_is_larger.add_suffix(' (Continuous >)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_is_extreme_max = left_discrete_values == 5\n",
    "left_is_extreme_max = left_is_extreme_max.add_suffix(' (Left Max)')\n",
    "left_is_extreme_min = left_discrete_values == 1\n",
    "left_is_extreme_min = left_is_extreme_min.add_suffix(' (Left Min)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item and defect metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_type = defects['defect type'].loc[df['left']].reset_index(drop=True).rename('left')\n",
    "right_type = defects['defect type'].loc[df['right']].reset_index(drop=True).rename('right')\n",
    "\n",
    "item_topic = items['topic'].loc[df['item']].reset_index(drop=True).rename('item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_encoder = OneHotEncoder()\n",
    "\n",
    "metadata = metadata_encoder.fit_transform(pd.concat([\n",
    "    left_type,\n",
    "    right_type,\n",
    "    item_topic\n",
    "], axis=1))\n",
    "\n",
    "metadata = pd.DataFrame(metadata.toarray(), columns=metadata_encoder.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature groups and combined dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_df = pd.concat([\n",
    "    df[left_discrete_features],\n",
    "    df[right_discrete_features],\n",
    "    discrete_diff,\n",
    "    df[left_continuous_features],\n",
    "    df[right_continuous_features],\n",
    "    continuous_diff,\n",
    "    discrete_is_larger,\n",
    "    continuous_is_larger,\n",
    "    left_is_extreme_max,\n",
    "    left_is_extreme_min,\n",
    "    metadata,\n",
    "], axis=1)\n",
    "\n",
    "print(\"Final engineered dataframe shape:\", engineered_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_groups = {\n",
    "    \"Left Discrete\": left_discrete_features,\n",
    "    \"Right Discrete\": right_discrete_features,\n",
    "    \"Discrete Diff\": discrete_diff.columns.tolist(),\n",
    "    \"Left+Right Continuous\": left_continuous_features + right_continuous_features,\n",
    "    \"Continuous Diff\": continuous_diff.columns.tolist(),\n",
    "    \"Derived Rules\": discrete_is_larger.columns.tolist() + continuous_is_larger.columns.tolist(),\n",
    "    \"Additional Rules\": left_is_extreme_max.columns.tolist() + left_is_extreme_min.columns.tolist(),\n",
    "    \"Metadata\": metadata.columns.tolist(),\n",
    "    \"All Features\": engineered_df.columns.tolist()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        random_state=SEED,\n",
    "        max_depth=3\n",
    "    ),\n",
    "\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(\n",
    "        random_state=SEED,\n",
    "        max_depth=3\n",
    "    ),\n",
    "\n",
    "    \"Logistic Regression\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            random_state=SEED,\n",
    "            max_iter=5000,\n",
    "            solver=\"lbfgs\"\n",
    "        ))\n",
    "    ]),\n",
    "    \"LASSO Logistic\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            random_state=SEED,\n",
    "            max_iter=5000,\n",
    "            solver=\"liblinear\",\n",
    "            penalty=\"l1\"\n",
    "        ))\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['left won']\n",
    "\n",
    "groups = df['submission id']\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "results = []\n",
    "fold_predictions = {}\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in tqdm(enumerate(logo.split(engineered_df, y, groups=groups)), desc=\"Iterating over folds\", total=groups.nunique()):\n",
    "    X_train_full, X_test_full = engineered_df.iloc[train_idx], engineered_df.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        for group_name, cols in feature_groups.items():\n",
    "            X_train = X_train_full[cols]\n",
    "            X_test = X_test_full[cols]\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            # fallback for models without predict_proba\n",
    "            if hasattr(model, \"predict_proba\"):\n",
    "                y_proba = model.predict_proba(X_test)[:, 1]\n",
    "            else:\n",
    "                y_proba = model.predict(X_test)\n",
    "\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            try:\n",
    "                auc = roc_auc_score(y_test, y_proba)\n",
    "            except ValueError:\n",
    "                # y_test might only contain one class\n",
    "                auc = np.nan\n",
    "\n",
    "            results.append({\n",
    "                \"fold\": fold_idx,\n",
    "                \"model\": model_name,\n",
    "                \"group\": group_name,\n",
    "                \"accuracy\": acc,\n",
    "                \"auc\": auc\n",
    "            })\n",
    "\n",
    "            fold_predictions[(fold_idx, model_name, group_name)] = pd.DataFrame({\n",
    "                \"y_true\": y_test.values,\n",
    "                \"y_pred\": y_pred,\n",
    "                \"y_proba\": y_proba\n",
    "            }, index=y_test.index)\n",
    "\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by model and feature group\n",
    "summary = results_df.groupby([\"model\", \"group\"])[[\"accuracy\", \"auc\"]].agg([\"mean\", \"std\"]).reset_index()\n",
    "# flatten multiindex\n",
    "summary.columns = [\"_\".join(col).strip() for col in summary.columns.values]\n",
    "summary['stability_score'] = summary['auc_mean'] - 0.5 * summary['auc_std']\n",
    "summary.sort_values('stability_score', ascending=False, inplace=True)\n",
    "display(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparams = summary.iloc[0]\n",
    "best_model_name, best_feature_group = best_hyperparams[\"model\"].iloc[0], best_hyperparams[\"group\"].iloc[0]\n",
    "print(f\"Selected model: {best_model_name}; feature group: {best_feature_group}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = models[best_model_name]\n",
    "selected_cols = feature_groups.get(best_feature_group, feature_groups[\"All Features\"])\n",
    "selected_cols = [c for c in selected_cols if c in engineered_df.columns]\n",
    "\n",
    "X_full = engineered_df[selected_cols]\n",
    "y_full = y\n",
    "\n",
    "best_model.fit(X_full, y_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(best_model, FINAL_MODEL_PATH)\n",
    "joblib.dump(selected_cols, FINAL_FEATURES_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10 = summary.head(10)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(data=top10, x=\"group\", y=\"stability_score\", hue=\"model\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.title(\"Top model+feature combinations by stability score\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"top_models.png\", dpi=RESOLUTION)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wilcoxon paired test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "\n",
    "feature_group_to_compare = \"All Features\"\n",
    "\n",
    "pairwise_tests = []\n",
    "models_list = results_df[\"model\"].unique()\n",
    "\n",
    "for i, m1 in enumerate(models_list):\n",
    "    for m2 in models_list[i+1:]:\n",
    "        df1 = results_df[\n",
    "            (results_df[\"model\"] == m1) & \n",
    "            (results_df[\"group\"] == feature_group_to_compare)\n",
    "        ].sort_values(\"fold\")\n",
    "\n",
    "        df2 = results_df[\n",
    "            (results_df[\"model\"] == m2) & \n",
    "            (results_df[\"group\"] == feature_group_to_compare)\n",
    "        ].sort_values(\"fold\")\n",
    "\n",
    "        if len(df1) == len(df2) and len(df1) > 0:\n",
    "            stat, p = wilcoxon(df1[\"auc\"], df2[\"auc\"])\n",
    "            pairwise_tests.append({\n",
    "                \"model_1\": m1,\n",
    "                \"model_2\": m2,\n",
    "                \"wilcoxon_stat\": stat,\n",
    "                \"p_value\": p\n",
    "            })\n",
    "\n",
    "pd.DataFrame(pairwise_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostic plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "for model_name in results_df[\"model\"].unique():\n",
    "    y_true_all = []\n",
    "    y_proba_all = []\n",
    "    \n",
    "    for key, pred_df in fold_predictions.items():\n",
    "        _, mname, group = key\n",
    "        if mname == model_name and group == \"All Features\":\n",
    "            y_true_all.extend(pred_df[\"y_true\"])\n",
    "            y_proba_all.extend(pred_df[\"y_proba\"])\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true_all, y_proba_all)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.plot(fpr, tpr, lw=2, label=f\"{model_name} (AUC={roc_auc:.3f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve (Aggregated Across Folds)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(IMAGE_DIR / \"roc_curve.png\", dpi=RESOLUTION)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "for model_name in results_df[\"model\"].unique():\n",
    "    y_true_all = []\n",
    "    y_proba_all = []\n",
    "    \n",
    "    for key, pred_df in fold_predictions.items():\n",
    "        _, mname, group = key\n",
    "        if mname == model_name and group == \"All Features\":\n",
    "            y_true_all.extend(pred_df[\"y_true\"])\n",
    "            y_proba_all.extend(pred_df[\"y_proba\"])\n",
    "\n",
    "    prob_true, prob_pred = calibration_curve(\n",
    "        y_true_all, y_proba_all, n_bins=10, strategy=\"quantile\"\n",
    "    )\n",
    "\n",
    "    plt.plot(prob_pred, prob_true, marker=\"o\", label=model_name)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"Perfect\")\n",
    "plt.xlabel(\"Predicted Probability\")\n",
    "plt.ylabel(\"Empirical Win Rate\")\n",
    "plt.title(\"Calibration Plot\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(IMAGE_DIR / \"calibration_curve.png\", dpi=RESOLUTION)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "for model_name in results_df[\"model\"].unique():\n",
    "    y_proba_all = []\n",
    "    \n",
    "    for key, pred_df in fold_predictions.items():\n",
    "        _, mname, group = key\n",
    "        if mname == model_name and group == \"All Features\":\n",
    "            y_proba_all.extend(pred_df[\"y_proba\"])\n",
    "\n",
    "    sns.kdeplot(y_proba_all, label=model_name, fill=True, alpha=0.3)\n",
    "\n",
    "plt.title(\"Distribution of Predicted Probabilities\")\n",
    "plt.xlabel(\"p(left wins)\")\n",
    "plt.legend()\n",
    "plt.savefig(IMAGE_DIR / \"predicted_probabilities_hist.png\", dpi=RESOLUTION)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharex=False)\n",
    "\n",
    "# --- Accuracy ---\n",
    "sns.barplot(\n",
    "    data=results_df,\n",
    "    x=\"group\",\n",
    "    y=\"accuracy\",\n",
    "    errorbar=\"sd\",\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(\"Ablation Study: Accuracy by Feature Group\")\n",
    "axes[0].set_ylabel(\"Accuracy\")\n",
    "axes[0].set_xlabel(\"Feature Group\")\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# --- AUC ---\n",
    "sns.barplot(\n",
    "    data=results_df,\n",
    "    x=\"group\",\n",
    "    y=\"auc\",\n",
    "    errorbar=\"sd\",\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(\"Ablation Study: AUC by Feature Group\")\n",
    "axes[1].set_ylabel(\"AUC\")\n",
    "axes[1].set_xlabel(\"Feature Group\")\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"feature_group_stats.png\", dpi=RESOLUTION)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_importances = []\n",
    "\n",
    "for group_name, cols in feature_groups.items():\n",
    "    X_full = engineered_df[cols]\n",
    "    model = deepcopy(best_model)\n",
    "    model.fit(X_full, y)\n",
    "    \n",
    "    df_imp = pd.DataFrame({\n",
    "        \"group\": group_name,\n",
    "        \"feature\": cols,\n",
    "        \"importance\": model.feature_importances_\n",
    "    })\n",
    "    all_importances.append(df_imp)\n",
    "\n",
    "importance_df = pd.concat(all_importances, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot so rows=features, columns=groups, values=importance\n",
    "heatmap_df = importance_df.pivot_table(\n",
    "    index=\"feature\", columns=\"group\", values=\"importance\", fill_value=0\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12,heatmap_df.shape[0]/5))\n",
    "sns.heatmap(heatmap_df, cmap=\"viridis\", linewidths=0.5)\n",
    "plt.title(\"Feature Importance Heatmap Across Groups\")\n",
    "plt.xlabel(\"Feature Group\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig(IMAGE_DIR / \"feature_importance.png\", dpi=RESOLUTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_ranking = importance_df.groupby(\"feature\")[\"importance\"].mean().sort_values(ascending=False).head(20)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(x=global_ranking.values, y=global_ranking.index, hue=global_ranking.index, palette=\"viridis\")\n",
    "plt.title(\"Top 20 Features Overall\")\n",
    "plt.xlabel(\"Mean Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig(IMAGE_DIR / \"top_20_features.png\", dpi=RESOLUTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Single feature models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
