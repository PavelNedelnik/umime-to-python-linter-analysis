{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "from src.prioritization import *\n",
    "\n",
    "SAMPLE_SIZE = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CONFIG_ENV\"] = \"debug\"\n",
    "if False:\n",
    "    os.environ[\"CONFIG_ENV\"] = \"production\"\n",
    "\n",
    "from config import load_config\n",
    "config = load_config()\n",
    "\n",
    "DEBUG = config[\"DEBUG\"]\n",
    "RESOLUTION = config['DEFAULTS']['resolution']\n",
    "RANDOM_SEED = config['DEFAULTS']['random_seed']\n",
    "\n",
    "# input data\n",
    "STORAGE_PATH = config['PATHS']['storage']\n",
    "HOLD_OUT_DATA_PATH = config['PATHS']['student_hold_out_set']\n",
    "TRAINED_MODELS_PATH = config['PATHS']['hold_out_trained_heuristics']\n",
    "FINAL_MODEL_PATH = None  # TODO\n",
    "\n",
    "# output data\n",
    "STUDY_OUTPUT_PATH = HOLD_OUT_DATA_PATH / 'student_study_submissions'\n",
    "IMAGE_DIR = config['PATHS']['images'] / 'student_study_preparation'\n",
    "\n",
    "os.makedirs(STUDY_OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv(STORAGE_PATH / 'items.csv', index_col=0)\n",
    "defects = pd.read_csv(STORAGE_PATH / f'defects.csv', index_col=0)\n",
    "\n",
    "log = pd.read_csv(HOLD_OUT_DATA_PATH / 'log.csv', index_col=0, parse_dates=['time'])\n",
    "defect_log = pd.read_csv(HOLD_OUT_DATA_PATH / 'defect_log.csv', index_col=0)\n",
    "defect_log.columns = defect_log.columns.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    TaskCommonModel,\n",
    "    TaskCharacteristicModel,\n",
    "    StudentCommonModel,\n",
    "    StudentCharacteristicModel,\n",
    "    StudentEncounteredBeforeModel,\n",
    "    DefectMultiplicityModel,\n",
    "    SeverityModel,\n",
    "]\n",
    "\n",
    "models = [model.load(TRAINED_MODELS_PATH / f'{model.get_model_name()}.pkl') for model in models]\n",
    "\n",
    "models = {model.get_model_name(): model for model in models}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Quality filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at least two defects\n",
    "is_non_trivial = (defect_log > 0).sum(axis=1) > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# severity is decisive\n",
    "severities = (defect_log > 0).astype(int) * defects.loc[defect_log.columns]['severity']\n",
    "is_decisive = severities.apply(lambda s: np.ptp(s) > 0, axis=1)\n",
    "is_non_trivial = is_non_trivial & is_decisive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Pct of tied severity results: {is_decisive[is_non_trivial].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Generate heuristic scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sampling statistics and remember heuristic scores\n",
    "discrete_scores = []\n",
    "continuous_scores = []\n",
    "\n",
    "# Ensure log is sorted by time\n",
    "log = log.sort_values(by='time')\n",
    "\n",
    "for idx, submission in tqdm(log.iloc[:500].iterrows() if DEBUG else log.iterrows(), total=log.shape[0], desc=\"Calculating statistics for sampling\"):\n",
    "    defect_counts = defect_log.loc[idx]\n",
    "\n",
    "    if not is_non_trivial.loc[idx]:\n",
    "        for model in models.values():\n",
    "            model.update(submission, defect_counts)\n",
    "        continue\n",
    "\n",
    "    # Heuristic scores\n",
    "    discrete_model_scores = {name: model.discretize(submission, defect_counts).dropna() for name, model in models.items()}\n",
    "    continuous_model_scores = {name: model._calculate_scores(submission, defect_counts) for name, model in models.items()}\n",
    "\n",
    "    # Save model scores\n",
    "    for defect in defect_counts[defect_counts > 0].index:\n",
    "        discrete_row = {\"submission id\": idx, \"defect id\": defect}\n",
    "        continuous_row = discrete_row.copy()\n",
    "        for name, scores in discrete_model_scores.items():\n",
    "            discrete_row[name] = scores[defect]\n",
    "            continuous_row[name] = continuous_model_scores[name][defect]\n",
    "        discrete_scores.append(discrete_row)\n",
    "        continuous_scores.append(continuous_row)\n",
    "\n",
    "discrete_scores = pd.DataFrame(discrete_scores)\n",
    "continuous_scores = pd.DataFrame(continuous_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Construct dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode as pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Filter for cases where model and baseline disagree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eligible = log.loc[is_non_trivial]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Survey sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "# Stratify by topic\n",
    "strata_labels = items['topic'].unique()\n",
    "strata_counts = np.zeros(strata_labels.shape[0], dtype=int)\n",
    "strata = log.loc[eligible.index, 'item'].map(items['topic'])\n",
    "\n",
    "# Greedy sampling\n",
    "sampled_ids = []\n",
    "while len(sampled_ids) < SAMPLE_SIZE:\n",
    "    # Least represented strata\n",
    "    topic_idx = rng.choice(np.where(strata_counts == strata_counts.min())[0], size=1)\n",
    "    # Collect eligible submissions\n",
    "    unassigned_ids_in_strata = eligible[strata == strata_labels[topic_idx][0]].index.difference(sampled_ids)\n",
    "    try:\n",
    "        sampled_ids.append(rng.choice(unassigned_ids_in_strata, replace=False))\n",
    "    except ValueError:\n",
    "        pass\n",
    "    strata_counts[topic_idx] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defect_counts = defect_log.loc[sampled_ids].sum().sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.bar(defect_counts.index.astype(str), defect_counts.values)\n",
    "ax.set_xlabel(\"Defect ID\")\n",
    "ax.set_ylabel(\"Number of Submissions\")\n",
    "ax.set_title(\"Coverage of Defects in Hold-Out Partition\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"defect_coverage.png\", dpi=RESOLUTION)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_counts = log.loc[sampled_ids, \"item\"].value_counts().reindex(items.index).sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.bar(task_counts.index.astype(str), task_counts.values)\n",
    "ax.set_xlabel(\"Task ID\")\n",
    "ax.set_ylabel(\"Number of Submissions\")\n",
    "ax.set_title(\"Coverage of Tasks in Hold-Out Partition\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"task_coverage.png\", dpi=RESOLUTION)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = items['topic'].unique()\n",
    "topic_counts = items.loc[log.loc[sampled_ids, \"item\"], \"topic\"].value_counts().reindex(topics).sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.bar(topic_counts.index.astype(str), topic_counts.values)\n",
    "ax.set_xlabel(\"Topic\")\n",
    "ax.set_ylabel(\"Number of Submissions\")\n",
    "ax.set_title(\"Coverage of Topics in Hold-Out Partition\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"topic_coverage.png\", dpi=RESOLUTION)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Generate survey files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Save the survey files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
