{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import json\n",
    "import networkx as nx\n",
    "\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "from src.prioritization import *\n",
    "from src.feature_engineering import add_heuristic_scores, add_feature_sets\n",
    "\n",
    "SAMPLE_SIZE = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CONFIG_ENV\"] = \"debug\"\n",
    "if False:\n",
    "    os.environ[\"CONFIG_ENV\"] = \"production\"\n",
    "\n",
    "from config import load_config\n",
    "config = load_config()\n",
    "\n",
    "DEBUG = config[\"DEBUG\"]\n",
    "RESOLUTION = config['DEFAULTS']['resolution']\n",
    "RANDOM_SEED = config['DEFAULTS']['random_seed']\n",
    "\n",
    "# input data\n",
    "STORAGE_PATH = config['PATHS']['storage']\n",
    "HOLD_OUT_DATA_PATH = config['PATHS']['student_hold_out_set']\n",
    "TRAINED_SCORING_MODELS_PATH = config['PATHS']['hold_out_trained_heuristics']\n",
    "benchmark_path = config['PATHS']['benchmark_dataset']\n",
    "FINAL_MODEL_PATH = benchmark_path / \"final_teacher_model.pkl\"\n",
    "FINAL_FEATURES_PATH = benchmark_path / \"final_selected_features.pkl\"\n",
    "FINAL_HEURISTIC_HYPERPARAMETERS_PATH = benchmark_path / \"final_heuristic_selected_features.pkl\"\n",
    "\n",
    "# output data\n",
    "STUDY_OUTPUT_PATH = HOLD_OUT_DATA_PATH / 'student_study_submissions'\n",
    "IMAGE_DIR = config['PATHS']['images'] / 'student_study_preparation'\n",
    "\n",
    "os.makedirs(STUDY_OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv(STORAGE_PATH / 'items.csv', index_col=0)\n",
    "defects = pd.read_csv(STORAGE_PATH / f'defects.csv', index_col=0)\n",
    "\n",
    "log = pd.read_csv(HOLD_OUT_DATA_PATH / 'log.csv', index_col=0, parse_dates=['time'])\n",
    "defect_log = pd.read_csv(HOLD_OUT_DATA_PATH / 'defect_log.csv', index_col=0)\n",
    "defect_log.columns = defect_log.columns.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_models = [\n",
    "    TaskCommonModel,\n",
    "    TaskCharacteristicModel,\n",
    "    StudentCommonModel,\n",
    "    StudentCharacteristicModel,\n",
    "    StudentEncounteredBeforeModel,\n",
    "    DefectMultiplicityModel,\n",
    "    SeverityModel,\n",
    "]\n",
    "\n",
    "scoring_models = [model.load(TRAINED_SCORING_MODELS_PATH / f'{model.get_model_name()}.pkl') for model in scoring_models]\n",
    "\n",
    "scoring_models = {model.get_model_name(): model for model in scoring_models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordering_model = joblib.load(FINAL_MODEL_PATH)\n",
    "model_columns = joblib.load(FINAL_FEATURES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_heuristic, secondary_heuristic = json.load(open(FINAL_HEURISTIC_HYPERPARAMETERS_PATH, \"r\")).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_heuristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_baseline_predict(primary_col, secondary_col):\n",
    "    \"\"\"Baseline ordering heuristic.\n",
    "    \n",
    "    Args:\n",
    "        row: A single row of the log.\n",
    "        primary_col: The name of the primary heuristic column (as a dict).\n",
    "        secondary_col: The name of the secondary heuristic column (as a dict).\n",
    "        \n",
    "    Returns:\n",
    "        A function that takes a single row of the log and returns a prediction.\n",
    "    \"\"\"\n",
    "    primary_col, primary_val_type = primary_col['heuristic'], primary_col['value_type']\n",
    "    secondary_col, secondary_val_type = secondary_col['heuristic'], secondary_col['value_type']\n",
    "    \n",
    "    def predict(row):\n",
    "        value_left = row[f'{primary_col} (Left {primary_val_type})']\n",
    "        value_right = row[f'{primary_col} (Right {primary_val_type})']\n",
    "\n",
    "        if value_left > value_right:\n",
    "            return 1\n",
    "        elif value_right > value_left:\n",
    "            return 0\n",
    "\n",
    "        # TIE → use secondary heuristic\n",
    "        value_left = row[f'{secondary_col} (Left {secondary_val_type})']\n",
    "        value_right = row[f'{secondary_col} (Right {secondary_val_type})']\n",
    "\n",
    "        if value_left > value_right:\n",
    "            return 1\n",
    "        return 0\n",
    "    return predict\n",
    "\n",
    "\n",
    "def make_baseline_tiebreak(secondary_col):\n",
    "    \"\"\"Tiebreak function to be used with the baseline oredering heuristic.\n",
    "\n",
    "    Arguments:\n",
    "        secondary_col -- The name of the secondary heuristic column (as a dict).\n",
    "    Returns:\n",
    "        A function that takes a single row of the log and returns a tiebreak score.\n",
    "    \"\"\"\n",
    "    secondary_col, secondary_val_type = secondary_col['heuristic'], secondary_col['value_type']\n",
    "    def predict(row):\n",
    "        value_left = row[f'{secondary_col} (Left {secondary_val_type})']\n",
    "        value_right = row[f'{secondary_col} (Right {secondary_val_type})']\n",
    "        return value_left - value_right\n",
    "    return predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Quality filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at least two defects\n",
    "is_non_trivial = (defect_log > 0).sum(axis=1) > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Generate all heuristic scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sampling statistics and remember heuristic scores\n",
    "discrete_scores = []\n",
    "continuous_scores = []\n",
    "\n",
    "# Ensure log is sorted by time\n",
    "log = log.sort_values(by='time')\n",
    "\n",
    "for idx, submission in tqdm(log.iloc[:500].iterrows() if DEBUG else log.iterrows(), total=log.shape[0], desc=\"Calculating statistics for sampling\"):\n",
    "    defect_counts = defect_log.loc[idx]\n",
    "\n",
    "    if not is_non_trivial.loc[idx]:\n",
    "        for model in scoring_models.values():\n",
    "            model.update(submission, defect_counts)\n",
    "        continue\n",
    "\n",
    "    # Heuristic scores\n",
    "    discrete_model_scores = {name: model.discretize(submission, defect_counts).dropna() for name, model in scoring_models.items()}\n",
    "    continuous_model_scores = {name: model._calculate_scores(submission, defect_counts) for name, model in scoring_models.items()}\n",
    "\n",
    "    # Save model scores\n",
    "    for defect in defect_counts[defect_counts > 0].index:\n",
    "        discrete_row = {\"submission id\": idx, \"defect id\": defect}\n",
    "        continuous_row = discrete_row.copy()\n",
    "        for name, scores in discrete_model_scores.items():\n",
    "            discrete_row[name] = scores[defect]\n",
    "            continuous_row[name] = continuous_model_scores[name][defect]\n",
    "        discrete_scores.append(discrete_row)\n",
    "        continuous_scores.append(continuous_row)\n",
    "\n",
    "    for model in scoring_models.values():\n",
    "        model.update(submission, defect_counts)\n",
    "\n",
    "discrete_scores = pd.DataFrame(discrete_scores)\n",
    "continuous_scores = pd.DataFrame(continuous_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Construct dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the pairwise dataset\n",
    "long_defects = defect_log.melt(var_name='defect id', value_name='count', ignore_index=False).reset_index(names=['submission id'])\n",
    "long_defects = long_defects[long_defects['count'] > 0]\n",
    "\n",
    "def generate_defect_pairs(group):\n",
    "    \"\"\"Generate all possible pairs of defects in a submission.\"\"\"\n",
    "    defects = group['defect id'].tolist()\n",
    "    return pd.DataFrame(combinations(defects, 2), columns=['left', 'right'])\n",
    "\n",
    "all_pairs = (\n",
    "    long_defects.groupby('submission id')\n",
    "    .apply(generate_defect_pairs, include_groups=False)\n",
    "    .reset_index(level=1, drop=True)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# add metadata\n",
    "all_pairs['item'] = log.loc[all_pairs['submission id'], 'item'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add scores\n",
    "df = add_heuristic_scores(all_pairs, discrete_scores, continuous_scores)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add features\n",
    "\n",
    "X, _ = add_feature_sets(df, items, defects)\n",
    "\n",
    "X = X[model_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Calculate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_submission(submission_df: pd.DataFrame, prediction_col: str, tiebreak_col: str):\n",
    "    \"\"\"\n",
    "    Rank all defects in a single submission from pairwise predictions.\n",
    "    \n",
    "    Args:\n",
    "        submission_df: All rows corresponding to a single submission.\n",
    "        prediction_col: Name of column containing pairwise predictions.\n",
    "        tiebreak_col: Name of column containing pairwise tiebreak scores.\n",
    "    Returns:\n",
    "        List of nodes in ranked order (highest rank first).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build full directed graph of pairwise decisions\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    nodes = pd.unique(submission_df[['left','right']].values.ravel())\n",
    "    G.add_nodes_from(nodes)\n",
    "    \n",
    "    for _, row in submission_df.iterrows():\n",
    "        left = row['left']\n",
    "        right = row['right']\n",
    "        pred = row[prediction_col]\n",
    "        tiebreak = row[tiebreak_col]\n",
    "        \n",
    "        if pred == 1:\n",
    "            G.add_edge(left, right, tiebreak=tiebreak)\n",
    "        else:\n",
    "            G.add_edge(right, left, tiebreak=tiebreak)\n",
    "    \n",
    "    # Topological sort with cycle-breaking\n",
    "    ranked_nodes = []\n",
    "    while len(G) > 0:\n",
    "        # Nodes with zero in-degree (sources)\n",
    "        zero_in = [n for n, d in G.in_degree() if d == 0]\n",
    "        \n",
    "        if zero_in:\n",
    "            # Break ties by sum of outgoing tiebreak scores\n",
    "            scores = {n: sum(data['tiebreak'] for _, _, data in G.out_edges(n, data=True)) \n",
    "                      for n in zero_in}\n",
    "            next_node = max(scores, key=scores.get)\n",
    "        else:\n",
    "            # Cycle exists → pick node with lowest \"net in vs out tiebreak\"\n",
    "            scores = {}\n",
    "            for n in G.nodes():\n",
    "                out_score = sum(data['tiebreak'] for _, _, data in G.out_edges(n, data=True))\n",
    "                in_score = sum(data['tiebreak'] for _, _, data in G.in_edges(n, data=True))\n",
    "                scores[n] = out_score - in_score  # higher is more \"dominant\"\n",
    "            # Remove node with highest dominance first\n",
    "            next_node = max(scores, key=scores.get)\n",
    "        \n",
    "        ranked_nodes.append(next_node)\n",
    "        G.remove_node(next_node)\n",
    "    \n",
    "    return ranked_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['model_probability'] = ordering_model.predict_proba(X)[:, 1]\n",
    "df['model_prediction'] = ordering_model.predict(X)\n",
    "\n",
    "df['baseline_tiebreak'] = df.apply(make_baseline_tiebreak(secondary_heuristic), axis=1)\n",
    "df['baseline_prediction'] = df.apply(make_baseline_predict(primary_heuristic, secondary_heuristic), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agree_on_first = {}\n",
    "model_rankings = {}\n",
    "baseline_rankings = {}\n",
    "\n",
    "for submission_id, submission_df in df.groupby('submission id'):\n",
    "    model_ranked = rank_submission(submission_df, 'model_prediction', 'model_probability')\n",
    "    baseline_ranked = rank_submission(submission_df, 'baseline_prediction', 'baseline_tiebreak')\n",
    "    agree_on_first[submission_id] = model_ranked[0] == baseline_ranked[0]\n",
    "    model_rankings[submission_id] = model_ranked\n",
    "    baseline_rankings[submission_id] = baseline_ranked\n",
    "\n",
    "agree_on_first = pd.Series(agree_on_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eligible = agree_on_first[agree_on_first].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Sample for the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "# Stratify by topic\n",
    "strata_labels = items['topic'].unique()\n",
    "strata_counts = np.zeros(strata_labels.shape[0], dtype=int)\n",
    "strata = log.loc[eligible, 'item'].map(items['topic'])\n",
    "\n",
    "# Greedy sampling\n",
    "sampled_ids = []\n",
    "while len(sampled_ids) < SAMPLE_SIZE:\n",
    "    # Least represented strata\n",
    "    topic_idx = rng.choice(np.where(strata_counts == strata_counts.min())[0], size=1)\n",
    "    # Collect eligible submissions\n",
    "    in_strata = strata[strata == strata_labels[topic_idx][0]].index\n",
    "    unassigned_ids_in_strata = eligible.intersection(in_strata).difference(sampled_ids)\n",
    "    try:\n",
    "        sampled_ids.append(rng.choice(unassigned_ids_in_strata, replace=False))\n",
    "    except ValueError:\n",
    "        pass\n",
    "    strata_counts[topic_idx] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defect_counts = defect_log.loc[sampled_ids].sum().sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.bar(defect_counts.index.astype(str), defect_counts.values)\n",
    "ax.set_xlabel(\"Defect ID\")\n",
    "ax.set_ylabel(\"Number of Submissions\")\n",
    "ax.set_title(\"Coverage of Defects in Hold-Out Partition\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"defect_coverage.png\", dpi=RESOLUTION)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_counts = log.loc[sampled_ids, \"item\"].value_counts().reindex(items.index).sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.bar(task_counts.index.astype(str), task_counts.values)\n",
    "ax.set_xlabel(\"Task ID\")\n",
    "ax.set_ylabel(\"Number of Submissions\")\n",
    "ax.set_title(\"Coverage of Tasks in Hold-Out Partition\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"task_coverage.png\", dpi=RESOLUTION)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = items['topic'].unique()\n",
    "topic_counts = items.loc[log.loc[sampled_ids, \"item\"], \"topic\"].value_counts().reindex(topics).sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.bar(topic_counts.index.astype(str), topic_counts.values)\n",
    "ax.set_xlabel(\"Topic\")\n",
    "ax.set_ylabel(\"Number of Submissions\")\n",
    "ax.set_title(\"Coverage of Topics in Hold-Out Partition\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"topic_coverage.png\", dpi=RESOLUTION)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Generate explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Generate survey files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Export"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
