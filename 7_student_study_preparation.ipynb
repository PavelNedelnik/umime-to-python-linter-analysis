{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import json\n",
    "import networkx as nx\n",
    "\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import kendalltau\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from src.prioritization import *\n",
    "from src.feature_engineering import select_features, build_pairwise_features\n",
    "from src.ordering import rank_submission\n",
    "from src.explanations import explain_submission, explain_baseline_submission\n",
    "\n",
    "SAMPLE_SIZE = 25\n",
    "N_STUDENTS = 10\n",
    "MAIN_TASKS_PER_STUDENT = 5\n",
    "MIN_CONTRIBUTION = 1e-3\n",
    "MAX_EXPLANATION_LENGTH = 3\n",
    "\n",
    "RETRAIN_MODELS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CONFIG_ENV\"] = \"debug\"\n",
    "if False:\n",
    "    os.environ[\"CONFIG_ENV\"] = \"production\"\n",
    "\n",
    "from config import load_config\n",
    "config = load_config()\n",
    "\n",
    "DEBUG = config[\"DEBUG\"]\n",
    "RESOLUTION = config['DEFAULTS']['resolution']\n",
    "RANDOM_SEED = config['DEFAULTS']['random_seed']\n",
    "\n",
    "# input data\n",
    "STORAGE_PATH = config['PATHS']['storage']\n",
    "HOLD_OUT_DATA_PATH = config['PATHS']['student_hold_out_set']\n",
    "TRAINED_SCORING_MODELS_PATH = config['PATHS']['hold_out_trained_heuristics']\n",
    "benchmark_path = config['PATHS']['benchmark_dataset']\n",
    "FINAL_MODEL_PATH = benchmark_path / \"final_teacher_model.pkl\"\n",
    "FINAL_FEATURES_PATH = benchmark_path / \"final_selected_features.pkl\"\n",
    "FINAL_BASELINE_PATH = benchmark_path / \"baseline_models\"\n",
    "FINAL_BASELINE_FEATURES_PATH = benchmark_path / \"baseline_features\"\n",
    "\n",
    "# output data\n",
    "STUDY_OUTPUT_PATH = HOLD_OUT_DATA_PATH / 'student_study_submissions'\n",
    "IMAGE_DIR = config['PATHS']['images'] / 'student_study_preparation'\n",
    "\n",
    "os.makedirs(STUDY_OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv(STORAGE_PATH / 'items.csv', index_col=0)\n",
    "defects = pd.read_csv(STORAGE_PATH / f'defects.csv', index_col=0)\n",
    "\n",
    "log = pd.read_csv(HOLD_OUT_DATA_PATH / 'log.csv', index_col=0, parse_dates=['time'])\n",
    "defect_log = pd.read_csv(HOLD_OUT_DATA_PATH / 'defect_log.csv', index_col=0)\n",
    "defect_log.columns = defect_log.columns.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_models = [\n",
    "    TaskCommonModel,\n",
    "    TaskCharacteristicModel,\n",
    "    StudentCommonModel,\n",
    "    StudentCharacteristicModel,\n",
    "    StudentEncounteredBeforeModel,\n",
    "    DefectMultiplicityModel,\n",
    "    SeverityModel,\n",
    "]\n",
    "\n",
    "scoring_models = [model.load(TRAINED_SCORING_MODELS_PATH / f'{model.get_model_name()}.pkl') for model in scoring_models]\n",
    "\n",
    "scoring_models = {model.get_model_name(): model for model in scoring_models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordering_model = joblib.load(FINAL_MODEL_PATH)\n",
    "model_columns = joblib.load(FINAL_FEATURES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_model = joblib.load(FINAL_BASELINE_PATH / 'primary_model.joblib')\n",
    "secondary_model = joblib.load(FINAL_BASELINE_PATH / 'secondary_model.joblib')\n",
    "primary_cols = joblib.load(FINAL_BASELINE_FEATURES_PATH / 'primary_features.joblib')\n",
    "secondary_cols = joblib.load(FINAL_BASELINE_FEATURES_PATH / 'secondary_features.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Quality filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at least two defects\n",
    "is_non_trivial = (defect_log > 0).sum(axis=1) > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Generate all heuristic scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_MODELS:\n",
    "    # Calculate sampling statistics and remember heuristic scores\n",
    "    discrete_scores = []\n",
    "    continuous_scores = []\n",
    "\n",
    "    # Ensure log is sorted by time\n",
    "    log = log.sort_values(by='time')\n",
    "\n",
    "    for idx, submission in tqdm(log.iloc[:500].iterrows() if DEBUG else log.iterrows(), total=log.shape[0], desc=\"Calculating statistics for sampling\"):\n",
    "        defect_counts = defect_log.loc[idx]\n",
    "\n",
    "        if not is_non_trivial.loc[idx]:\n",
    "            for model in scoring_models.values():\n",
    "                model.update(submission, defect_counts)\n",
    "            continue\n",
    "\n",
    "        # Heuristic scores\n",
    "        discrete_model_scores = {name: model.discretize(submission, defect_counts).dropna() for name, model in scoring_models.items()}\n",
    "        continuous_model_scores = {name: model._calculate_scores(submission, defect_counts) for name, model in scoring_models.items()}\n",
    "\n",
    "        # Save model scores\n",
    "        for defect in defect_counts[defect_counts > 0].index:\n",
    "            discrete_row = {\"submission id\": idx, \"defect id\": defect}\n",
    "            continuous_row = discrete_row.copy()\n",
    "            for name, scores in discrete_model_scores.items():\n",
    "                discrete_row[name] = scores[defect]\n",
    "                continuous_row[name] = continuous_model_scores[name][defect]\n",
    "            discrete_scores.append(discrete_row)\n",
    "            continuous_scores.append(continuous_row)\n",
    "\n",
    "        for model in scoring_models.values():\n",
    "            model.update(submission, defect_counts)\n",
    "\n",
    "    discrete_scores = pd.DataFrame(discrete_scores)\n",
    "    continuous_scores = pd.DataFrame(continuous_scores)\n",
    "\n",
    "    discrete_scores.to_csv(HOLD_OUT_DATA_PATH / 'student_holdout_discrete_scores.csv', index=False)\n",
    "    continuous_scores.to_csv(HOLD_OUT_DATA_PATH / 'student_holdout_continuous_scores.csv', index=False)\n",
    "else:\n",
    "    discrete_scores = pd.read_csv(HOLD_OUT_DATA_PATH / 'student_holdout_discrete_scores.csv')\n",
    "    continuous_scores = pd.read_csv(HOLD_OUT_DATA_PATH / 'student_holdout_continuous_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize scale\n",
    "for name, model in scoring_models.items():\n",
    "    if model.get_discretization_scale() == '-2-2':\n",
    "        discrete_scores[name] = discrete_scores[name] + 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Construct dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the pairwise dataset\n",
    "long_defects = defect_log.melt(var_name='defect id', value_name='count', ignore_index=False).reset_index(names=['submission id'])\n",
    "long_defects = long_defects[long_defects['count'] > 0]\n",
    "\n",
    "def generate_defect_pairs(group):\n",
    "    \"\"\"Generate all possible pairs of defects in a submission.\"\"\"\n",
    "    defects = group['defect id'].tolist()\n",
    "    return pd.DataFrame(combinations(defects, 2), columns=['left', 'right'])\n",
    "\n",
    "all_pairs = (\n",
    "    long_defects.groupby('submission id')\n",
    "    .apply(generate_defect_pairs, include_groups=False)\n",
    "    .reset_index(level=1, drop=True)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# add metadata\n",
    "all_pairs['item'] = log.loc[all_pairs['submission id'], 'item'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add scores\n",
    "df, catalog = build_pairwise_features(all_pairs, discrete_scores, continuous_scores, items, defects)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Calculate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(ordering_model, Pipeline) and hasattr(ordering_model['clf'], 'coef_'):\n",
    "    weights = ordering_model['clf'].coef_.ravel()\n",
    "    bias = ordering_model['clf'].intercept_[0]\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ordering_model = df[model_columns]\n",
    "df['model_tiebreak'] = ordering_model.predict_proba(X_ordering_model)[:, 1]\n",
    "df['model_prediction'] = ordering_model.predict(X_ordering_model)\n",
    "\n",
    "X_primary_heuristic = df[primary_cols]\n",
    "X_secondary_heuristic = df[secondary_cols]\n",
    "df['baseline_tiebreak'] = secondary_model.predict_proba(X_secondary_heuristic)[:, 1]\n",
    "df['baseline_prediction'] = primary_model.predict(X_primary_heuristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agree_on_first = {}\n",
    "model_rankings = {}\n",
    "baseline_rankings = {}\n",
    "model_explanations = {}\n",
    "baseline_explanations = {}\n",
    "\n",
    "for submission_id, submission_df in df.groupby('submission id'):\n",
    "    model_ranked = rank_submission(submission_df, 'model_prediction', 'model_tiebreak')\n",
    "    baseline_ranked = rank_submission(submission_df, 'baseline_prediction', 'baseline_tiebreak')\n",
    "    model_rankings[submission_id] = model_ranked\n",
    "    baseline_rankings[submission_id] = baseline_ranked\n",
    "    agree_on_first[submission_id] = model_ranked[0] == baseline_ranked[0]\n",
    "    model_explanations[submission_id] = explain_submission(\n",
    "        submission_df, model_ranked, X_ordering_model.loc[submission_df.index],\n",
    "        weights, catalog\n",
    "    )\n",
    "    baseline_explanations[submission_id] = explain_baseline_submission(\n",
    "        submission_df, baseline_ranked, primary_cols, secondary_cols\n",
    "    )\n",
    "\n",
    "agree_on_first = pd.Series(agree_on_first)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Sample for the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eligible = agree_on_first[~agree_on_first].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "# Stratify by topic\n",
    "strata_labels = items['topic'].unique()\n",
    "strata_counts = np.zeros(strata_labels.shape[0], dtype=int)\n",
    "strata = log.loc[eligible, 'item'].map(items['topic'])\n",
    "\n",
    "# Greedy sampling\n",
    "sampled_ids = []\n",
    "while len(sampled_ids) < SAMPLE_SIZE:\n",
    "    # Least represented strata\n",
    "    topic_idx = rng.choice(np.where(strata_counts == strata_counts.min())[0], size=1)\n",
    "    # Collect eligible submissions\n",
    "    in_strata = strata[strata == strata_labels[topic_idx][0]].index\n",
    "    unassigned_ids_in_strata = in_strata.difference(sampled_ids)\n",
    "    try:\n",
    "        sampled_ids.append(rng.choice(unassigned_ids_in_strata, replace=False))\n",
    "    except ValueError:\n",
    "        pass\n",
    "    strata_counts[topic_idx] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defect_counts = defect_log.loc[sampled_ids].sum().sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.bar(defect_counts.index.astype(str), defect_counts.values)\n",
    "ax.set_xlabel(\"Defect ID\")\n",
    "ax.set_ylabel(\"Number of Submissions\")\n",
    "ax.set_title(\"Coverage of Defects in Hold-Out Partition\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"defect_coverage.png\", dpi=RESOLUTION)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_counts = log.loc[sampled_ids, \"item\"].value_counts().reindex(items.index).sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.bar(task_counts.index.astype(str), task_counts.values)\n",
    "ax.set_xlabel(\"Task ID\")\n",
    "ax.set_ylabel(\"Number of Submissions\")\n",
    "ax.set_title(\"Coverage of Tasks in Hold-Out Partition\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"task_coverage.png\", dpi=RESOLUTION)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = items['topic'].unique()\n",
    "topic_counts = items.loc[log.loc[sampled_ids, \"item\"], \"topic\"].value_counts().reindex(topics).sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.bar(topic_counts.index.astype(str), topic_counts.values)\n",
    "ax.set_xlabel(\"Topic\")\n",
    "ax.set_ylabel(\"Number of Submissions\")\n",
    "ax.set_title(\"Coverage of Topics in Hold-Out Partition\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"topic_coverage.png\", dpi=RESOLUTION)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Generate survey files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_study_html(\n",
    "    submission_id,\n",
    "    task_row,\n",
    "    student_id,\n",
    "    task_number,\n",
    "    task_info,\n",
    "    defects_df,\n",
    "    ranking_A,\n",
    "    ranking_B,\n",
    "    explanations_A,\n",
    "    explanations_B,\n",
    "    output_path,\n",
    "    rng=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a neutral HTML page comparing two versions of defect prioritization.\n",
    "\n",
    "    Versions are labeled \"Version A\" and \"Version B\".\n",
    "    Left/right assignment is randomized.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    left_version : list - Which version appears on the left.\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    # Randomize which version appears on the left\n",
    "    if rng.random() < 0.5:\n",
    "        left_label, right_label = \"Version A\", \"Version B\"\n",
    "        left_order, right_order = ranking_A, ranking_B\n",
    "        left_expl, right_expl = explanations_A, explanations_B\n",
    "        left_version = \"A\"\n",
    "    else:\n",
    "        left_label, right_label = \"Version B\", \"Version A\"\n",
    "        left_order, right_order = ranking_B, ranking_A\n",
    "        left_expl, right_expl = explanations_B, explanations_A\n",
    "        left_version = \"B\"\n",
    "\n",
    "    # --- Minimal CSS ---\n",
    "    css = \"\"\"\n",
    "<style>\n",
    "body {\n",
    "    font-family: Arial, sans-serif;\n",
    "    margin: 20px;\n",
    "    color: #222;\n",
    "}\n",
    ".code-block {\n",
    "    background: #f0f0f0;\n",
    "    padding: 10px;\n",
    "    border-radius: 4px;\n",
    "    white-space: pre-wrap;\n",
    "    font-family: monospace;\n",
    "}\n",
    ".section {\n",
    "    margin-bottom: 30px;\n",
    "}\n",
    ".comparison-container {\n",
    "    display: flex;\n",
    "    gap: 20px;\n",
    "}\n",
    ".task-meta {\n",
    "    background: #f0f0f0;\n",
    "    padding: 10px;\n",
    "    border-radius: 6px;\n",
    "    margin-bottom: 15px;\n",
    "    font-size: 0.9rem;\n",
    "}\n",
    ".task-meta p {\n",
    "    margin: 2px 0;\n",
    "}\n",
    "@media (max-width: 900px) {\n",
    "    .comparison-container {\n",
    "        flex-direction: column;\n",
    "    }\n",
    "}\n",
    ".column {\n",
    "    flex: 1;\n",
    "}\n",
    ".column h3 {\n",
    "    text-align: center;\n",
    "}\n",
    ".defect-card {\n",
    "    border: 1px solid #ccc;\n",
    "    padding: 12px;\n",
    "    margin-bottom: 15px;\n",
    "    border-radius: 4px;\n",
    "    background: #fafafa;\n",
    "}\n",
    ".defect-card h4 {\n",
    "    margin: 0 0 5px 0;\n",
    "}\n",
    "details summary {\n",
    "    cursor: pointer;\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "    # --- Defect card builder ---\n",
    "    def build_defect_cards(order, explanations):\n",
    "        N = len(order)\n",
    "        parts = []\n",
    "\n",
    "        for rank_idx, defect_id in enumerate(order, start=1):\n",
    "            defect = defects_df.loc[defect_id]\n",
    "\n",
    "            name = defect[\"defect name\"]\n",
    "            desc = defect[\"description\"]\n",
    "            example = defect.get(\"code example\", \"\")\n",
    "            fix = defect.get(\"code fix example\", \"\")\n",
    "            expl_lines = explanations.get(defect_id, [])\n",
    "\n",
    "            card = f\"\"\"\n",
    "            <div class=\"defect-card\">\n",
    "                <h4>Rank {rank_idx} of {N}: {name}</h4>\n",
    "                <p>{desc}</p>\n",
    "            \"\"\"\n",
    "\n",
    "            if isinstance(example, str) and example.strip():\n",
    "                card += f\"\"\"\n",
    "                <div><strong>Example:</strong>\n",
    "                    <div class=\"code-block\">{example}</div>\n",
    "                </div>\n",
    "                \"\"\"\n",
    "\n",
    "            if isinstance(fix, str) and fix.strip():\n",
    "                card += f\"\"\"\n",
    "                <div><strong>Fix:</strong>\n",
    "                    <div class=\"code-block\">{fix}</div>\n",
    "                </div>\n",
    "                \"\"\"\n",
    "\n",
    "            if expl_lines:\n",
    "                card += \"<details><summary>Why this is here</summary><ul>\"\n",
    "                for line in expl_lines:\n",
    "                    card += f\"<li>{line}</li>\"\n",
    "                card += \"</ul></details>\"\n",
    "\n",
    "            card += \"</div>\"\n",
    "            parts.append(card)\n",
    "\n",
    "        return \"\\n\".join(parts)\n",
    "\n",
    "    left_cards = build_defect_cards(left_order, left_expl)\n",
    "    right_cards = build_defect_cards(right_order, right_expl)\n",
    "\n",
    "    # --- Final HTML ---\n",
    "    html = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <meta charset=\"utf-8\">\n",
    "    <meta name=\"student-id\" content=\"{student_id}\">\n",
    "    <meta name=\"submission-id\" content=\"{submission_id}\">\n",
    "    <meta name=\"task-number\" content=\"{task_number}\">\n",
    "    <meta name=\"version-on-left\" content=\"{left_version}\">\n",
    "    <title>Task {task_number}</title>\n",
    "    {css}\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<div class=\"task-meta\">\n",
    "    <p><strong>Student ID:</strong> {student_id}</p>\n",
    "    <p><strong>Task Number:</strong> {task_number}</p>\n",
    "</div>\n",
    "\n",
    "<div class=\"section\">\n",
    "    <h2>Task: {task_info['name']}</h2>\n",
    "    <p>{task_info['instructions']}</p>\n",
    "\n",
    "    <h3>Student Submission</h3>\n",
    "    <div class=\"code-block\">{task_row['answer']}</div>\n",
    "</div>\n",
    "\n",
    "<div class=\"comparison-container\">\n",
    "\n",
    "    <div class=\"column\">\n",
    "        <h3>{left_label}</h3>\n",
    "        {left_cards}\n",
    "    </div>\n",
    "\n",
    "    <div class=\"column\">\n",
    "        <h3>{right_label}</h3>\n",
    "        {right_cards}\n",
    "    </div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html)\n",
    "\n",
    "    return left_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign tasks to students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_ids = list(range(SAMPLE_SIZE))  # internal indices 0..SAMPLE_SIZE-1\n",
    "\n",
    "task_usage = np.zeros(SAMPLE_SIZE, dtype=int)\n",
    "student_main_tasks = {sid: [] for sid in range(N_STUDENTS)}\n",
    "\n",
    "for sid in range(N_STUDENTS):\n",
    "    # pick tasks with lowest usage\n",
    "    available = np.argsort(task_usage)\n",
    "    chosen = []\n",
    "    for t in available:\n",
    "        if len(chosen) == MAIN_TASKS_PER_STUDENT:\n",
    "            break\n",
    "        chosen.append(t)\n",
    "    student_main_tasks[sid] = chosen\n",
    "    task_usage[chosen] += 1\n",
    "\n",
    "print(\"Task usage distribution:\", task_usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate student folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_students_metadata = {}\n",
    "\n",
    "for sid in range(N_STUDENTS):\n",
    "    student_folder = STUDY_OUTPUT_PATH / f\"student_{sid+1:02d}\"\n",
    "    main_folder = student_folder / \"main_tasks\"\n",
    "    extra_folder = student_folder / \"extra_tasks\"\n",
    "\n",
    "    student_folder.mkdir(exist_ok=True)\n",
    "    main_folder.mkdir(exist_ok=True)\n",
    "    extra_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    # tasks for this student\n",
    "    main_tasks = student_main_tasks[sid]\n",
    "    extra_tasks = [t for t in task_ids if t not in main_tasks]\n",
    "\n",
    "    left_versions = {}\n",
    "\n",
    "    # generate main tasks\n",
    "    for task_index in main_tasks:\n",
    "        submission_id = sampled_ids[task_index]\n",
    "\n",
    "        row = log.loc[submission_id]\n",
    "        task_info = items.loc[row[\"item\"]]\n",
    "\n",
    "        order_A = model_rankings[submission_id]\n",
    "        order_B = baseline_rankings[submission_id]\n",
    "        expl_A = model_explanations[submission_id]\n",
    "        expl_B = baseline_explanations[submission_id]\n",
    "\n",
    "        output_path = main_folder / f\"task_{task_index+1}.html\"\n",
    "\n",
    "        which_left = generate_study_html(\n",
    "            submission_id=submission_id,\n",
    "            task_row=row,\n",
    "            task_info=task_info,\n",
    "            defects_df=defects,\n",
    "            ranking_A=order_A,\n",
    "            ranking_B=order_B,\n",
    "            explanations_A=expl_A,\n",
    "            explanations_B=expl_B,\n",
    "            output_path=output_path,\n",
    "            rng=rng,\n",
    "            student_id=sid+1,\n",
    "            task_number=task_index+1,\n",
    "        )\n",
    "\n",
    "        left_versions[int(task_index + 1)] = which_left\n",
    "\n",
    "    # Generate extra tasks\n",
    "    for task_index in extra_tasks:\n",
    "        submission_id = sampled_ids[task_index]\n",
    "\n",
    "        row = log.loc[submission_id]\n",
    "        task_info = items.loc[row[\"item\"]]\n",
    "\n",
    "        order_A = model_rankings[submission_id]\n",
    "        order_B = baseline_rankings[submission_id]\n",
    "        expl_A = model_explanations[submission_id]\n",
    "        expl_B = baseline_explanations[submission_id]\n",
    "\n",
    "        output_path = extra_folder / f\"task_{task_index+1}.html\"\n",
    "\n",
    "        which_left = generate_study_html(\n",
    "            submission_id=submission_id,\n",
    "            task_row=row,\n",
    "            task_info=task_info,\n",
    "            defects_df=defects,\n",
    "            ranking_A=order_A,\n",
    "            ranking_B=order_B,\n",
    "            explanations_A=expl_A,\n",
    "            explanations_B=expl_B,\n",
    "            output_path=output_path,\n",
    "            rng=rng,\n",
    "            student_id=sid+1,\n",
    "            task_number=task_index+1,\n",
    "        )\n",
    "\n",
    "        left_versions[int(task_index + 1)] = which_left\n",
    "\n",
    "    # Save assignment file\n",
    "    assignment = {\n",
    "        \"student_id\": sid + 1,\n",
    "        \"main_tasks\": [int(t) for t in main_tasks],\n",
    "        \"extra_tasks\": [int(t) for t in extra_tasks],\n",
    "        \"left_versions\": left_versions,   # robust mapping\n",
    "    }\n",
    "\n",
    "    with open(student_folder / \"assignment.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(assignment, f, indent=2)\n",
    "\n",
    "    all_students_metadata[sid+1] = assignment\n",
    "\n",
    "print(\"Finished generating all student packages!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
