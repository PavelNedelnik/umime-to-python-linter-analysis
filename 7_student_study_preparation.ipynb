{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import json\n",
    "import networkx as nx\n",
    "\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "from src.prioritization import *\n",
    "from src.feature_engineering import add_heuristic_scores, add_feature_sets\n",
    "from src.ordering import rank_submission, make_baseline_tiebreak, make_baseline_predict\n",
    "\n",
    "SAMPLE_SIZE = 30\n",
    "\n",
    "RETRAIN_MODELS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CONFIG_ENV\"] = \"debug\"\n",
    "if False:\n",
    "    os.environ[\"CONFIG_ENV\"] = \"production\"\n",
    "\n",
    "from config import load_config\n",
    "config = load_config()\n",
    "\n",
    "DEBUG = config[\"DEBUG\"]\n",
    "RESOLUTION = config['DEFAULTS']['resolution']\n",
    "RANDOM_SEED = config['DEFAULTS']['random_seed']\n",
    "\n",
    "# input data\n",
    "STORAGE_PATH = config['PATHS']['storage']\n",
    "HOLD_OUT_DATA_PATH = config['PATHS']['student_hold_out_set']\n",
    "TRAINED_SCORING_MODELS_PATH = config['PATHS']['hold_out_trained_heuristics']\n",
    "benchmark_path = config['PATHS']['benchmark_dataset']\n",
    "FINAL_MODEL_PATH = benchmark_path / \"final_teacher_model.pkl\"\n",
    "FINAL_FEATURES_PATH = benchmark_path / \"final_selected_features.pkl\"\n",
    "FINAL_HEURISTIC_HYPERPARAMETERS_PATH = benchmark_path / \"final_heuristic_selected_features.pkl\"\n",
    "\n",
    "# output data\n",
    "STUDY_OUTPUT_PATH = HOLD_OUT_DATA_PATH / 'student_study_submissions'\n",
    "IMAGE_DIR = config['PATHS']['images'] / 'student_study_preparation'\n",
    "\n",
    "os.makedirs(STUDY_OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv(STORAGE_PATH / 'items.csv', index_col=0)\n",
    "defects = pd.read_csv(STORAGE_PATH / f'defects.csv', index_col=0)\n",
    "\n",
    "log = pd.read_csv(HOLD_OUT_DATA_PATH / 'log.csv', index_col=0, parse_dates=['time'])\n",
    "defect_log = pd.read_csv(HOLD_OUT_DATA_PATH / 'defect_log.csv', index_col=0)\n",
    "defect_log.columns = defect_log.columns.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_models = [\n",
    "    TaskCommonModel,\n",
    "    TaskCharacteristicModel,\n",
    "    StudentCommonModel,\n",
    "    StudentCharacteristicModel,\n",
    "    StudentEncounteredBeforeModel,\n",
    "    DefectMultiplicityModel,\n",
    "    SeverityModel,\n",
    "]\n",
    "\n",
    "scoring_models = [model.load(TRAINED_SCORING_MODELS_PATH / f'{model.get_model_name()}.pkl') for model in scoring_models]\n",
    "\n",
    "scoring_models = {model.get_model_name(): model for model in scoring_models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordering_model = joblib.load(FINAL_MODEL_PATH)\n",
    "model_columns = joblib.load(FINAL_FEATURES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_heuristic, secondary_heuristic = json.load(open(FINAL_HEURISTIC_HYPERPARAMETERS_PATH, \"r\")).values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Quality filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at least two defects\n",
    "is_non_trivial = (defect_log > 0).sum(axis=1) > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Generate all heuristic scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_MODELS:\n",
    "    # Calculate sampling statistics and remember heuristic scores\n",
    "    discrete_scores = []\n",
    "    continuous_scores = []\n",
    "\n",
    "    # Ensure log is sorted by time\n",
    "    log = log.sort_values(by='time')\n",
    "\n",
    "    for idx, submission in tqdm(log.iloc[:500].iterrows() if DEBUG else log.iterrows(), total=log.shape[0], desc=\"Calculating statistics for sampling\"):\n",
    "        defect_counts = defect_log.loc[idx]\n",
    "\n",
    "        if not is_non_trivial.loc[idx]:\n",
    "            for model in scoring_models.values():\n",
    "                model.update(submission, defect_counts)\n",
    "            continue\n",
    "\n",
    "        # Heuristic scores\n",
    "        discrete_model_scores = {name: model.discretize(submission, defect_counts).dropna() for name, model in scoring_models.items()}\n",
    "        continuous_model_scores = {name: model._calculate_scores(submission, defect_counts) for name, model in scoring_models.items()}\n",
    "\n",
    "        # Save model scores\n",
    "        for defect in defect_counts[defect_counts > 0].index:\n",
    "            discrete_row = {\"submission id\": idx, \"defect id\": defect}\n",
    "            continuous_row = discrete_row.copy()\n",
    "            for name, scores in discrete_model_scores.items():\n",
    "                discrete_row[name] = scores[defect]\n",
    "                continuous_row[name] = continuous_model_scores[name][defect]\n",
    "            discrete_scores.append(discrete_row)\n",
    "            continuous_scores.append(continuous_row)\n",
    "\n",
    "        for model in scoring_models.values():\n",
    "            model.update(submission, defect_counts)\n",
    "\n",
    "    discrete_scores = pd.DataFrame(discrete_scores)\n",
    "    continuous_scores = pd.DataFrame(continuous_scores)\n",
    "\n",
    "    discrete_scores.to_csv(HOLD_OUT_DATA_PATH / 'student_holdout_discrete_scores.csv', index=False)\n",
    "    continuous_scores.to_csv(HOLD_OUT_DATA_PATH / 'student_holdout_continuous_scores.csv', index=False)\n",
    "else:\n",
    "    discrete_scores = pd.read_csv(HOLD_OUT_DATA_PATH / 'student_holdout_discrete_scores.csv')\n",
    "    continuous_scores = pd.read_csv(HOLD_OUT_DATA_PATH / 'student_holdout_continuous_scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Construct dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the pairwise dataset\n",
    "long_defects = defect_log.melt(var_name='defect id', value_name='count', ignore_index=False).reset_index(names=['submission id'])\n",
    "long_defects = long_defects[long_defects['count'] > 0]\n",
    "\n",
    "def generate_defect_pairs(group):\n",
    "    \"\"\"Generate all possible pairs of defects in a submission.\"\"\"\n",
    "    defects = group['defect id'].tolist()\n",
    "    return pd.DataFrame(combinations(defects, 2), columns=['left', 'right'])\n",
    "\n",
    "all_pairs = (\n",
    "    long_defects.groupby('submission id')\n",
    "    .apply(generate_defect_pairs, include_groups=False)\n",
    "    .reset_index(level=1, drop=True)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# add metadata\n",
    "all_pairs['item'] = log.loc[all_pairs['submission id'], 'item'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add scores\n",
    "df = add_heuristic_scores(all_pairs, discrete_scores, continuous_scores)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add features\n",
    "\n",
    "X, _ = add_feature_sets(df, items, defects)\n",
    "\n",
    "X = X[model_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Calculate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['model_probability'] = ordering_model.predict_proba(X)[:, 1]\n",
    "df['model_prediction'] = ordering_model.predict(X)\n",
    "\n",
    "df['baseline_tiebreak'] = df.apply(make_baseline_tiebreak(secondary_heuristic), axis=1)\n",
    "df['baseline_prediction'] = df.apply(make_baseline_predict(primary_heuristic, secondary_heuristic), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agree_on_first = {}\n",
    "model_rankings = {}\n",
    "baseline_rankings = {}\n",
    "\n",
    "for submission_id, submission_df in df.groupby('submission id'):\n",
    "    model_ranked = rank_submission(submission_df, 'model_prediction', 'model_probability')\n",
    "    baseline_ranked = rank_submission(submission_df, 'baseline_prediction', 'baseline_tiebreak')\n",
    "    agree_on_first[submission_id] = model_ranked[0] == baseline_ranked[0]\n",
    "    model_rankings[submission_id] = model_ranked\n",
    "    baseline_rankings[submission_id] = baseline_ranked\n",
    "\n",
    "agree_on_first = pd.Series(agree_on_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eligible = agree_on_first[agree_on_first].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Sample for the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "# Stratify by topic\n",
    "strata_labels = items['topic'].unique()\n",
    "strata_counts = np.zeros(strata_labels.shape[0], dtype=int)\n",
    "strata = log.loc[eligible, 'item'].map(items['topic'])\n",
    "\n",
    "# Greedy sampling\n",
    "sampled_ids = []\n",
    "while len(sampled_ids) < SAMPLE_SIZE:\n",
    "    # Least represented strata\n",
    "    topic_idx = rng.choice(np.where(strata_counts == strata_counts.min())[0], size=1)\n",
    "    # Collect eligible submissions\n",
    "    in_strata = strata[strata == strata_labels[topic_idx][0]].index\n",
    "    unassigned_ids_in_strata = in_strata.difference(sampled_ids)\n",
    "    try:\n",
    "        sampled_ids.append(rng.choice(unassigned_ids_in_strata, replace=False))\n",
    "    except ValueError:\n",
    "        pass\n",
    "    strata_counts[topic_idx] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defect_counts = defect_log.loc[sampled_ids].sum().sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.bar(defect_counts.index.astype(str), defect_counts.values)\n",
    "ax.set_xlabel(\"Defect ID\")\n",
    "ax.set_ylabel(\"Number of Submissions\")\n",
    "ax.set_title(\"Coverage of Defects in Hold-Out Partition\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"defect_coverage.png\", dpi=RESOLUTION)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_counts = log.loc[sampled_ids, \"item\"].value_counts().reindex(items.index).sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.bar(task_counts.index.astype(str), task_counts.values)\n",
    "ax.set_xlabel(\"Task ID\")\n",
    "ax.set_ylabel(\"Number of Submissions\")\n",
    "ax.set_title(\"Coverage of Tasks in Hold-Out Partition\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"task_coverage.png\", dpi=RESOLUTION)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = items['topic'].unique()\n",
    "topic_counts = items.loc[log.loc[sampled_ids, \"item\"], \"topic\"].value_counts().reindex(topics).sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.bar(topic_counts.index.astype(str), topic_counts.values)\n",
    "ax.set_xlabel(\"Topic\")\n",
    "ax.set_ylabel(\"Number of Submissions\")\n",
    "ax.set_title(\"Coverage of Topics in Hold-Out Partition\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"topic_coverage.png\", dpi=RESOLUTION)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Generate explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Generate survey files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Export"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
